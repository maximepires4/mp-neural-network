# Activations API

Activation functions introduce non-linearity into the network, allowing it to learn complex patterns.

## Base Activation

::: mpneuralnetwork.activations.Activation

## Hidden Layers

These activations are typically used in intermediate layers.

::: mpneuralnetwork.activations.ReLU
::: mpneuralnetwork.activations.PReLU
::: mpneuralnetwork.activations.Swish
::: mpneuralnetwork.activations.Tanh
::: mpneuralnetwork.activations.Sigmoid

## Output Layers

These activations are typically used in the final layer to produce probability distributions.

::: mpneuralnetwork.activations.Softmax
::: mpneuralnetwork.activations.Sigmoid
