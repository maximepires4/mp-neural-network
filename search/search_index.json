{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Home","text":""},{"location":"#mpneuralnetwork","title":"MPNeuralNetwork \ud83e\udde0","text":"<p>A fully vectorized Deep Learning framework built from scratch using only NumPy and CuPy.</p> <p>\ud83d\udcd6 Read the Full Documentation</p>"},{"location":"#philosophy-goal","title":"Philosophy &amp; Goal","text":"<p>In an era of high-level frameworks like PyTorch or TensorFlow, it is easy to treat Neural Networks as \"black boxes\".</p> <p>MPNeuralNetwork is an engineering initiative designed to demystify the underlying mathematics of Deep Learning. By rebuilding the engine from the ground up, I aimed to bridge the gap between theoretical equations and production-grade code.</p>"},{"location":"#key-objectives","title":"Key Objectives:","text":"<ol> <li>Mathematical Rigor: Implementing backpropagation, chain rule derivatives, and loss functions manually.</li> <li>Performance Optimization: Moving from naive scalar loops to fully vectorized matrix operations and implementing <code>im2col</code> for convolutions.</li> <li>Software Architecture: Applying SOLID principles for a modular design.</li> </ol>"},{"location":"#key-features","title":"Key Features","text":"<p>MPNeuralNetwork goes beyond basic matrix operations by incorporating an \"intelligent\" engine.</p> <ul> <li>Fully Vectorized: Optimized for batch processing. Convolutions use <code>im2col</code> for hardware acceleration.</li> <li>GPU Acceleration: Seamless support for NVIDIA GPUs via CuPy. Switch backends with a single environment variable.</li> <li>Early Stopping &amp; Checkpointing: Automatically monitors validation loss and restores the best weights.</li> <li>Smart Initialization: Automatically applies He Init (for ReLU) or Xavier (for Sigmoid/Tanh).</li> <li>Comprehensive Regularization: Supports Dropout, L1/L2 Weight Decay (AdamW style).</li> <li>Numerical Stability: Internally handles logits for Softmax/Sigmoid to prevent overflow.</li> <li>Full Serialization: Save/Load model state to <code>.npz</code> files.</li> </ul> <p>\ud83d\udc49 Learn more about the internal engine</p>"},{"location":"#component-inventory","title":"Component Inventory","text":"Category Available Components Layers <code>Dense</code>, <code>Convolutional</code>, <code>MaxPooling2D</code>, <code>AveragePooling2D</code>, <code>Dropout</code>, <code>BatchNormalization</code>, <code>Flatten</code> Activations <code>ReLU</code>, <code>Sigmoid</code>, <code>Tanh</code>, <code>Softmax</code>, <code>PReLU</code>, <code>Swish</code> Optimizers <code>SGD</code>, <code>RMSprop</code>, <code>AdamW</code> Losses <code>MSE</code>, <code>BinaryCrossEntropy</code>, <code>CategoricalCrossEntropy</code>"},{"location":"#installation","title":"Installation","text":"<pre><code>pip install mpneuralnetwork\n</code></pre>"},{"location":"#quick-start","title":"Quick Start","text":""},{"location":"#mnist-classification","title":"MNIST Classification","text":"<pre><code>import numpy as np\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.preprocessing import OneHotEncoder\nfrom mpneuralnetwork.layers import Dense, Dropout\nfrom mpneuralnetwork.activations import ReLU\nfrom mpneuralnetwork.losses import CategoricalCrossEntropy\nfrom mpneuralnetwork.optimizers import Adam\nfrom mpneuralnetwork.model import Model\n\n# 1. Load Data (MNIST)\nX, y = fetch_openml('mnist_784', version=1, return_X_y=True, as_frame=False)\nX = (X / 255.0).astype(np.float32) # Normalize &amp; Float32\ny = OneHotEncoder(sparse_output=False).fit_transform(y.reshape(-1, 1))\n\n# 2. Define the Architecture\nnetwork = [\n    Dense(128, input_size=784), # Auto-He Init\n    ReLU(),\n    Dropout(0.2),\n    Dense(10)                   # Output Logits\n]\n\n# 3. Initialize\nmodel = Model(\n    layers=network,\n    loss=CategoricalCrossEntropy(),\n    optimizer=Adam(learning_rate=0.001)\n)\n\n# 4. Train (Auto-Validation Split)\nmodel.train(X, y, epochs=5, batch_size=64, auto_evaluation=0.2)\n</code></pre> <p>\ud83d\udc49 See full tutorials in the User Guide</p>"},{"location":"#architecture-performance","title":"Architecture &amp; Performance","text":""},{"location":"#vectorization","title":"Vectorization","text":"<p>The training loop handles 3D/2D tensors, replacing slow Python loops with NumPy's BLAS routines. Convolutional layers use the <code>im2col</code> technique, transforming convolutions into efficient Matrix Multiplications (GEMM).</p>"},{"location":"#optimization","title":"Optimization","text":"<p>The framework enforces Float32 precision globally to halve memory usage and double bandwidth. Recent benchmarks show a 26% speedup and 50% memory reduction compared to the initial implementation.</p> <p>\ud83d\udc49 Read the Optimization &amp; Benchmarking Guide</p>"},{"location":"#roadmap","title":"Roadmap","text":"<p>The roadmap has been moved to our GitHub Project board.</p>"},{"location":"#author","title":"Author","text":"<p>Maxime Pires - AI Engineer | CentraleSupelec</p> <p>LinkedIn | Portfolio</p>"},{"location":"INTERNALS/","title":"Architecture &amp; Internals","text":"<p>MPNeuralNetwork is not just a collection of matrix operations. It includes a lightweight \"engine\" that automates many of the tedious and error-prone aspects of Deep Learning configuration, built upon a modular architecture.</p> <p>This guide explains the design decisions and the \"magic\" happening under the hood.</p>"},{"location":"INTERNALS/#1-design-philosophy-solid","title":"1. Design Philosophy (SOLID)","text":""},{"location":"INTERNALS/#decoupling-layers-optimizers-srp","title":"Decoupling Layers &amp; Optimizers (SRP)","text":"<p>To avoid \"God Classes\", the responsibility of calculating gradients from updating parameters is strictly separated.</p> <ul> <li>The Layer's Job: It computes <code>dE/dW</code> (gradient) during the backward pass and stores it. It knows nothing about learning rates or update rules.</li> <li>The Optimizer's Job: The Optimizer class iterates over the layers, retrieves parameters via the generic <code>layer.params</code> property, and applies the update rule (keeping track of momentum/velocity if needed).</li> </ul>"},{"location":"INTERNALS/#unified-optimizer-design-adam-vs-adamw","title":"Unified Optimizer Design (Adam vs AdamW)","text":"<p>A common confusion in Deep Learning libraries is the difference between Adam + L2 Regularization and AdamW (Decoupled Weight Decay).</p> <ul> <li>Standard Approach: Usually, L2 regularization is added to the loss function, meaning its gradient is added to the backpropagated gradient. In adaptive methods like Adam, this means the regularization term is effectively scaled by the inverse of the gradient variance, which can be suboptimal.</li> <li>MPNN Implementation:</li> <li>L1 Regularization follows the standard approach (added to gradients) to promote sparsity.</li> <li>L2 Regularization is implemented as Decoupled Weight Decay (AdamW). The decay is applied directly to the weights after the adaptive update step.</li> </ul> <p>This design means that by simply setting <code>optimizer=Adam(regularization='L2')</code>, you are effectively using AdamW, the state-of-the-art optimizer for training modern deep networks.</p>"},{"location":"INTERNALS/#2-smart-weight-initialization","title":"2. Smart Weight Initialization","text":"<p>One of the most common reasons for a neural network failing to converge is improper weight initialization.</p>"},{"location":"INTERNALS/#the-problem","title":"The Problem","text":"<ul> <li>Sigmoid/Tanh activations require weights to be small to avoid saturation (vanishing gradients).</li> <li>ReLU units require slightly larger weights to maintain variance (avoiding dead neurons).</li> </ul>"},{"location":"INTERNALS/#the-solution","title":"The Solution","text":"<p>The <code>Model</code> class performs a static analysis of your architecture before training begins. It looks at the connection between layers to decide the optimal initialization strategy.</p> <ul> <li>He Initialization (Kaiming): Applied if the layer is followed by <code>ReLU</code>, <code>PReLU</code>, or <code>Swish</code>.</li> <li>Xavier Initialization (Glorot): Applied if the layer is followed by <code>Sigmoid</code>, <code>Tanh</code>, or <code>Softmax</code>.</li> <li>Bias Disabling: If a <code>Dense</code> or <code>Convolutional</code> layer is immediately followed by <code>BatchNormalization</code>, the bias of the preceding layer is automatically disabled (set to <code>False</code>), as normalization makes it redundant.</li> </ul> <pre><code># Example of implicit automation\nmodel = Model([\n    Dense(100, input_size=784),  # Engine detects ReLU next -&gt; Uses He Init\n    ReLU(),\n    Dense(10)                    # Engine detects nothing/Softmax next -&gt; Uses Xavier\n])\n</code></pre>"},{"location":"INTERNALS/#3-training-loop-automation","title":"3. Training Loop Automation","text":"<p>The <code>Model.train()</code> method encapsulates a production-grade training loop with several automated features.</p>"},{"location":"INTERNALS/#auto-validation-split","title":"Auto-Validation Split","text":"<p>Instead of manually slicing your NumPy arrays, you can pass <code>auto_evaluation=0.2</code>.</p> <ul> <li>The engine shuffles the data.</li> <li>It reserves the last 20% for validation.</li> <li>It ensures no data leakage between training and validation sets.</li> </ul>"},{"location":"INTERNALS/#early-stopping-checkpointing","title":"Early Stopping &amp; Checkpointing","text":"<p>The model monitors the validation loss at every epoch.</p> <ul> <li>Patience: If the loss doesn't improve for <code>early_stopping</code> epochs, training halts to prevent overfitting.</li> <li>Best Weight Restoration: Crucially, the model keeps a copy of the weights that achieved the lowest validation loss. When training ends (naturally or via early stopping), these \"best weights\" are automatically restored. You never end up with the overfitted weights from the final epoch.</li> </ul>"},{"location":"INTERNALS/#4-backend-abstraction-cpugpu","title":"4. Backend Abstraction (CPU/GPU)","text":"<p>To support both CPU and GPU execution without code duplication, MPNeuralNetwork implements a unified backend interface.</p>"},{"location":"INTERNALS/#the-xp-abstraction","title":"The <code>xp</code> Abstraction","text":"<p>The library defines a global <code>xp</code> module alias that points to either:</p> <ul> <li><code>numpy</code> (for CPU execution)</li> <li><code>cupy</code> (for NVIDIA GPU execution)</li> </ul> <p>All tensor operations (creation, math, reshaping) use <code>xp.array</code>, <code>xp.dot</code>, etc., instead of hardcoded <code>np.*</code> calls.</p>"},{"location":"INTERNALS/#device-transfers","title":"Device Transfers","text":"<p>The <code>model</code> and <code>data</code> must reside on the same device.</p> <ul> <li><code>to_device(array)</code>: Moves a NumPy array to the configured device (GPU if enabled).</li> <li><code>to_host(array)</code>: Moves a device array back to CPU (NumPy) for printing or saving.</li> </ul>"},{"location":"INTERNALS/#5-backend-optimizations","title":"5. Backend Optimizations","text":""},{"location":"INTERNALS/#vectorization-im2col","title":"Vectorization &amp; <code>im2col</code>","text":"<p>Python loops are too slow for Deep Learning. MPNN vectorizes operations to leverage BLAS/LAPACK routines via NumPy.</p> <ul> <li>Batch Processing: All layers operate on 3D/4D tensors <code>(Batch_Size, ...)</code>, eliminating the outer loop over samples.</li> <li>Convolution via <code>im2col</code>:</li> <li>2D Convolutions are difficult to vectorize directly.</li> <li>Solution: implementation of <code>im2col</code> (Image to Column), which stretches image patches into columns.</li> <li>This transforms the convolution operation into a single, massive Matrix Multiplication (<code>GEMM</code>).</li> <li>Result: Orders of magnitude faster than iterative sliding windows.</li> </ul>"},{"location":"INTERNALS/#memory-management-float32","title":"Memory Management (Float32)","text":"<p>By default, Python and NumPy use <code>float64</code> (double precision). Deep Learning rarely needs this precision.</p> <ul> <li>The framework enforces <code>DTYPE = np.float32</code> globally.</li> <li>Inputs are automatically cast to float32.</li> <li>This reduces RAM usage by 50% and doubles memory bandwidth throughput.</li> </ul>"},{"location":"INTERNALS/#5-numerical-stability","title":"5. Numerical Stability","text":""},{"location":"INTERNALS/#logits-handling","title":"Logits Handling","text":"<p>Calculating <code>Softmax</code> then <code>CrossEntropy</code> separately is numerically unstable (can lead to <code>NaN</code> or <code>Infinity</code> due to exponentials).</p> <ul> <li>MPNN uses the \"Logits\" pattern.</li> <li>The <code>CategoricalCrossEntropy</code> and <code>BinaryCrossEntropy</code> losses expect raw outputs (logits) from the final layer, not probabilities.</li> <li>They internally compute the loss using the log-sum-exp trick or equivalent stable formulas.</li> </ul> <p>Note: When you call <code>model.predict()</code>, the engine automatically applies the final activation (Softmax/Sigmoid) so you get human-readable probabilities.</p>"},{"location":"INTERNALS/#6-smart-metrics","title":"6. Smart Metrics","text":"<p>To reduce boilerplate, the framework automatically assigns relevant metrics if the user doesn't specify any.</p> <ul> <li>Regression (MSE): Automatically tracks <code>RMSE</code> and <code>R2Score</code>.</li> <li>Classification (CrossEntropy): Automatically tracks <code>Accuracy</code> and <code>F1Score</code>.</li> </ul>"},{"location":"OPTIMIZATION_GUIDE/","title":"Optimization &amp; Performance Guide","text":"<p>MPNeuralNetwork is designed to be performant, but achieving optimal speed requires understanding how to configure the framework correctly.</p>"},{"location":"OPTIMIZATION_GUIDE/#1-data-precision-float32","title":"1. Data Precision (Float32)","text":"<p>Rule #1: Use Float32.</p> <p>By default, Python and NumPy use <code>float64</code> (double precision). Deep Learning rarely benefits from this extra precision, but it costs 2x memory and ~2x bandwidth.</p> <p>The framework enforces <code>float32</code> internally (<code>DTYPE = np.float32</code>), but you should ensure your input data is cast before feeding it to the model to avoid on-the-fly conversion overhead.</p> <pre><code># Default float64\nX_train = np.random.randn(1000, 784)\n\n# Explicit float32\nX_train = np.random.randn(1000, 784).astype(np.float32)\n</code></pre>"},{"location":"OPTIMIZATION_GUIDE/#2-batch-size-selection","title":"2. Batch Size Selection","text":"<p>Choosing the right batch size is a trade-off between convergence stability and hardware utilization.</p> <ul> <li>Too Small (1-16): High overhead due to Python loops. The vectorization engine (BLAS/LAPACK) is starved of data.</li> <li>Too Large (2048+): Can lead to generalization issues and out-of-memory (OOM) errors.</li> <li>Optimal (32-512): Typically, powers of 2 like 32, 64, 128, or 256 provide the best balance.</li> </ul>"},{"location":"OPTIMIZATION_GUIDE/#3-hardware-acceleration-gpu","title":"3. Hardware Acceleration (GPU)","text":"<p>MPNN supports NVIDIA GPUs via CuPy. This provides massive speedups for large matrix multiplications (Dense layers) and Convolutions.</p>"},{"location":"OPTIMIZATION_GUIDE/#prerequisites","title":"Prerequisites","text":"<ul> <li>NVIDIA GPU</li> <li>CUDA Toolkit installed</li> <li><code>cupy</code> python package installed matching your CUDA version (e.g., <code>pip install cupy-cuda12x</code>)</li> </ul> <p>Note: Check your CUDA version with <code>nvcc --version</code> and install the corresponding package listed in the CuPy Installation Guide.</p>"},{"location":"OPTIMIZATION_GUIDE/#enabling-gpu-mode","title":"Enabling GPU Mode","text":"<p>Set the environment variable <code>MPNN_BACKEND</code> before running your script.</p> <pre><code># Run on GPU\nexport MPNN_BACKEND=cupy\npython my_script.py\n</code></pre> <p>Or inside Python (before importing <code>mpneuralnetwork</code>):</p> <pre><code>import os\nos.environ[\"MPNN_BACKEND\"] = \"cupy\"\nimport mpneuralnetwork\n</code></pre>"},{"location":"OPTIMIZATION_GUIDE/#4-benchmarking","title":"4. Benchmarking","text":"<p>The project includes a robust benchmarking suite to measure performance improvements or regressions.</p>"},{"location":"OPTIMIZATION_GUIDE/#running-benchmarks","title":"Running Benchmarks","text":"<p>Benchmarks are located in the <code>benchmark/</code> directory. The runner script executes them and profiles both time and memory.</p> <pre><code>python benchmark/run_benchmarks.py\n</code></pre> <p>This will generate reports in <code>output/benchmark_TIMESTAMP/</code>:</p> <ul> <li><code>*.prof</code>: CPU profile data.</li> <li><code>*.bin</code>: Memory usage data (Memray).</li> <li><code>flamegraph.html</code>: Interactive memory usage visualization.</li> </ul>"},{"location":"OPTIMIZATION_GUIDE/#analyzing-results","title":"Analyzing Results","text":"<p>Use <code>snakeviz</code> to visualize CPU bottlenecks:</p> <pre><code>snakeviz output/benchmark_.../cpu_profile.prof\n</code></pre>"},{"location":"OPTIMIZATION_GUIDE/#5-common-bottlenecks","title":"5. Common Bottlenecks","text":""},{"location":"OPTIMIZATION_GUIDE/#im2col-memory-usage","title":"<code>im2col</code> Memory Usage","text":"<p>Convolutional layers use <code>im2col</code> to vectorize operations. This expands the input image into a large matrix.</p> <ul> <li>Impact: Memory usage grows by factor of $K^2$ (Kernel Size squared).</li> <li>Mitigation:<ul> <li>Reduce the <code>batch_size</code>.</li> <li>Use smaller kernels (e.g., 3x3 instead of 5x5).</li> <li>Use Strides: Increasing <code>stride</code> (e.g., <code>stride=2</code>) drastically reduces the output spatial dimensions and the size of the intermediate <code>im2col</code> matrix, saving both memory and compute.</li> </ul> </li> </ul>"},{"location":"OPTIMIZATION_GUIDE/#data-copying","title":"Data Copying","text":"<p>The framework tries to minimize copies, but some operations (like <code>flatten</code> or <code>transpose</code> on non-contiguous arrays) force a copy.</p> <ul> <li>Tip: Ensure your data is C-contiguous if you are doing manual pre-processing: <code>x = np.ascontiguousarray(x)</code>.</li> </ul>"},{"location":"QUALITY_GUIDE/","title":"Code Quality &amp; Contribution Guide","text":"<p>This guide outlines the standards, tools, and workflows used in MPNeuralNetwork to ensure a robust, maintainable, and high-quality codebase.</p>"},{"location":"QUALITY_GUIDE/#1-quality-standards-tools","title":"1. Quality Standards &amp; Tools","text":"<p>We rely on a strict set of tools to enforce quality automatically.</p> Tool Purpose Scope Ruff Linter &amp; Formatter. Replaces Flake8, Black, and Isort. Entire project (<code>src</code>, <code>tests</code>, <code>examples</code>) Mypy Static Type Checker. Strictly enforced on <code>src/mpneuralnetwork</code> Pytest Unit Testing framework. <code>tests/</code> Coverage Code coverage measurement. <code>src/mpneuralnetwork</code> (must cover &gt;90%) MkDocs Documentation Generator. <code>docs/</code>"},{"location":"QUALITY_GUIDE/#2-local-development-workflow","title":"2. Local Development Workflow","text":""},{"location":"QUALITY_GUIDE/#installation","title":"Installation","text":"<p>Install the project in editable mode with all development dependencies:</p> <pre><code>pip install -e .[dev,test,docs]\npre-commit install\n</code></pre>"},{"location":"QUALITY_GUIDE/#routine-commands","title":"Routine Commands","text":"<p>During development, you should frequently run these commands to ensure your code meets the standards.</p>"},{"location":"QUALITY_GUIDE/#format-lint-ruff","title":"Format &amp; Lint (Ruff)","text":"<p>Fixes style issues and imports automatically.</p> <pre><code>ruff check . --fix\nruff format .\n</code></pre>"},{"location":"QUALITY_GUIDE/#type-checking-mypy","title":"Type Checking (Mypy)","text":"<p>Verifies type safety. Configuration is loaded from <code>pyproject.toml</code>.</p> <pre><code>mypy\n</code></pre>"},{"location":"QUALITY_GUIDE/#run-tests-pytest","title":"Run Tests (Pytest)","text":"<p>Executes the test suite and reports coverage.</p> <pre><code>coverage run -m pytest\ncoverage report -m\n</code></pre>"},{"location":"QUALITY_GUIDE/#preview-documentation","title":"Preview Documentation","text":"<p>To preview the documentation site locally as you edit markdown files:</p> <pre><code>mkdocs serve\n</code></pre> <p>Open <code>http://127.0.0.1:8000</code> in your browser.</p>"},{"location":"QUALITY_GUIDE/#3-git-workflow-commit-convention","title":"3. Git Workflow &amp; Commit Convention","text":"<p>To automate versioning and changelog generation, we strictly adhere to the Conventional Commits specification.</p>"},{"location":"QUALITY_GUIDE/#commit-message-format","title":"Commit Message Format","text":"<pre><code>&lt;type&gt;[optional scope]: &lt;description&gt;\n\n[optional body]\n\n[optional footer(s)]\n</code></pre>"},{"location":"QUALITY_GUIDE/#allowed-types","title":"Allowed Types","text":"Type Meaning Version Impact Example feat A new feature MINOR (<code>1.1.0</code>) <code>feat(layer): add LSTM support</code> fix A bug fix PATCH (<code>1.0.1</code>) <code>fix(optim): resolve div by zero in Adam</code> docs Documentation only None <code>docs: update installation guide</code> style Formatting, missing semi-colons, etc. None <code>style: format with ruff</code> refactor Code change that is neither fix nor feat None <code>refactor: simplify activation logic</code> perf A code change that improves performance None <code>perf: vectorize loss calculation</code> test Adding or correcting tests None <code>test: add unit tests for Conv2D</code> chore Build process or aux tool changes None <code>chore: update dependencies</code> <p>BREAKING CHANGES: Adding <code>BREAKING CHANGE:</code> in the footer or appending <code>!</code> after the type/scope triggers a MAJOR version bump (<code>2.0.0</code>).</p>"},{"location":"QUALITY_GUIDE/#4-release-process-cicd","title":"4. Release Process (CI/CD)","text":"<p>We use python-semantic-release to fully automate the release cycle.</p>"},{"location":"QUALITY_GUIDE/#how-it-works","title":"How it works","text":"<ol> <li>Push to Main: You merge a Pull Request or push code to the <code>main</code> branch.</li> <li>CI Checks: GitHub Actions triggers the <code>Tests</code> workflow (Lint + Types + Pytest).</li> <li>Release Decision:<ul> <li>If tests pass: The release workflow analyzes new commits since the last tag.</li> <li>If commits contain <code>feat</code> or <code>fix</code>: A new version is calculated.</li> </ul> </li> <li>Publication:<ul> <li><code>pyproject.toml</code> is updated with the new version.</li> <li>A <code>CHANGELOG.md</code> is generated or updated.</li> <li>A Git Tag is created.</li> <li>The package is built and uploaded to PyPI.</li> </ul> </li> </ol>"},{"location":"ROADMAP/","title":"Project Roadmap","text":"<p>This roadmap outlines the planned improvements and features for <code>mp-neural-network</code>.</p> <p>Note: This roadmap is being progressively migrated to our GitHub Project. Please refer to the project board for the most up-to-date planning and status.</p>"},{"location":"ROADMAP/#upcoming-features","title":"Upcoming Features","text":"<ul> <li>RNN/LSTM Layers: Support for sequential data.</li> <li>Channels last optimization: Optimize convolutional layers for channels_last format.</li> <li>Advanced Schedulers: Implement Learning Rate Schedulers (ReduceLROnPlateau, CosineAnnealing).</li> <li>Data Loaders: Create a generator-based <code>DataLoader</code> for training on datasets larger than RAM (or VRAM).</li> <li>Stable parameter identification system: Remove dependency on <code>id(param)</code> for tracking optimizer states (moments/velocities), implement a more robust approach (e.g., persistent UUIDs or structured naming).</li> <li>Explicit Shape Checking: Add clear error messages when connecting incompatible layers</li> </ul>"},{"location":"ROADMAP/#completed-features","title":"Completed Features","text":""},{"location":"ROADMAP/#v11","title":"v1.1","text":"<ul> <li>GPU Acceleration: Explore optional CuPy backend for NVIDIA GPU support.</li> <li>Advanced Documentation: Add Google-style docstrings to all public classes</li> <li>CNN Padding and strides: Support for padding and strides in convolutional layers.</li> <li>Softmax temperature: Support for temperature scaling in softmax.</li> </ul>"},{"location":"ROADMAP/#v10","title":"v1.0","text":"<ul> <li>Float32 Transition: Global <code>DTYPE = np.float32</code> enforced to reduce RAM by ~50%.</li> <li>In-Place Operations: Eliminated redundant copies in <code>train</code>, <code>predict</code>, and <code>evaluate</code>.</li> <li>Optimized Shuffle: Shuffling indices instead of copying the full dataset.</li> <li>Lazy Metrics: Metrics are computed only when necessary, speeding up training.</li> <li>Batch Vectorization</li> <li>Numerical Stability Fixes (Logits)</li> <li>Advanced Optimizers: Adam, RMSprop, SGD Momentum.</li> <li>Smart Initialization: Auto He/Xavier.</li> <li>Regularization: Dropout Layer &amp; L1/L2 Weight Decay.</li> <li>Convolutional Layers: Conv2D implementation with <code>im2col</code>.</li> <li>Model Serialization: Saving/Loading weights to JSON/Pickle.</li> <li>Training Utils: Early Stopping, Checkpointing, Auto-Metrics.</li> <li>Pooling Layers: MaxPool / AvgPool.</li> <li>BatchNormalization: 1D (Dense) and 2D (Spatial/CNN).</li> <li>Memory Optimization: In-place operations, reduced copies, global float32.</li> <li>Performance Boost: Training loop optimization (index shuffling) and type enforcement (~26% speedup).</li> </ul>"},{"location":"USER_GUIDE/","title":"User Guide: Getting Started","text":"<p>This tutorial will walk you through the essential concepts of the framework, from installation to building your first Deep Learning models.</p>"},{"location":"USER_GUIDE/#1-installation","title":"1. Installation","text":"<p>First, ensure you have Python 3.10 or later installed.</p>"},{"location":"USER_GUIDE/#from-pypi-stable","title":"From PyPI (Stable)","text":"<pre><code>pip install mpneuralnetwork\n</code></pre>"},{"location":"USER_GUIDE/#from-source-development","title":"From Source (Development)","text":"<pre><code>git clone https://github.com/maximepires4/mp-neural-network.git\ncd mp-neural-network\npip install -e .\n</code></pre>"},{"location":"USER_GUIDE/#2-hardware-acceleration-gpu","title":"2. Hardware Acceleration (GPU)","text":"<p>MPNeuralNetwork supports GPU acceleration using CuPy.</p>"},{"location":"USER_GUIDE/#requirements","title":"Requirements","text":"<ul> <li>NVIDIA GPU</li> <li>CUDA Toolkit installed</li> <li><code>cupy</code> installed (<code>pip install cupy-cuda12x</code> depending on your CUDA version)</li> </ul>"},{"location":"USER_GUIDE/#usage","title":"Usage","text":"<p>To enable GPU mode, simply set the <code>MPNN_BACKEND</code> environment variable before running your script:</p> <pre><code># Linux / macOS\nexport MPNN_BACKEND=cupy\npython my_script.py\n</code></pre> <p>If CuPy is not found or the variable is not set, it falls back to NumPy (CPU).</p>"},{"location":"USER_GUIDE/#3-core-concepts","title":"3. Core Concepts","text":"<p>MPNeuralNetwork exposes some of the mathematical machinery while keeping the API clean.</p>"},{"location":"USER_GUIDE/#the-model-container","title":"The <code>Model</code> Container","text":"<p>Everything revolves around the <code>Model</code> class. It manages:</p> <ol> <li>Layers: The sequence of transformations (Dense, Conv2D, etc.).</li> <li>Loss Function: How error is calculated (MSE, CrossEntropy).</li> <li>Optimizer: How weights are updated (SGD, Adam).</li> </ol>"},{"location":"USER_GUIDE/#data-format","title":"Data Format","text":"<p>The framework uses NumPy arrays (or CuPy if on GPU).</p> <ul> <li>Inputs (X): Must be float32 arrays of shape <code>(batch_size, features...)</code>.</li> <li>Targets (y):</li> <li>Regression: <code>(batch_size, outputs)</code>.</li> <li>Classification: One-hot encoded <code>(batch_size, num_classes)</code>.</li> </ul>"},{"location":"USER_GUIDE/#3-tutorial-1-your-first-neural-network-mnist","title":"3. Tutorial 1: Your First Neural Network (MNIST)","text":"<p>Let's build a classic Multi-Layer Perceptron (MLP) to classify handwritten digits from the MNIST dataset.</p>"},{"location":"USER_GUIDE/#step-1-prepare-data","title":"Step 1: Prepare Data","text":"<pre><code>import numpy as np\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.preprocessing import OneHotEncoder\n\n# 1. Load Data\nmnist = fetch_openml('mnist_784', version=1)\nX = mnist.data.to_numpy() / 255.0  # Normalize to [0, 1]\ny = mnist.target.to_numpy().reshape(-1, 1)\n\n# 2. One-Hot Encode Labels\nencoder = OneHotEncoder(sparse_output=False)\ny = encoder.fit_transform(y)\n\n# 3. Split Train/Test\nX_train, X_test = X[:60000], X[60000:]\ny_train, y_test = y[:60000], y[60000:]\n</code></pre>"},{"location":"USER_GUIDE/#step-2-define-architecture","title":"Step 2: Define Architecture","text":"<p>We will build a network with:</p> <ul> <li>Input: 784 features (28x28 pixels flattened)</li> <li>Hidden Layer 1: 128 neurons + ReLU + Dropout</li> <li>Output Layer: 10 neurons (digits 0-9)</li> </ul> <pre><code>from mpneuralnetwork.layers import Dense, Dropout\nfrom mpneuralnetwork.activations import ReLU\nfrom mpneuralnetwork.losses import CategoricalCrossEntropy\nfrom mpneuralnetwork.optimizers import Adam\nfrom mpneuralnetwork.model import Model\n\nnetwork = [\n    Dense(128, input_size=784),  # Weights auto-initialized (He Init)\n    ReLU(),\n    Dropout(0.2),                # 20% dropout during training\n    Dense(10)                    # Output logits (no Softmax here!)\n]\n\nmodel = Model(\n    layers=network,\n    loss=CategoricalCrossEntropy(),  # Handles Softmax internally\n    optimizer=Adam(learning_rate=0.001),\n    metrics=[] # Accuracy is auto-added\n)\n</code></pre>"},{"location":"USER_GUIDE/#step-3-train","title":"Step 3: Train","text":"<p>We use the <code>train()</code> method. Note the <code>auto_evaluation</code> parameter which automatically creates a validation set.</p> <pre><code>model.train(\n    X_train, y_train,\n    epochs=20,\n    batch_size=64,\n    auto_evaluation=0.2,  # Use 20% of data for validation\n    early_stopping=3      # Stop if no improvement for 3 epochs\n)\n</code></pre>"},{"location":"USER_GUIDE/#step-4-evaluate-predict","title":"Step 4: Evaluate &amp; Predict","text":"<pre><code># Check performance on test set\nmodel.test(X_test, y_test)\n\n# Make predictions\npreds = model.predict(X_test[:5])\nprint(\"Predicted probabilities:\", preds)\n</code></pre>"},{"location":"USER_GUIDE/#4-tutorial-2-convolutional-neural-network-cnn","title":"4. Tutorial 2: Convolutional Neural Network (CNN)","text":"<p>For image data, CNNs are superior. MPNeuralNetwork supports 2D convolutions using the optimized <code>im2col</code> technique.</p> <p>Note on Shapes: CNNs expect 4D input tensors: <code>(Batch, Channels, Height, Width)</code>.</p> <pre><code># Reshape flat MNIST data to 4D\nX_train_cnn = X_train.reshape(-1, 1, 28, 28)\nX_test_cnn = X_test.reshape(-1, 1, 28, 28)\n</code></pre>"},{"location":"USER_GUIDE/#defining-the-cnn","title":"Defining the CNN","text":"<pre><code>from mpneuralnetwork.layers import Convolutional, Flatten, MaxPooling2D, BatchNormalization2D\n\ncnn_network = [\n    # Conv1: 1 input channel -&gt; 32 filters of size 3x3\n    # stride=1 and padding=1 preserves spatial dimensions (28x28)\n    Convolutional(output_depth=32, kernel_size=3, input_shape=(1, 28, 28), stride=1, padding=1),\n    BatchNormalization2D(),\n    ReLU(),\n\n    # Pool1: Downsample by 2 (14x14)\n    MaxPooling2D(pool_size=2, strides=2),\n\n    # Conv2: 32 input channels -&gt; 64 filters\n    Convolutional(output_depth=64, kernel_size=3, stride=1, padding=1),\n    BatchNormalization2D(),\n    ReLU(),\n\n    # Pool2: Downsample by 2 (7x7)\n    MaxPooling2D(pool_size=2, strides=2),\n\n    # Flatten: Convert 3D feature maps to 1D vector\n    Flatten(),\n\n    # Dense Classifier\n    Dense(100),\n    ReLU(),\n    Dense(10)\n]\n\nmodel_cnn = Model(\n    layers=cnn_network,\n    loss=CategoricalCrossEntropy(),\n    optimizer=Adam(learning_rate=0.001)\n)\n\nmodel_cnn.train(X_train_cnn, y_train, epochs=10, batch_size=32)\n</code></pre>"},{"location":"USER_GUIDE/#5-saving-and-loading","title":"5. Saving and Loading","text":"<p>You can save your trained model to disk to use it later without retraining.</p> <pre><code>from mpneuralnetwork.serialization import save_model, load_model\n\n# Save full model (architecture + weights + optimizer state)\nsave_model(model, \"my_mnist_model.npz\")\n\n# Load it back\nloaded_model = load_model(\"my_mnist_model.npz\")\n\n# Resume training or predict immediately\nloaded_model.predict(X_test[:1])\n</code></pre>"},{"location":"USER_GUIDE/#6-next-steps","title":"6. Next Steps","text":"<ul> <li>Learn about performance tuning in the Optimization Guide.</li> <li>Explore the full API Reference.</li> </ul>"},{"location":"reference/activations/","title":"Activations API","text":"<p>Activation functions introduce non-linearity into the network, allowing it to learn complex patterns.</p>"},{"location":"reference/activations/#base-activation","title":"Base Activation","text":""},{"location":"reference/activations/#mpneuralnetwork.activations.Activation","title":"<code>mpneuralnetwork.activations.Activation</code>","text":"<p>               Bases: <code>Layer</code></p> <p>Base class for activation functions.</p> <p>Activations are treated as layers in this framework. They apply a non-linear transformation element-wise to the input.</p> <p>Attributes:</p> Name Type Description <code>activation</code> <code>Callable</code> <p>The function to apply during the forward pass.</p> <code>activation_prime</code> <code>Callable</code> <p>The derivative of the function for the backward pass.</p> Source code in <code>src/mpneuralnetwork/activations.py</code> <pre><code>class Activation(Layer):\n    \"\"\"Base class for activation functions.\n\n    Activations are treated as layers in this framework. They apply a non-linear\n    transformation element-wise to the input.\n\n    Attributes:\n        activation (Callable): The function to apply during the forward pass.\n        activation_prime (Callable): The derivative of the function for the backward pass.\n    \"\"\"\n\n    def __init__(self, activation: T, activation_prime: T) -&gt; None:\n        \"\"\"Initializes the activation layer.\n\n        Args:\n            activation (Callable[[ArrayType], ArrayType]): The activation function.\n            activation_prime (Callable[[ArrayType], ArrayType]): The derivative of the activation function.\n        \"\"\"\n        self.activation: T = activation\n        self.activation_prime: T = activation_prime\n\n    def forward(self, input_batch: ArrayType, training: bool = True) -&gt; ArrayType:\n        \"\"\"Applies the activation function to the input.\n\n        Args:\n            input_batch (ArrayType): Input data of any shape.\n            training (bool, optional): Whether in training mode. Defaults to True.\n\n        Returns:\n            ArrayType: Activated output with the same shape as `input_batch`.\n        \"\"\"\n        self.input = input_batch\n        return self.activation(self.input)\n\n    def backward(self, output_gradient_batch: ArrayType) -&gt; ArrayType:\n        \"\"\"Computes the gradient of the activation function.\n\n        Applies the chain rule: `grad = output_gradient * activation'(input)`.\n\n        Args:\n            output_gradient_batch (ArrayType): Gradient flowing from the next layer.\n\n        Returns:\n            ArrayType: Gradient with respect to the input.\n        \"\"\"\n        res: ArrayType = xp.multiply(output_gradient_batch, self.activation_prime(self.input))\n        return res\n\n    @property\n    def params(self) -&gt; dict[str, tuple[ArrayType, ArrayType]]:\n        \"\"\"Activations usually have no trainable parameters.\n\n        Returns:\n            dict: Empty dictionary.\n        \"\"\"\n        return {}\n</code></pre>"},{"location":"reference/activations/#mpneuralnetwork.activations.Activation.params","title":"<code>params</code>  <code>property</code>","text":"<p>Activations usually have no trainable parameters.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict[str, tuple[ArrayType, ArrayType]]</code> <p>Empty dictionary.</p>"},{"location":"reference/activations/#mpneuralnetwork.activations.Activation.__init__","title":"<code>__init__(activation, activation_prime)</code>","text":"<p>Initializes the activation layer.</p> <p>Parameters:</p> Name Type Description Default <code>activation</code> <code>Callable[[ArrayType], ArrayType]</code> <p>The activation function.</p> required <code>activation_prime</code> <code>Callable[[ArrayType], ArrayType]</code> <p>The derivative of the activation function.</p> required Source code in <code>src/mpneuralnetwork/activations.py</code> <pre><code>def __init__(self, activation: T, activation_prime: T) -&gt; None:\n    \"\"\"Initializes the activation layer.\n\n    Args:\n        activation (Callable[[ArrayType], ArrayType]): The activation function.\n        activation_prime (Callable[[ArrayType], ArrayType]): The derivative of the activation function.\n    \"\"\"\n    self.activation: T = activation\n    self.activation_prime: T = activation_prime\n</code></pre>"},{"location":"reference/activations/#mpneuralnetwork.activations.Activation.backward","title":"<code>backward(output_gradient_batch)</code>","text":"<p>Computes the gradient of the activation function.</p> <p>Applies the chain rule: <code>grad = output_gradient * activation'(input)</code>.</p> <p>Parameters:</p> Name Type Description Default <code>output_gradient_batch</code> <code>ArrayType</code> <p>Gradient flowing from the next layer.</p> required <p>Returns:</p> Name Type Description <code>ArrayType</code> <code>ArrayType</code> <p>Gradient with respect to the input.</p> Source code in <code>src/mpneuralnetwork/activations.py</code> <pre><code>def backward(self, output_gradient_batch: ArrayType) -&gt; ArrayType:\n    \"\"\"Computes the gradient of the activation function.\n\n    Applies the chain rule: `grad = output_gradient * activation'(input)`.\n\n    Args:\n        output_gradient_batch (ArrayType): Gradient flowing from the next layer.\n\n    Returns:\n        ArrayType: Gradient with respect to the input.\n    \"\"\"\n    res: ArrayType = xp.multiply(output_gradient_batch, self.activation_prime(self.input))\n    return res\n</code></pre>"},{"location":"reference/activations/#mpneuralnetwork.activations.Activation.forward","title":"<code>forward(input_batch, training=True)</code>","text":"<p>Applies the activation function to the input.</p> <p>Parameters:</p> Name Type Description Default <code>input_batch</code> <code>ArrayType</code> <p>Input data of any shape.</p> required <code>training</code> <code>bool</code> <p>Whether in training mode. Defaults to True.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>ArrayType</code> <code>ArrayType</code> <p>Activated output with the same shape as <code>input_batch</code>.</p> Source code in <code>src/mpneuralnetwork/activations.py</code> <pre><code>def forward(self, input_batch: ArrayType, training: bool = True) -&gt; ArrayType:\n    \"\"\"Applies the activation function to the input.\n\n    Args:\n        input_batch (ArrayType): Input data of any shape.\n        training (bool, optional): Whether in training mode. Defaults to True.\n\n    Returns:\n        ArrayType: Activated output with the same shape as `input_batch`.\n    \"\"\"\n    self.input = input_batch\n    return self.activation(self.input)\n</code></pre>"},{"location":"reference/activations/#hidden-layers","title":"Hidden Layers","text":"<p>These activations are typically used in intermediate layers.</p>"},{"location":"reference/activations/#mpneuralnetwork.activations.ReLU","title":"<code>mpneuralnetwork.activations.ReLU</code>","text":"<p>               Bases: <code>Activation</code></p> <p>Rectified Linear Unit activation function.</p> Formula <p><code>f(x) = max(0, x)</code></p> <p>Range: [0, inf). Computationally efficient and mitigates the vanishing gradient problem. Most common activation for hidden layers in deep networks.</p> Source code in <code>src/mpneuralnetwork/activations.py</code> <pre><code>class ReLU(Activation):\n    \"\"\"Rectified Linear Unit activation function.\n\n    Formula:\n        `f(x) = max(0, x)`\n\n    Range: [0, inf).\n    Computationally efficient and mitigates the vanishing gradient problem.\n    Most common activation for hidden layers in deep networks.\n    \"\"\"\n\n    def __init__(self) -&gt; None:\n        super().__init__(lambda x: xp.maximum(0, x, dtype=DTYPE), lambda x: x &gt; 0)\n</code></pre>"},{"location":"reference/activations/#mpneuralnetwork.activations.PReLU","title":"<code>mpneuralnetwork.activations.PReLU</code>","text":"<p>               Bases: <code>Activation</code></p> <p>Parametric Rectified Linear Unit.</p> Formula <p><code>f(x) = x</code> if <code>x &gt; 0</code> <code>f(x) = alpha * x</code> if <code>x &lt;= 0</code></p> <p>Where <code>alpha</code> is a learnable parameter updated during training. Allows the network to learn the negative slope, avoiding \"dying ReLU\" problems.</p> <p>Parameters:</p> Name Type Description Default <code>alpha</code> <code>float</code> <p>Initial value for the negative slope. Defaults to 0.01.</p> <code>0.01</code> Source code in <code>src/mpneuralnetwork/activations.py</code> <pre><code>class PReLU(Activation):\n    \"\"\"Parametric Rectified Linear Unit.\n\n    Formula:\n        `f(x) = x` if `x &gt; 0`\n        `f(x) = alpha * x` if `x &lt;= 0`\n\n    Where `alpha` is a learnable parameter updated during training.\n    Allows the network to learn the negative slope, avoiding \"dying ReLU\" problems.\n\n    Args:\n        alpha (float, optional): Initial value for the negative slope. Defaults to 0.01.\n    \"\"\"\n\n    def __init__(self, alpha: float = 0.01) -&gt; None:\n        super().__init__(\n            lambda x: xp.maximum(alpha * x, x, dtype=DTYPE),\n            lambda x: xp.where(x &lt; 0, alpha, 1),\n        )\n        self.alpha: float = alpha\n\n    def get_config(self) -&gt; dict:\n        config = super().get_config()\n        config.update({\"alpha\": self.alpha})\n        return config\n</code></pre>"},{"location":"reference/activations/#mpneuralnetwork.activations.Swish","title":"<code>mpneuralnetwork.activations.Swish</code>","text":"<p>               Bases: <code>Activation</code></p> <p>Swish activation function.</p> Formula <p><code>f(x) = x * sigmoid(x)</code></p> <p>Range: (~-0.28, inf). Proposed by Google. A smooth, non-monotonic function that often outperforms ReLU on deep networks.</p> Source code in <code>src/mpneuralnetwork/activations.py</code> <pre><code>class Swish(Activation):\n    \"\"\"Swish activation function.\n\n    Formula:\n        `f(x) = x * sigmoid(x)`\n\n    Range: (~-0.28, inf).\n    Proposed by Google. A smooth, non-monotonic function that often outperforms ReLU\n    on deep networks.\n    \"\"\"\n\n    def __init__(self) -&gt; None:\n        super().__init__(\n            lambda x: x / (1 + xp.exp(-x, dtype=DTYPE)),\n            lambda x: (1 + xp.exp(-x, dtype=DTYPE) + x * xp.exp(-x, dtype=DTYPE)) / (1 + xp.exp(-x, dtype=DTYPE)) ** 2,\n        )\n</code></pre>"},{"location":"reference/activations/#mpneuralnetwork.activations.Tanh","title":"<code>mpneuralnetwork.activations.Tanh</code>","text":"<p>               Bases: <code>Activation</code></p> <p>Hyperbolic Tangent activation function.</p> Formula <p><code>f(x) = tanh(x) = (exp(x) - exp(-x)) / (exp(x) + exp(-x))</code></p> <p>Range: (-1, 1). Zero-centered, making it often preferable to Sigmoid for hidden layers.</p> Source code in <code>src/mpneuralnetwork/activations.py</code> <pre><code>class Tanh(Activation):\n    \"\"\"Hyperbolic Tangent activation function.\n\n    Formula:\n        `f(x) = tanh(x) = (exp(x) - exp(-x)) / (exp(x) + exp(-x))`\n\n    Range: (-1, 1).\n    Zero-centered, making it often preferable to Sigmoid for hidden layers.\n    \"\"\"\n\n    def __init__(self) -&gt; None:\n        super().__init__(\n            lambda x: xp.tanh(x, dtype=DTYPE),\n            lambda x: (1 - xp.tanh(x, dtype=DTYPE) ** 2),\n        )\n</code></pre>"},{"location":"reference/activations/#mpneuralnetwork.activations.Sigmoid","title":"<code>mpneuralnetwork.activations.Sigmoid</code>","text":"<p>               Bases: <code>Activation</code></p> <p>Sigmoid activation function.</p> Formula <p><code>f(x) = 1 / (1 + exp(-x))</code></p> <p>Range: (0, 1). Used for binary classification (output layer) or gating mechanisms (like in LSTMs). Can suffer from vanishing gradients in deep networks.</p> Source code in <code>src/mpneuralnetwork/activations.py</code> <pre><code>class Sigmoid(Activation):\n    \"\"\"Sigmoid activation function.\n\n    Formula:\n        `f(x) = 1 / (1 + exp(-x))`\n\n    Range: (0, 1).\n    Used for binary classification (output layer) or gating mechanisms (like in LSTMs).\n    Can suffer from vanishing gradients in deep networks.\n    \"\"\"\n\n    def __init__(self) -&gt; None:\n        def sigmoid(x: ArrayType) -&gt; ArrayType:\n            return 1 / (1 + xp.exp(-x, dtype=DTYPE))  # type: ignore[no-any-return]\n\n        super().__init__(lambda x: sigmoid(x), lambda x: sigmoid(x) * (1 - sigmoid(x)))\n</code></pre>"},{"location":"reference/activations/#output-layers","title":"Output Layers","text":"<p>These activations are typically used in the final layer to produce probability distributions.</p>"},{"location":"reference/activations/#mpneuralnetwork.activations.Softmax","title":"<code>mpneuralnetwork.activations.Softmax</code>","text":"<p>               Bases: <code>Layer</code></p> <p>Softmax activation function.</p> Formula <p><code>f(x)_i = exp(x_i / T) / sum(exp(x_j / T))</code></p> <p>Typically used in the output layer for multi-class classification. Converts a vector of K real numbers into a probability distribution of K possible outcomes. The temperature parameter T is used to scale the logits before computing the softmax.</p> Source code in <code>src/mpneuralnetwork/activations.py</code> <pre><code>class Softmax(Layer):\n    \"\"\"Softmax activation function.\n\n    Formula:\n        `f(x)_i = exp(x_i / T) / sum(exp(x_j / T))`\n\n    Typically used in the output layer for multi-class classification.\n    Converts a vector of K real numbers into a probability distribution of K possible outcomes.\n    The temperature parameter T is used to scale the logits before computing the softmax.\n    \"\"\"\n\n    def __init__(self, temperature: float = 1.0, epsilon: float = 1e-8) -&gt; None:\n        \"\"\"Initializes the Softmax layer.\n\n        Args:\n            temperature (float, optional): Temperature parameter. Defaults to 1.0.\n            epsilon (float, optional): Small float added to denominator to avoid dividing by zero. Defaults to 1e-8.\n        \"\"\"\n        self.temperature: float = temperature\n        self.epsilon: float = epsilon\n\n    def forward(self, input_batch: ArrayType, training: bool = True) -&gt; ArrayType:\n        \"\"\"Applies Softmax function.\n\n        Args:\n            input_batch (ArrayType): Input logits of shape (batch_size, num_classes).\n            training (bool, optional): Unused. Defaults to True.\n\n        Returns:\n            ArrayType: Probabilities of shape (batch_size, num_classes).\n        \"\"\"\n        scaled_logits = input_batch / (self.temperature + self.epsilon)\n\n        m = xp.max(scaled_logits, axis=1, keepdims=True)\n        e = xp.exp(scaled_logits - m, dtype=DTYPE)\n\n        self.output = e / xp.sum(e, axis=1, keepdims=True, dtype=DTYPE)\n        return self.output\n\n    def backward(self, output_gradient_batch: ArrayType) -&gt; ArrayType:\n        \"\"\"Computes gradient for Softmax.\n\n        Note: This is rarely used directly if using `CategoricalCrossEntropy` loss,\n        as the framework optimizes the combined gradient calculation for numerical stability.\n\n        Args:\n            output_gradient_batch (ArrayType): Gradient from next layer.\n\n        Returns:\n            ArrayType: Gradient w.r.t input.\n        \"\"\"\n        sum_s_times_g: ArrayType = xp.sum(self.output * output_gradient_batch, axis=1, keepdims=True, dtype=DTYPE)  # type: ignore[assignment]\n\n        res: ArrayType = (self.output * (output_gradient_batch - sum_s_times_g)) / (self.temperature + self.epsilon)\n        return res\n\n    @property\n    def params(self) -&gt; dict[str, tuple[ArrayType, ArrayType]]:\n        return {}\n</code></pre>"},{"location":"reference/activations/#mpneuralnetwork.activations.Softmax.__init__","title":"<code>__init__(temperature=1.0, epsilon=1e-08)</code>","text":"<p>Initializes the Softmax layer.</p> <p>Parameters:</p> Name Type Description Default <code>temperature</code> <code>float</code> <p>Temperature parameter. Defaults to 1.0.</p> <code>1.0</code> <code>epsilon</code> <code>float</code> <p>Small float added to denominator to avoid dividing by zero. Defaults to 1e-8.</p> <code>1e-08</code> Source code in <code>src/mpneuralnetwork/activations.py</code> <pre><code>def __init__(self, temperature: float = 1.0, epsilon: float = 1e-8) -&gt; None:\n    \"\"\"Initializes the Softmax layer.\n\n    Args:\n        temperature (float, optional): Temperature parameter. Defaults to 1.0.\n        epsilon (float, optional): Small float added to denominator to avoid dividing by zero. Defaults to 1e-8.\n    \"\"\"\n    self.temperature: float = temperature\n    self.epsilon: float = epsilon\n</code></pre>"},{"location":"reference/activations/#mpneuralnetwork.activations.Softmax.backward","title":"<code>backward(output_gradient_batch)</code>","text":"<p>Computes gradient for Softmax.</p> <p>Note: This is rarely used directly if using <code>CategoricalCrossEntropy</code> loss, as the framework optimizes the combined gradient calculation for numerical stability.</p> <p>Parameters:</p> Name Type Description Default <code>output_gradient_batch</code> <code>ArrayType</code> <p>Gradient from next layer.</p> required <p>Returns:</p> Name Type Description <code>ArrayType</code> <code>ArrayType</code> <p>Gradient w.r.t input.</p> Source code in <code>src/mpneuralnetwork/activations.py</code> <pre><code>def backward(self, output_gradient_batch: ArrayType) -&gt; ArrayType:\n    \"\"\"Computes gradient for Softmax.\n\n    Note: This is rarely used directly if using `CategoricalCrossEntropy` loss,\n    as the framework optimizes the combined gradient calculation for numerical stability.\n\n    Args:\n        output_gradient_batch (ArrayType): Gradient from next layer.\n\n    Returns:\n        ArrayType: Gradient w.r.t input.\n    \"\"\"\n    sum_s_times_g: ArrayType = xp.sum(self.output * output_gradient_batch, axis=1, keepdims=True, dtype=DTYPE)  # type: ignore[assignment]\n\n    res: ArrayType = (self.output * (output_gradient_batch - sum_s_times_g)) / (self.temperature + self.epsilon)\n    return res\n</code></pre>"},{"location":"reference/activations/#mpneuralnetwork.activations.Softmax.forward","title":"<code>forward(input_batch, training=True)</code>","text":"<p>Applies Softmax function.</p> <p>Parameters:</p> Name Type Description Default <code>input_batch</code> <code>ArrayType</code> <p>Input logits of shape (batch_size, num_classes).</p> required <code>training</code> <code>bool</code> <p>Unused. Defaults to True.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>ArrayType</code> <code>ArrayType</code> <p>Probabilities of shape (batch_size, num_classes).</p> Source code in <code>src/mpneuralnetwork/activations.py</code> <pre><code>def forward(self, input_batch: ArrayType, training: bool = True) -&gt; ArrayType:\n    \"\"\"Applies Softmax function.\n\n    Args:\n        input_batch (ArrayType): Input logits of shape (batch_size, num_classes).\n        training (bool, optional): Unused. Defaults to True.\n\n    Returns:\n        ArrayType: Probabilities of shape (batch_size, num_classes).\n    \"\"\"\n    scaled_logits = input_batch / (self.temperature + self.epsilon)\n\n    m = xp.max(scaled_logits, axis=1, keepdims=True)\n    e = xp.exp(scaled_logits - m, dtype=DTYPE)\n\n    self.output = e / xp.sum(e, axis=1, keepdims=True, dtype=DTYPE)\n    return self.output\n</code></pre>"},{"location":"reference/activations/#mpneuralnetwork.activations.Sigmoid","title":"<code>mpneuralnetwork.activations.Sigmoid</code>","text":"<p>               Bases: <code>Activation</code></p> <p>Sigmoid activation function.</p> Formula <p><code>f(x) = 1 / (1 + exp(-x))</code></p> <p>Range: (0, 1). Used for binary classification (output layer) or gating mechanisms (like in LSTMs). Can suffer from vanishing gradients in deep networks.</p> Source code in <code>src/mpneuralnetwork/activations.py</code> <pre><code>class Sigmoid(Activation):\n    \"\"\"Sigmoid activation function.\n\n    Formula:\n        `f(x) = 1 / (1 + exp(-x))`\n\n    Range: (0, 1).\n    Used for binary classification (output layer) or gating mechanisms (like in LSTMs).\n    Can suffer from vanishing gradients in deep networks.\n    \"\"\"\n\n    def __init__(self) -&gt; None:\n        def sigmoid(x: ArrayType) -&gt; ArrayType:\n            return 1 / (1 + xp.exp(-x, dtype=DTYPE))  # type: ignore[no-any-return]\n\n        super().__init__(lambda x: sigmoid(x), lambda x: sigmoid(x) * (1 - sigmoid(x)))\n</code></pre>"},{"location":"reference/backend/","title":"Backend &amp; Hardware Acceleration","text":"<p>MPNeuralNetwork supports both CPU and GPU execution through a unified backend interface. This module handles the abstraction between <code>numpy</code> (CPU) and <code>cupy</code> (GPU).</p>"},{"location":"reference/backend/#configuration","title":"Configuration","text":"<p>The backend is selected at runtime via the <code>MPNN_BACKEND</code> environment variable.</p> <ul> <li>CPU (Default): <code>MPNN_BACKEND=numpy</code></li> <li>GPU (NVIDIA): <code>MPNN_BACKEND=cupy</code> (Requires <code>cupy</code> to be installed)</li> </ul>"},{"location":"reference/backend/#api-reference","title":"API Reference","text":""},{"location":"reference/backend/#global-types","title":"Global Types","text":""},{"location":"reference/backend/#mpneuralnetwork.backend.DTYPE","title":"<code>mpneuralnetwork.backend.DTYPE = cp.float32</code>  <code>module-attribute</code>","text":""},{"location":"reference/backend/#mpneuralnetwork.backend.ArrayType","title":"<code>mpneuralnetwork.backend.ArrayType = np.ndarray | Any</code>  <code>module-attribute</code>","text":"<p>TypeAlias: Union[np.ndarray, cp.ndarray] - Represents an array on either CPU or GPU.</p>"},{"location":"reference/backend/#functions","title":"Functions","text":""},{"location":"reference/backend/#mpneuralnetwork.backend.to_device","title":"<code>mpneuralnetwork.backend.to_device(array)</code>","text":"<p>Transfers an array to the current backend device.</p> <p>If the backend is CPU (NumPy), returns the array as a numpy array. If the backend is GPU (CuPy), moves the array to GPU memory.</p> <p>Parameters:</p> Name Type Description Default <code>array</code> <code>ArrayType</code> <p>The input array (numpy or cupy).</p> required <p>Returns:</p> Name Type Description <code>ArrayType</code> <code>ArrayType</code> <p>The array on the configured device.</p> Source code in <code>src/mpneuralnetwork/backend.py</code> <pre><code>def to_device(array: ArrayType) -&gt; ArrayType:\n    \"\"\"Transfers an array to the current backend device.\n\n    If the backend is CPU (NumPy), returns the array as a numpy array.\n    If the backend is GPU (CuPy), moves the array to GPU memory.\n\n    Args:\n        array (ArrayType): The input array (numpy or cupy).\n\n    Returns:\n        ArrayType: The array on the configured device.\n    \"\"\"\n    if xp.__name__ == \"cupy\":\n        return xp.asarray(array)\n    return np.asarray(array)\n</code></pre>"},{"location":"reference/backend/#mpneuralnetwork.backend.to_host","title":"<code>mpneuralnetwork.backend.to_host(array)</code>","text":"<p>Transfers an array to the CPU (Host).</p> <p>If the array is on GPU (CuPy), it is transferred to CPU. If it's already on CPU, it is returned as is.</p> <p>Parameters:</p> Name Type Description Default <code>array</code> <code>ArrayType</code> <p>The input array.</p> required <p>Returns:</p> Name Type Description <code>NDArray</code> <code>NDArray</code> <p>A NumPy array.</p> Source code in <code>src/mpneuralnetwork/backend.py</code> <pre><code>def to_host(array: ArrayType) -&gt; NDArray:\n    \"\"\"Transfers an array to the CPU (Host).\n\n    If the array is on GPU (CuPy), it is transferred to CPU.\n    If it's already on CPU, it is returned as is.\n\n    Args:\n        array (ArrayType): The input array.\n\n    Returns:\n        NDArray: A NumPy array.\n    \"\"\"\n    if hasattr(array, \"get\"):\n        return array.get()  # type: ignore\n    return np.asarray(array)\n</code></pre>"},{"location":"reference/backend/#mpneuralnetwork.backend.get_backend","title":"<code>mpneuralnetwork.backend.get_backend()</code>","text":"<p>Returns the current backend module.</p> <p>Returns:</p> Name Type Description <code>ModuleType</code> <code>ModuleType</code> <p><code>numpy</code> or <code>cupy</code> module.</p> Source code in <code>src/mpneuralnetwork/backend.py</code> <pre><code>def get_backend() -&gt; ModuleType:\n    \"\"\"Returns the current backend module.\n\n    Returns:\n        ModuleType: `numpy` or `cupy` module.\n    \"\"\"\n    return xp\n</code></pre>"},{"location":"reference/layers/","title":"Layers API","text":"<p>Layers are the building blocks of neural networks. They store parameters (weights, biases) and implement the forward/backward propagation logic.</p>"},{"location":"reference/layers/#base-layer","title":"Base Layer","text":""},{"location":"reference/layers/#mpneuralnetwork.layers.Layer","title":"<code>mpneuralnetwork.layers.Layer</code>","text":"<p>Abstract base class for all neural network layers.</p> <p>This class defines the interface that all layers must implement, including forward/backward passes and parameter management.</p> <p>Attributes:</p> Name Type Description <code>input_shape</code> <code>tuple[int, ...]</code> <p>Shape of the input data (excluding batch dimension).</p> <code>output_shape</code> <code>tuple[int, ...]</code> <p>Shape of the output data (excluding batch dimension).</p> <code>input</code> <code>ArrayType</code> <p>Caches the input for the backward pass.</p> <code>output</code> <code>ArrayType</code> <p>Caches the output.</p> Source code in <code>src/mpneuralnetwork/layers/layer.py</code> <pre><code>class Layer:\n    \"\"\"Abstract base class for all neural network layers.\n\n    This class defines the interface that all layers must implement, including\n    forward/backward passes and parameter management.\n\n    Attributes:\n        input_shape (tuple[int, ...]): Shape of the input data (excluding batch dimension).\n        output_shape (tuple[int, ...]): Shape of the output data (excluding batch dimension).\n        input (ArrayType): Caches the input for the backward pass.\n        output (ArrayType): Caches the output.\n    \"\"\"\n\n    def __init__(self, output_shape: int | tuple[int, ...] | None = None, input_shape: int | tuple[int, ...] | None = None) -&gt; None:\n        \"\"\"Initializes the Layer.\n\n        Args:\n            output_shape (int | tuple[int, ...], optional): Desired output shape.\n            input_shape (int | tuple[int, ...], optional): Known input shape.\n        \"\"\"\n        self.output_shape: tuple[int, ...]\n        if output_shape is not None:\n            if isinstance(output_shape, int):\n                output_shape = (output_shape,)\n            self.output_shape = output_shape\n\n        self.input_shape: tuple[int, ...]\n        if input_shape is not None:\n            if isinstance(input_shape, int):\n                input_shape = (input_shape,)\n            self.input_shape = input_shape\n\n        self.input: ArrayType\n        self.output: ArrayType\n\n    def get_config(self) -&gt; dict:\n        \"\"\"Returns the configuration of the layer for serialization.\n\n        Returns:\n            dict: Dictionary containing layer configuration.\n        \"\"\"\n        return {\"type\": self.__class__.__name__}\n\n    def build(self, input_shape: int | tuple[int, ...]) -&gt; None:\n        \"\"\"Configures the layer based on the input shape.\n\n        Called automatically by the Model before training.\n\n        Args:\n            input_shape (int | tuple[int, ...]): The shape of the input.\n        \"\"\"\n        if isinstance(input_shape, int):\n            input_shape = (input_shape,)\n\n        self.input_shape = input_shape\n\n        if not hasattr(self, \"output_shape\"):\n            self.output_shape = input_shape\n\n    @abstractmethod\n    def forward(self, input_batch: ArrayType, training: bool = True) -&gt; ArrayType:\n        \"\"\"Performs the forward propagation pass.\n\n        Args:\n            input_batch (ArrayType): Input data of shape (batch_size, ...).\n            training (bool, optional): Whether the layer is in training mode. Defaults to True.\n\n        Returns:\n            ArrayType: Output data.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def backward(self, output_gradient_batch: ArrayType) -&gt; ArrayType:\n        \"\"\"Performs the backward propagation pass.\n\n        Computes the gradient of the loss function with respect to the input.\n        Also calculates gradients for any trainable parameters.\n\n        Args:\n            output_gradient_batch (ArrayType): Gradient of the loss w.r.t the output.\n\n        Returns:\n            ArrayType: Gradient of the loss w.r.t the input.\n        \"\"\"\n        pass\n\n    @property\n    def params(self) -&gt; dict[str, tuple[ArrayType, ArrayType]]:\n        \"\"\"Returns trainable parameters and their gradients.\n\n        Returns:\n            dict[str, tuple[ArrayType, ArrayType]]: A dictionary where keys are parameter names\n            (e.g., \"weights\", \"biases\") and values are tuples of (parameter_value, parameter_gradient).\n        \"\"\"\n        return {}\n\n    def load_params(self, params: dict[str, ArrayType]) -&gt; None:\n        \"\"\"Loads trainable parameters into the layer.\n\n        Args:\n            params (dict[str, ArrayType]): Dictionary mapping parameter names to values.\n        \"\"\"\n        pass\n\n    @property\n    def state(self) -&gt; dict[str, ArrayType]:\n        \"\"\"Returns non-trainable internal state (e.g., BatchNorm running means).\n\n        Returns:\n            dict[str, ArrayType]: Dictionary of state variables.\n        \"\"\"\n        return {}\n\n    @state.setter\n    def state(self, state: dict[str, ArrayType]) -&gt; None:\n        \"\"\"Restores non-trainable internal state.\n\n        Args:\n            state (dict[str, ArrayType]): Dictionary of state variables.\n        \"\"\"\n        pass\n\n    @property\n    def input_size(self) -&gt; int:\n        \"\"\"Returns the total number of elements in the input (excluding batch).\"\"\"\n        return int(np.prod(self.input_shape))\n\n    @property\n    def output_size(self) -&gt; int:\n        \"\"\"Returns the total number of elements in the output (excluding batch).\"\"\"\n        return int(np.prod(self.output_shape))\n</code></pre>"},{"location":"reference/layers/#mpneuralnetwork.layers.Layer.input_size","title":"<code>input_size</code>  <code>property</code>","text":"<p>Returns the total number of elements in the input (excluding batch).</p>"},{"location":"reference/layers/#mpneuralnetwork.layers.Layer.output_size","title":"<code>output_size</code>  <code>property</code>","text":"<p>Returns the total number of elements in the output (excluding batch).</p>"},{"location":"reference/layers/#mpneuralnetwork.layers.Layer.params","title":"<code>params</code>  <code>property</code>","text":"<p>Returns trainable parameters and their gradients.</p> <p>Returns:</p> Type Description <code>dict[str, tuple[ArrayType, ArrayType]]</code> <p>dict[str, tuple[ArrayType, ArrayType]]: A dictionary where keys are parameter names</p> <code>dict[str, tuple[ArrayType, ArrayType]]</code> <p>(e.g., \"weights\", \"biases\") and values are tuples of (parameter_value, parameter_gradient).</p>"},{"location":"reference/layers/#mpneuralnetwork.layers.Layer.state","title":"<code>state</code>  <code>property</code> <code>writable</code>","text":"<p>Returns non-trainable internal state (e.g., BatchNorm running means).</p> <p>Returns:</p> Type Description <code>dict[str, ArrayType]</code> <p>dict[str, ArrayType]: Dictionary of state variables.</p>"},{"location":"reference/layers/#mpneuralnetwork.layers.Layer.__init__","title":"<code>__init__(output_shape=None, input_shape=None)</code>","text":"<p>Initializes the Layer.</p> <p>Parameters:</p> Name Type Description Default <code>output_shape</code> <code>int | tuple[int, ...]</code> <p>Desired output shape.</p> <code>None</code> <code>input_shape</code> <code>int | tuple[int, ...]</code> <p>Known input shape.</p> <code>None</code> Source code in <code>src/mpneuralnetwork/layers/layer.py</code> <pre><code>def __init__(self, output_shape: int | tuple[int, ...] | None = None, input_shape: int | tuple[int, ...] | None = None) -&gt; None:\n    \"\"\"Initializes the Layer.\n\n    Args:\n        output_shape (int | tuple[int, ...], optional): Desired output shape.\n        input_shape (int | tuple[int, ...], optional): Known input shape.\n    \"\"\"\n    self.output_shape: tuple[int, ...]\n    if output_shape is not None:\n        if isinstance(output_shape, int):\n            output_shape = (output_shape,)\n        self.output_shape = output_shape\n\n    self.input_shape: tuple[int, ...]\n    if input_shape is not None:\n        if isinstance(input_shape, int):\n            input_shape = (input_shape,)\n        self.input_shape = input_shape\n\n    self.input: ArrayType\n    self.output: ArrayType\n</code></pre>"},{"location":"reference/layers/#mpneuralnetwork.layers.Layer.backward","title":"<code>backward(output_gradient_batch)</code>  <code>abstractmethod</code>","text":"<p>Performs the backward propagation pass.</p> <p>Computes the gradient of the loss function with respect to the input. Also calculates gradients for any trainable parameters.</p> <p>Parameters:</p> Name Type Description Default <code>output_gradient_batch</code> <code>ArrayType</code> <p>Gradient of the loss w.r.t the output.</p> required <p>Returns:</p> Name Type Description <code>ArrayType</code> <code>ArrayType</code> <p>Gradient of the loss w.r.t the input.</p> Source code in <code>src/mpneuralnetwork/layers/layer.py</code> <pre><code>@abstractmethod\ndef backward(self, output_gradient_batch: ArrayType) -&gt; ArrayType:\n    \"\"\"Performs the backward propagation pass.\n\n    Computes the gradient of the loss function with respect to the input.\n    Also calculates gradients for any trainable parameters.\n\n    Args:\n        output_gradient_batch (ArrayType): Gradient of the loss w.r.t the output.\n\n    Returns:\n        ArrayType: Gradient of the loss w.r.t the input.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/layers/#mpneuralnetwork.layers.Layer.build","title":"<code>build(input_shape)</code>","text":"<p>Configures the layer based on the input shape.</p> <p>Called automatically by the Model before training.</p> <p>Parameters:</p> Name Type Description Default <code>input_shape</code> <code>int | tuple[int, ...]</code> <p>The shape of the input.</p> required Source code in <code>src/mpneuralnetwork/layers/layer.py</code> <pre><code>def build(self, input_shape: int | tuple[int, ...]) -&gt; None:\n    \"\"\"Configures the layer based on the input shape.\n\n    Called automatically by the Model before training.\n\n    Args:\n        input_shape (int | tuple[int, ...]): The shape of the input.\n    \"\"\"\n    if isinstance(input_shape, int):\n        input_shape = (input_shape,)\n\n    self.input_shape = input_shape\n\n    if not hasattr(self, \"output_shape\"):\n        self.output_shape = input_shape\n</code></pre>"},{"location":"reference/layers/#mpneuralnetwork.layers.Layer.forward","title":"<code>forward(input_batch, training=True)</code>  <code>abstractmethod</code>","text":"<p>Performs the forward propagation pass.</p> <p>Parameters:</p> Name Type Description Default <code>input_batch</code> <code>ArrayType</code> <p>Input data of shape (batch_size, ...).</p> required <code>training</code> <code>bool</code> <p>Whether the layer is in training mode. Defaults to True.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>ArrayType</code> <code>ArrayType</code> <p>Output data.</p> Source code in <code>src/mpneuralnetwork/layers/layer.py</code> <pre><code>@abstractmethod\ndef forward(self, input_batch: ArrayType, training: bool = True) -&gt; ArrayType:\n    \"\"\"Performs the forward propagation pass.\n\n    Args:\n        input_batch (ArrayType): Input data of shape (batch_size, ...).\n        training (bool, optional): Whether the layer is in training mode. Defaults to True.\n\n    Returns:\n        ArrayType: Output data.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/layers/#mpneuralnetwork.layers.Layer.get_config","title":"<code>get_config()</code>","text":"<p>Returns the configuration of the layer for serialization.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Dictionary containing layer configuration.</p> Source code in <code>src/mpneuralnetwork/layers/layer.py</code> <pre><code>def get_config(self) -&gt; dict:\n    \"\"\"Returns the configuration of the layer for serialization.\n\n    Returns:\n        dict: Dictionary containing layer configuration.\n    \"\"\"\n    return {\"type\": self.__class__.__name__}\n</code></pre>"},{"location":"reference/layers/#mpneuralnetwork.layers.Layer.load_params","title":"<code>load_params(params)</code>","text":"<p>Loads trainable parameters into the layer.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>dict[str, ArrayType]</code> <p>Dictionary mapping parameter names to values.</p> required Source code in <code>src/mpneuralnetwork/layers/layer.py</code> <pre><code>def load_params(self, params: dict[str, ArrayType]) -&gt; None:\n    \"\"\"Loads trainable parameters into the layer.\n\n    Args:\n        params (dict[str, ArrayType]): Dictionary mapping parameter names to values.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/layers/#1d-layers-dense","title":"1D Layers (Dense)","text":"<p>Layers typically used for Multi-Layer Perceptrons (MLP) or final classification stages.</p>"},{"location":"reference/layers/#mpneuralnetwork.layers.Dense","title":"<code>mpneuralnetwork.layers.Dense</code>","text":"<p>               Bases: <code>Layer</code></p> <p>Fully Connected (Dense) Layer.</p> <p>Every neuron in the input is connected to every neuron in the output.</p> Operation <p><code>Y = X @ W + b</code></p> <p>Attributes:</p> Name Type Description <code>output_size</code> <code>int</code> <p>Dimensionality of the output space.</p> <code>input_size</code> <code>int</code> <p>Dimensionality of the input space.</p> <code>initialization</code> <code>Lit_W</code> <p>Weight initialization method (\"auto\", \"he\", \"xavier\").</p> <code>no_bias</code> <code>bool</code> <p>Whether to disable the bias vector.</p> <code>weights</code> <code>ArrayType</code> <p>Weight matrix of shape (input_size, output_size).</p> <code>biases</code> <code>ArrayType</code> <p>Bias vector of shape (1, output_size).</p> Source code in <code>src/mpneuralnetwork/layers/layer1d.py</code> <pre><code>class Dense(Layer):\n    \"\"\"Fully Connected (Dense) Layer.\n\n    Every neuron in the input is connected to every neuron in the output.\n\n    Operation:\n        `Y = X @ W + b`\n\n    Attributes:\n        output_size (int): Dimensionality of the output space.\n        input_size (int): Dimensionality of the input space.\n        initialization (Lit_W): Weight initialization method (\"auto\", \"he\", \"xavier\").\n        no_bias (bool): Whether to disable the bias vector.\n        weights (ArrayType): Weight matrix of shape (input_size, output_size).\n        biases (ArrayType): Bias vector of shape (1, output_size).\n    \"\"\"\n\n    def __init__(\n        self,\n        output_size: int,\n        input_size: int | None = None,\n        initialization: Lit_W = \"auto\",\n        no_bias: bool = False,\n    ) -&gt; None:\n        \"\"\"Initializes the Dense layer.\n\n        Args:\n            output_size (int): Number of neurons in this layer.\n            input_size (int | None, optional): Number of input features. If None, inferred at build time.\n            initialization (Lit_W, optional): Weight init strategy. Defaults to \"auto\".\n            no_bias (bool, optional): If True, bias is not used. Defaults to False.\n        \"\"\"\n        super().__init__(output_shape=output_size, input_shape=input_size)\n        self.initialization: Lit_W = initialization\n        self.no_bias: bool = no_bias\n\n        self.weights: ArrayType\n        self.weights_gradient: ArrayType\n\n        self.biases: ArrayType\n        self.biases_gradient: ArrayType\n\n        if input_size is not None:\n            self.build(input_size)\n\n    def get_config(self) -&gt; dict:\n        config = super().get_config()\n        config.update(\n            {\n                \"output_size\": self.output_size,\n                \"input_size\": self.input_size,\n                \"initialization\": self.initialization,\n                \"no_bias\": self.no_bias,\n            }\n        )\n        return config\n\n    def build(self, input_shape: int | tuple[int, ...]) -&gt; None:\n        super().build(input_shape)\n\n        if self.initialization != \"auto\":\n            self.init_weights(self.initialization, self.no_bias)\n\n    def init_weights(self, method: Lit_W, no_bias: bool) -&gt; None:\n        \"\"\"Initializes weights using the specified method.\n\n        Args:\n            method (Lit_W): Initialization method.\n                - \"he\": Kaiming He initialization (for ReLU).\n                - \"xavier\": Xavier Glorot initialization (for Sigmoid/Tanh).\n            no_bias (bool): Whether to disable bias (e.g. if followed by BatchNorm).\n        \"\"\"\n        std_dev = 0.1\n\n        if method == \"he\":\n            std_dev = xp.sqrt(2.0 / self.input_size, dtype=DTYPE)\n        elif method == \"xavier\":\n            std_dev = xp.sqrt(1.0 / self.input_size, dtype=DTYPE)\n\n        self.weights = xp.random.randn(self.input_size, self.output_size).astype(DTYPE) * std_dev\n        self.weights_gradient = xp.zeros_like(self.weights, dtype=DTYPE)\n\n        self.no_bias = no_bias\n\n        if not self.no_bias:\n            self.biases = xp.random.randn(1, self.output_size).astype(DTYPE)\n            self.biases_gradient = xp.zeros_like(self.biases, dtype=DTYPE)\n\n    def forward(self, input_batch: ArrayType, training: bool = True) -&gt; ArrayType:\n        \"\"\"Performs forward propagation.\n\n        Args:\n            input_batch (ArrayType): Input data of shape (batch_size, input_size).\n            training (bool, optional): Unused for Dense layer. Defaults to True.\n\n        Returns:\n            ArrayType: Output data of shape (batch_size, output_size).\n        \"\"\"\n        self.input = input_batch\n\n        res: ArrayType = self.input @ self.weights\n        if not self.no_bias:\n            res += self.biases\n        return res\n\n    def backward(self, output_gradient_batch: ArrayType) -&gt; ArrayType:\n        \"\"\"Performs backward propagation.\n\n        Computes gradients for weights, biases, and inputs.\n\n        Args:\n            output_gradient_batch (ArrayType): Gradient w.r.t output (batch_size, output_size).\n\n        Returns:\n            ArrayType: Gradient w.r.t input (batch_size, input_size).\n        \"\"\"\n        self.weights_gradient = self.input.T @ output_gradient_batch\n        if not self.no_bias:\n            self.biases_gradient = xp.sum(output_gradient_batch, axis=0, keepdims=True, dtype=DTYPE)  # type: ignore\n\n        grad: ArrayType = output_gradient_batch @ self.weights.T\n        return grad\n\n    @property\n    def params(self) -&gt; dict[str, tuple[ArrayType, ArrayType]]:\n        params = {\"weights\": (self.weights, self.weights_gradient)}\n        if not self.no_bias:\n            params[\"biases\"] = (self.biases, self.biases_gradient)\n        return params\n\n    def load_params(self, params: dict[str, ArrayType]) -&gt; None:\n        self.weights[:] = params[\"weights\"]\n        if not self.no_bias:\n            self.biases[:] = params[\"biases\"]\n</code></pre>"},{"location":"reference/layers/#mpneuralnetwork.layers.Dense.__init__","title":"<code>__init__(output_size, input_size=None, initialization='auto', no_bias=False)</code>","text":"<p>Initializes the Dense layer.</p> <p>Parameters:</p> Name Type Description Default <code>output_size</code> <code>int</code> <p>Number of neurons in this layer.</p> required <code>input_size</code> <code>int | None</code> <p>Number of input features. If None, inferred at build time.</p> <code>None</code> <code>initialization</code> <code>Lit_W</code> <p>Weight init strategy. Defaults to \"auto\".</p> <code>'auto'</code> <code>no_bias</code> <code>bool</code> <p>If True, bias is not used. Defaults to False.</p> <code>False</code> Source code in <code>src/mpneuralnetwork/layers/layer1d.py</code> <pre><code>def __init__(\n    self,\n    output_size: int,\n    input_size: int | None = None,\n    initialization: Lit_W = \"auto\",\n    no_bias: bool = False,\n) -&gt; None:\n    \"\"\"Initializes the Dense layer.\n\n    Args:\n        output_size (int): Number of neurons in this layer.\n        input_size (int | None, optional): Number of input features. If None, inferred at build time.\n        initialization (Lit_W, optional): Weight init strategy. Defaults to \"auto\".\n        no_bias (bool, optional): If True, bias is not used. Defaults to False.\n    \"\"\"\n    super().__init__(output_shape=output_size, input_shape=input_size)\n    self.initialization: Lit_W = initialization\n    self.no_bias: bool = no_bias\n\n    self.weights: ArrayType\n    self.weights_gradient: ArrayType\n\n    self.biases: ArrayType\n    self.biases_gradient: ArrayType\n\n    if input_size is not None:\n        self.build(input_size)\n</code></pre>"},{"location":"reference/layers/#mpneuralnetwork.layers.Dense.backward","title":"<code>backward(output_gradient_batch)</code>","text":"<p>Performs backward propagation.</p> <p>Computes gradients for weights, biases, and inputs.</p> <p>Parameters:</p> Name Type Description Default <code>output_gradient_batch</code> <code>ArrayType</code> <p>Gradient w.r.t output (batch_size, output_size).</p> required <p>Returns:</p> Name Type Description <code>ArrayType</code> <code>ArrayType</code> <p>Gradient w.r.t input (batch_size, input_size).</p> Source code in <code>src/mpneuralnetwork/layers/layer1d.py</code> <pre><code>def backward(self, output_gradient_batch: ArrayType) -&gt; ArrayType:\n    \"\"\"Performs backward propagation.\n\n    Computes gradients for weights, biases, and inputs.\n\n    Args:\n        output_gradient_batch (ArrayType): Gradient w.r.t output (batch_size, output_size).\n\n    Returns:\n        ArrayType: Gradient w.r.t input (batch_size, input_size).\n    \"\"\"\n    self.weights_gradient = self.input.T @ output_gradient_batch\n    if not self.no_bias:\n        self.biases_gradient = xp.sum(output_gradient_batch, axis=0, keepdims=True, dtype=DTYPE)  # type: ignore\n\n    grad: ArrayType = output_gradient_batch @ self.weights.T\n    return grad\n</code></pre>"},{"location":"reference/layers/#mpneuralnetwork.layers.Dense.forward","title":"<code>forward(input_batch, training=True)</code>","text":"<p>Performs forward propagation.</p> <p>Parameters:</p> Name Type Description Default <code>input_batch</code> <code>ArrayType</code> <p>Input data of shape (batch_size, input_size).</p> required <code>training</code> <code>bool</code> <p>Unused for Dense layer. Defaults to True.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>ArrayType</code> <code>ArrayType</code> <p>Output data of shape (batch_size, output_size).</p> Source code in <code>src/mpneuralnetwork/layers/layer1d.py</code> <pre><code>def forward(self, input_batch: ArrayType, training: bool = True) -&gt; ArrayType:\n    \"\"\"Performs forward propagation.\n\n    Args:\n        input_batch (ArrayType): Input data of shape (batch_size, input_size).\n        training (bool, optional): Unused for Dense layer. Defaults to True.\n\n    Returns:\n        ArrayType: Output data of shape (batch_size, output_size).\n    \"\"\"\n    self.input = input_batch\n\n    res: ArrayType = self.input @ self.weights\n    if not self.no_bias:\n        res += self.biases\n    return res\n</code></pre>"},{"location":"reference/layers/#mpneuralnetwork.layers.Dense.init_weights","title":"<code>init_weights(method, no_bias)</code>","text":"<p>Initializes weights using the specified method.</p> <p>Parameters:</p> Name Type Description Default <code>method</code> <code>Lit_W</code> <p>Initialization method. - \"he\": Kaiming He initialization (for ReLU). - \"xavier\": Xavier Glorot initialization (for Sigmoid/Tanh).</p> required <code>no_bias</code> <code>bool</code> <p>Whether to disable bias (e.g. if followed by BatchNorm).</p> required Source code in <code>src/mpneuralnetwork/layers/layer1d.py</code> <pre><code>def init_weights(self, method: Lit_W, no_bias: bool) -&gt; None:\n    \"\"\"Initializes weights using the specified method.\n\n    Args:\n        method (Lit_W): Initialization method.\n            - \"he\": Kaiming He initialization (for ReLU).\n            - \"xavier\": Xavier Glorot initialization (for Sigmoid/Tanh).\n        no_bias (bool): Whether to disable bias (e.g. if followed by BatchNorm).\n    \"\"\"\n    std_dev = 0.1\n\n    if method == \"he\":\n        std_dev = xp.sqrt(2.0 / self.input_size, dtype=DTYPE)\n    elif method == \"xavier\":\n        std_dev = xp.sqrt(1.0 / self.input_size, dtype=DTYPE)\n\n    self.weights = xp.random.randn(self.input_size, self.output_size).astype(DTYPE) * std_dev\n    self.weights_gradient = xp.zeros_like(self.weights, dtype=DTYPE)\n\n    self.no_bias = no_bias\n\n    if not self.no_bias:\n        self.biases = xp.random.randn(1, self.output_size).astype(DTYPE)\n        self.biases_gradient = xp.zeros_like(self.biases, dtype=DTYPE)\n</code></pre>"},{"location":"reference/layers/#mpneuralnetwork.layers.Dropout","title":"<code>mpneuralnetwork.layers.Dropout</code>","text":"<p>               Bases: <code>Layer</code></p> <p>Dropout Layer for regularization.</p> <p>Randomly sets input units to 0 with a frequency of <code>probability</code> at each step during training time, which helps prevent overfitting.</p> Training <p><code>output = input * mask</code> (where mask is Bernoulli(1-p)) Values are scaled by <code>1/(1-p)</code> to preserve magnitude.</p> Inference <p><code>output = input</code> (Identity function).</p> <p>Attributes:</p> Name Type Description <code>probability</code> <code>float</code> <p>The dropout rate (fraction of input units to drop).</p> Source code in <code>src/mpneuralnetwork/layers/layer1d.py</code> <pre><code>class Dropout(Layer):\n    \"\"\"Dropout Layer for regularization.\n\n    Randomly sets input units to 0 with a frequency of `probability` at each step during training time,\n    which helps prevent overfitting.\n\n    Training:\n        `output = input * mask` (where mask is Bernoulli(1-p))\n        Values are scaled by `1/(1-p)` to preserve magnitude.\n\n    Inference:\n        `output = input` (Identity function).\n\n    Attributes:\n        probability (float): The dropout rate (fraction of input units to drop).\n    \"\"\"\n\n    def __init__(self, probability: float = 0.5) -&gt; None:\n        \"\"\"Initializes Dropout.\n\n        Args:\n            probability (float, optional): Fraction of the input units to drop. Defaults to 0.5.\n        \"\"\"\n        super().__init__()\n        self.probability: float = probability\n        self.mask: ArrayType\n\n    def get_config(self) -&gt; dict:\n        config = super().get_config()\n        config.update({\"probability\": self.probability})\n        return config\n\n    def forward(self, input_batch: ArrayType, training: bool = True) -&gt; ArrayType:\n        \"\"\"Applies dropout to the input.\n\n        Args:\n            input_batch (ArrayType): Input data.\n            training (bool, optional): If True, applies random dropout. If False, returns input as is.\n\n        Returns:\n            ArrayType: Processed input.\n        \"\"\"\n        if not training:\n            return input_batch\n\n        self.mask = xp.random.binomial(1, 1 - self.probability, size=input_batch.shape).astype(DTYPE) / (1 - self.probability)\n\n        res: ArrayType = input_batch * self.mask\n        return res\n\n    def backward(self, output_gradient_batch: ArrayType) -&gt; ArrayType:\n        \"\"\"Propagates gradients through the dropout mask.\n\n        Args:\n            output_gradient_batch (ArrayType): Gradient from next layer.\n\n        Returns:\n            ArrayType: Gradient w.r.t input (zeroed out where inputs were dropped).\n        \"\"\"\n        grad: ArrayType = output_gradient_batch * self.mask\n        return grad\n</code></pre>"},{"location":"reference/layers/#mpneuralnetwork.layers.Dropout.__init__","title":"<code>__init__(probability=0.5)</code>","text":"<p>Initializes Dropout.</p> <p>Parameters:</p> Name Type Description Default <code>probability</code> <code>float</code> <p>Fraction of the input units to drop. Defaults to 0.5.</p> <code>0.5</code> Source code in <code>src/mpneuralnetwork/layers/layer1d.py</code> <pre><code>def __init__(self, probability: float = 0.5) -&gt; None:\n    \"\"\"Initializes Dropout.\n\n    Args:\n        probability (float, optional): Fraction of the input units to drop. Defaults to 0.5.\n    \"\"\"\n    super().__init__()\n    self.probability: float = probability\n    self.mask: ArrayType\n</code></pre>"},{"location":"reference/layers/#mpneuralnetwork.layers.Dropout.backward","title":"<code>backward(output_gradient_batch)</code>","text":"<p>Propagates gradients through the dropout mask.</p> <p>Parameters:</p> Name Type Description Default <code>output_gradient_batch</code> <code>ArrayType</code> <p>Gradient from next layer.</p> required <p>Returns:</p> Name Type Description <code>ArrayType</code> <code>ArrayType</code> <p>Gradient w.r.t input (zeroed out where inputs were dropped).</p> Source code in <code>src/mpneuralnetwork/layers/layer1d.py</code> <pre><code>def backward(self, output_gradient_batch: ArrayType) -&gt; ArrayType:\n    \"\"\"Propagates gradients through the dropout mask.\n\n    Args:\n        output_gradient_batch (ArrayType): Gradient from next layer.\n\n    Returns:\n        ArrayType: Gradient w.r.t input (zeroed out where inputs were dropped).\n    \"\"\"\n    grad: ArrayType = output_gradient_batch * self.mask\n    return grad\n</code></pre>"},{"location":"reference/layers/#mpneuralnetwork.layers.Dropout.forward","title":"<code>forward(input_batch, training=True)</code>","text":"<p>Applies dropout to the input.</p> <p>Parameters:</p> Name Type Description Default <code>input_batch</code> <code>ArrayType</code> <p>Input data.</p> required <code>training</code> <code>bool</code> <p>If True, applies random dropout. If False, returns input as is.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>ArrayType</code> <code>ArrayType</code> <p>Processed input.</p> Source code in <code>src/mpneuralnetwork/layers/layer1d.py</code> <pre><code>def forward(self, input_batch: ArrayType, training: bool = True) -&gt; ArrayType:\n    \"\"\"Applies dropout to the input.\n\n    Args:\n        input_batch (ArrayType): Input data.\n        training (bool, optional): If True, applies random dropout. If False, returns input as is.\n\n    Returns:\n        ArrayType: Processed input.\n    \"\"\"\n    if not training:\n        return input_batch\n\n    self.mask = xp.random.binomial(1, 1 - self.probability, size=input_batch.shape).astype(DTYPE) / (1 - self.probability)\n\n    res: ArrayType = input_batch * self.mask\n    return res\n</code></pre>"},{"location":"reference/layers/#mpneuralnetwork.layers.BatchNormalization","title":"<code>mpneuralnetwork.layers.BatchNormalization</code>","text":"<p>               Bases: <code>Layer</code></p> <p>Batch Normalization Layer (1D).</p> <p>Normalize the activations of the previous layer at each batch, i.e. applies a transformation that maintains the mean activation close to 0 and the activation standard deviation close to 1.</p> Training <p>Uses batch statistics (mean, variance) to normalize. Updates running moving averages.</p> Inference <p>Uses learned running statistics (cache_m, cache_v) to normalize.</p> <p>Attributes:</p> Name Type Description <code>momentum</code> <code>float</code> <p>Momentum for the moving average updating.</p> <code>epsilon</code> <code>float</code> <p>Small float added to variance to avoid dividing by zero.</p> <code>gamma</code> <code>ArrayType</code> <p>Learnable scale parameter.</p> <code>beta</code> <code>ArrayType</code> <p>Learnable shift parameter.</p> Source code in <code>src/mpneuralnetwork/layers/layer1d.py</code> <pre><code>class BatchNormalization(Layer):\n    \"\"\"Batch Normalization Layer (1D).\n\n    Normalize the activations of the previous layer at each batch, i.e. applies a transformation\n    that maintains the mean activation close to 0 and the activation standard deviation close to 1.\n\n    Training:\n        Uses batch statistics (mean, variance) to normalize. Updates running moving averages.\n\n    Inference:\n        Uses learned running statistics (cache_m, cache_v) to normalize.\n\n    Attributes:\n        momentum (float): Momentum for the moving average updating.\n        epsilon (float): Small float added to variance to avoid dividing by zero.\n        gamma (ArrayType): Learnable scale parameter.\n        beta (ArrayType): Learnable shift parameter.\n    \"\"\"\n\n    def __init__(self, momentum: float = 0.9, epsilon: float = 1e-8) -&gt; None:\n        \"\"\"Initializes BatchNormalization.\n\n        Args:\n            momentum (float, optional): Momentum for moving average (typically 0.9 or 0.99). Defaults to 0.9.\n            epsilon (float, optional): Epsilon for stability. Defaults to 1e-8.\n        \"\"\"\n        super().__init__()\n        self.momentum: float = momentum\n        self.epsilon: float = epsilon\n\n        self.gamma: ArrayType\n        self.beta: ArrayType\n\n        self.cache_m: ArrayType\n        self.cache_v: ArrayType\n\n    def build(self, input_shape: int | tuple[int, ...]) -&gt; None:\n        super().build(input_shape)\n\n        self.gamma = xp.ones((1, self.input_size), dtype=DTYPE)\n        self.gamma_gradient = xp.zeros_like(self.gamma, dtype=DTYPE)\n\n        self.beta = xp.zeros((1, self.input_size), dtype=DTYPE)\n        self.beta_gradient = xp.zeros_like(self.beta, dtype=DTYPE)\n\n        self.cache_m = xp.zeros((1, self.input_size), dtype=DTYPE)\n        self.cache_v = xp.ones((1, self.input_size), dtype=DTYPE)\n\n        self.std_inv: ArrayType\n        self.x_centered: ArrayType\n        self.x_norm: ArrayType\n\n    def get_config(self) -&gt; dict:\n        config = super().get_config()\n        config.update({\"momentum\": self.momentum, \"epsilon\": self.epsilon})\n        return config\n\n    def forward(self, input_batch: ArrayType, training: bool = True) -&gt; ArrayType:\n        \"\"\"Performs batch normalization.\n\n        Args:\n            input_batch (ArrayType): Input data of shape (batch_size, input_size).\n            training (bool, optional): If True, uses batch stats and updates running averages.\n                If False, uses running averages.\n\n        Returns:\n            ArrayType: Normalized and scaled data.\n        \"\"\"\n        self.input = input_batch\n\n        mean: ArrayType\n        var: ArrayType\n\n        if training:\n            mean = xp.mean(self.input, axis=0, keepdims=True, dtype=DTYPE)  # type: ignore\n            var = xp.var(self.input, axis=0, keepdims=True, dtype=DTYPE)\n\n            self.cache_m = self.momentum * self.cache_m + (1 - self.momentum) * mean\n            self.cache_v = self.momentum * self.cache_v + (1 - self.momentum) * var\n\n        else:\n            mean = self.cache_m\n            var = self.cache_v\n\n        self.std_inv = 1 / xp.sqrt(var + self.epsilon, dtype=DTYPE)\n        self.x_centered = self.input - mean\n        self.x_norm = self.x_centered * self.std_inv\n\n        res: ArrayType = self.x_norm * self.gamma\n        res += self.beta\n        return res\n\n    def backward(self, output_gradient_batch: ArrayType) -&gt; ArrayType:\n        \"\"\"Computes gradients for BN.\n\n        Args:\n            output_gradient_batch (ArrayType): Gradient w.r.t output.\n\n        Returns:\n            ArrayType: Gradient w.r.t input.\n        \"\"\"\n        self.gamma_gradient = xp.sum(self.x_norm * output_gradient_batch, axis=0, keepdims=True, dtype=DTYPE)  # type: ignore\n        self.beta_gradient = xp.sum(output_gradient_batch, axis=0, keepdims=True, dtype=DTYPE)  # type: ignore\n\n        N = output_gradient_batch.shape[0]\n        dx_norm = output_gradient_batch * self.gamma\n\n        grad: ArrayType = (\n            (1 / N)\n            * self.std_inv\n            * (\n                N * dx_norm\n                - xp.sum(dx_norm, axis=0, keepdims=True, dtype=DTYPE)\n                - self.x_norm * xp.sum(dx_norm * self.x_norm, axis=0, keepdims=True, dtype=DTYPE)\n            )\n        )\n        return grad\n\n    @property\n    def params(self) -&gt; dict[str, tuple[ArrayType, ArrayType]]:\n        return {  # type: ignore\n            \"gamma\": (self.gamma, self.gamma_gradient),\n            \"beta\": (self.beta, self.beta_gradient),\n        }\n\n    def load_params(self, params: dict[str, ArrayType]) -&gt; None:\n        self.gamma[:] = params[\"gamma\"]\n        self.beta[:] = params[\"beta\"]\n\n    @property\n    def state(self) -&gt; dict[str, ArrayType]:\n        return {\"cache_m\": self.cache_m, \"cache_v\": self.cache_v}\n\n    @state.setter\n    def state(self, state: dict[str, ArrayType]) -&gt; None:\n        self.cache_m = state[\"cache_m\"]\n        self.cache_v = state[\"cache_v\"]\n</code></pre>"},{"location":"reference/layers/#mpneuralnetwork.layers.BatchNormalization.__init__","title":"<code>__init__(momentum=0.9, epsilon=1e-08)</code>","text":"<p>Initializes BatchNormalization.</p> <p>Parameters:</p> Name Type Description Default <code>momentum</code> <code>float</code> <p>Momentum for moving average (typically 0.9 or 0.99). Defaults to 0.9.</p> <code>0.9</code> <code>epsilon</code> <code>float</code> <p>Epsilon for stability. Defaults to 1e-8.</p> <code>1e-08</code> Source code in <code>src/mpneuralnetwork/layers/layer1d.py</code> <pre><code>def __init__(self, momentum: float = 0.9, epsilon: float = 1e-8) -&gt; None:\n    \"\"\"Initializes BatchNormalization.\n\n    Args:\n        momentum (float, optional): Momentum for moving average (typically 0.9 or 0.99). Defaults to 0.9.\n        epsilon (float, optional): Epsilon for stability. Defaults to 1e-8.\n    \"\"\"\n    super().__init__()\n    self.momentum: float = momentum\n    self.epsilon: float = epsilon\n\n    self.gamma: ArrayType\n    self.beta: ArrayType\n\n    self.cache_m: ArrayType\n    self.cache_v: ArrayType\n</code></pre>"},{"location":"reference/layers/#mpneuralnetwork.layers.BatchNormalization.backward","title":"<code>backward(output_gradient_batch)</code>","text":"<p>Computes gradients for BN.</p> <p>Parameters:</p> Name Type Description Default <code>output_gradient_batch</code> <code>ArrayType</code> <p>Gradient w.r.t output.</p> required <p>Returns:</p> Name Type Description <code>ArrayType</code> <code>ArrayType</code> <p>Gradient w.r.t input.</p> Source code in <code>src/mpneuralnetwork/layers/layer1d.py</code> <pre><code>def backward(self, output_gradient_batch: ArrayType) -&gt; ArrayType:\n    \"\"\"Computes gradients for BN.\n\n    Args:\n        output_gradient_batch (ArrayType): Gradient w.r.t output.\n\n    Returns:\n        ArrayType: Gradient w.r.t input.\n    \"\"\"\n    self.gamma_gradient = xp.sum(self.x_norm * output_gradient_batch, axis=0, keepdims=True, dtype=DTYPE)  # type: ignore\n    self.beta_gradient = xp.sum(output_gradient_batch, axis=0, keepdims=True, dtype=DTYPE)  # type: ignore\n\n    N = output_gradient_batch.shape[0]\n    dx_norm = output_gradient_batch * self.gamma\n\n    grad: ArrayType = (\n        (1 / N)\n        * self.std_inv\n        * (\n            N * dx_norm\n            - xp.sum(dx_norm, axis=0, keepdims=True, dtype=DTYPE)\n            - self.x_norm * xp.sum(dx_norm * self.x_norm, axis=0, keepdims=True, dtype=DTYPE)\n        )\n    )\n    return grad\n</code></pre>"},{"location":"reference/layers/#mpneuralnetwork.layers.BatchNormalization.forward","title":"<code>forward(input_batch, training=True)</code>","text":"<p>Performs batch normalization.</p> <p>Parameters:</p> Name Type Description Default <code>input_batch</code> <code>ArrayType</code> <p>Input data of shape (batch_size, input_size).</p> required <code>training</code> <code>bool</code> <p>If True, uses batch stats and updates running averages. If False, uses running averages.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>ArrayType</code> <code>ArrayType</code> <p>Normalized and scaled data.</p> Source code in <code>src/mpneuralnetwork/layers/layer1d.py</code> <pre><code>def forward(self, input_batch: ArrayType, training: bool = True) -&gt; ArrayType:\n    \"\"\"Performs batch normalization.\n\n    Args:\n        input_batch (ArrayType): Input data of shape (batch_size, input_size).\n        training (bool, optional): If True, uses batch stats and updates running averages.\n            If False, uses running averages.\n\n    Returns:\n        ArrayType: Normalized and scaled data.\n    \"\"\"\n    self.input = input_batch\n\n    mean: ArrayType\n    var: ArrayType\n\n    if training:\n        mean = xp.mean(self.input, axis=0, keepdims=True, dtype=DTYPE)  # type: ignore\n        var = xp.var(self.input, axis=0, keepdims=True, dtype=DTYPE)\n\n        self.cache_m = self.momentum * self.cache_m + (1 - self.momentum) * mean\n        self.cache_v = self.momentum * self.cache_v + (1 - self.momentum) * var\n\n    else:\n        mean = self.cache_m\n        var = self.cache_v\n\n    self.std_inv = 1 / xp.sqrt(var + self.epsilon, dtype=DTYPE)\n    self.x_centered = self.input - mean\n    self.x_norm = self.x_centered * self.std_inv\n\n    res: ArrayType = self.x_norm * self.gamma\n    res += self.beta\n    return res\n</code></pre>"},{"location":"reference/layers/#2d-layers-convolutional","title":"2D Layers (Convolutional)","text":"<p>Layers designed for processing grid-like data (e.g., images) using the <code>im2col</code> optimization.</p>"},{"location":"reference/layers/#mpneuralnetwork.layers.Convolutional","title":"<code>mpneuralnetwork.layers.Convolutional</code>","text":"<p>               Bases: <code>Layer</code></p> <p>2D Convolutional Layer.</p> <p>Applies a 2D convolution over an input signal composed of several input planes. Uses the <code>im2col</code> optimization to convert convolution into matrix multiplication, allowing for efficient vectorization.</p> <p>Attributes:</p> Name Type Description <code>output_depth</code> <code>int</code> <p>Number of output channels (filters).</p> <code>kernel_size</code> <code>int</code> <p>Size of the square convolution kernel.</p> <code>stride</code> <code>int</code> <p>Step size of the convolution.</p> <code>padding</code> <code>int</code> <p>Amount of zero-padding applied to both sides of the input.</p> <code>initialization</code> <code>Lit_W</code> <p>Weight initialization strategy.</p> <code>no_bias</code> <code>bool</code> <p>Whether to disable bias.</p> <code>kernels</code> <code>ArrayType</code> <p>Learnable filters (output_depth, input_depth, k, k).</p> <code>biases</code> <code>ArrayType</code> <p>Learnable biases (output_depth,).</p> Source code in <code>src/mpneuralnetwork/layers/layer2d.py</code> <pre><code>class Convolutional(Layer):\n    \"\"\"2D Convolutional Layer.\n\n    Applies a 2D convolution over an input signal composed of several input planes.\n    Uses the `im2col` optimization to convert convolution into matrix multiplication,\n    allowing for efficient vectorization.\n\n    Attributes:\n        output_depth (int): Number of output channels (filters).\n        kernel_size (int): Size of the square convolution kernel.\n        stride (int): Step size of the convolution.\n        padding (int): Amount of zero-padding applied to both sides of the input.\n        initialization (Lit_W): Weight initialization strategy.\n        no_bias (bool): Whether to disable bias.\n        kernels (ArrayType): Learnable filters (output_depth, input_depth, k, k).\n        biases (ArrayType): Learnable biases (output_depth,).\n    \"\"\"\n\n    def __init__(\n        self,\n        output_depth: int,\n        kernel_size: int,\n        input_shape: tuple | None = None,\n        initialization: Lit_W = \"auto\",\n        no_bias: bool = False,\n        padding: int | Literal[\"valid\", \"same\"] = \"valid\",\n        stride: int = 1,\n    ) -&gt; None:\n        \"\"\"Initializes the Convolutional layer.\n\n        Args:\n            output_depth (int): Number of filters.\n            kernel_size (int): Height/Width of the filter (assumed square).\n            input_shape (tuple | None, optional): Shape of input (depth, height, width).\n            initialization (Lit_W, optional): Weight init method (\"auto\", \"he\", \"xavier\").\n            no_bias (bool, optional): Disable bias. Defaults to False.\n            padding (int | str, optional): Padding strategy. Can be an integer (amount of padding),\n                \"valid\" (no padding), or \"same\" (padding to preserve spatial dimensions with stride=1).\n                Defaults to \"valid\".\n            stride (int, optional): Stride of the convolution. Defaults to 1.\n        \"\"\"\n        super().__init__()\n        self.output_depth: int = output_depth\n        self.kernel_size: int = kernel_size\n        self.initialization: Lit_W = initialization\n        self.no_bias: bool = no_bias\n        self.stride: int = stride\n        self.padding_arg: int | Literal[\"valid\", \"same\"] = padding\n\n        self.padding: int\n        if self.padding_arg == \"valid\":\n            self.padding = 0\n        elif self.padding_arg == \"same\":\n            self.padding = (self.kernel_size - 1) // 2\n        elif isinstance(self.padding_arg, int):\n            self.padding = self.padding_arg\n        else:\n            raise ValueError(\"Padding must be 'valid', 'same', or an integer.\")\n\n        self.kernels: ArrayType\n        self.kernels_gradient: ArrayType\n        self.biases: ArrayType\n        self.biases_gradient: ArrayType\n        self.input_padded_shape: tuple\n\n        if input_shape is not None:\n            self.build(input_shape)\n\n    def get_config(self) -&gt; dict:\n        config = super().get_config()\n        config.update(\n            {\n                \"output_depth\": self.output_depth,\n                \"kernel_size\": self.kernel_size,\n                \"input_shape\": self.input_shape,\n                \"initialization\": self.initialization,\n                \"no_bias\": self.no_bias,\n                \"stride\": self.stride,\n                \"padding\": self.padding_arg,\n            }\n        )\n        return config\n\n    def build(self, input_shape: int | tuple[int, ...]) -&gt; None:\n        super().build(input_shape)\n\n        _, input_height, input_width = self.input_shape\n\n        output_height = (input_height - self.kernel_size + 2 * self.padding) // self.stride + 1\n        output_width = (input_width - self.kernel_size + 2 * self.padding) // self.stride + 1\n        self.output_shape = (self.output_depth, output_height, output_width)\n\n        if self.initialization != \"auto\":\n            self.init_weights(self.initialization, self.no_bias)\n\n    def init_weights(self, method: Lit_W, no_bias: bool) -&gt; None:\n        \"\"\"Initializes kernels and biases.\"\"\"\n        std_dev = 0.1\n\n        input_depth, _, _ = self.input_shape\n\n        if method == \"he\":\n            std_dev = xp.sqrt(2.0 / (input_depth * self.kernel_size * self.kernel_size), dtype=DTYPE)\n        elif method == \"xavier\":\n            std_dev = xp.sqrt(1.0 / (input_depth * self.kernel_size * self.kernel_size), dtype=DTYPE)\n\n        kernels_shape = (\n            self.output_depth,\n            input_depth,\n            self.kernel_size,\n            self.kernel_size,\n        )\n\n        self.kernels = xp.random.randn(*kernels_shape).astype(DTYPE) * std_dev\n        self.kernels_gradient = xp.zeros_like(self.kernels, dtype=DTYPE)\n\n        self.no_bias = no_bias\n\n        if not self.no_bias:\n            self.biases = xp.random.randn(self.output_depth).astype(DTYPE)\n            self.biases_gradient = xp.zeros_like(self.biases, dtype=DTYPE)\n\n    def forward(self, input_batch: ArrayType, training: bool = True) -&gt; ArrayType:\n        \"\"\"Performs 2D Convolution.\n\n        Args:\n            input_batch (ArrayType): Input data (N, C_in, H, W).\n            training (bool, optional): Unused. Defaults to True.\n\n        Returns:\n            ArrayType: Feature maps (N, C_out, H_out, W_out).\n        \"\"\"\n        self.input = input_batch\n\n        if self.padding &gt; 0:\n            input_batch_padded = xp.pad(\n                input_batch,\n                (\n                    (0, 0),\n                    (0, 0),\n                    (self.padding, self.padding),\n                    (self.padding, self.padding),\n                ),\n            )\n        else:\n            input_batch_padded = input_batch\n\n        self.input_padded_shape = input_batch_padded.shape\n\n        input_windows = im2col(input_batch_padded, self.kernel_size, self.stride)\n\n        output = xp.tensordot(input_windows, self.kernels, axes=((3, 4, 5), (1, 2, 3)))\n\n        if not self.no_bias:\n            output += self.biases\n\n        return output.transpose(0, 3, 1, 2)  # type: ignore[no-any-return]\n\n    def backward(self, output_gradient_batch: ArrayType) -&gt; ArrayType:\n        \"\"\"Backpropagates gradients through convolution.\n\n        Uses `col2im` to reconstruct the gradient for the input image.\n\n        Args:\n            output_gradient_batch (ArrayType): Gradient w.r.t output.\n\n        Returns:\n            ArrayType: Gradient w.r.t input.\n        \"\"\"\n        grad_transposed = output_gradient_batch.transpose(0, 2, 3, 1)\n\n        if not self.no_bias:\n            self.biases_gradient = xp.sum(grad_transposed, axis=(0, 1, 2), dtype=DTYPE)\n\n        if self.padding &gt; 0:\n            input_batch_padded = xp.pad(\n                self.input,\n                (\n                    (0, 0),\n                    (0, 0),\n                    (self.padding, self.padding),\n                    (self.padding, self.padding),\n                ),\n            )\n        else:\n            input_batch_padded = self.input\n\n        input_windows = im2col(input_batch_padded, self.kernel_size, self.stride)\n\n        self.kernels_gradient = xp.tensordot(grad_transposed, input_windows, axes=((0, 1, 2), (0, 1, 2)))\n\n        input_grad_windows = xp.tensordot(grad_transposed, self.kernels, axes=((3), (0)))\n\n        input_grad_windows_transposed = input_grad_windows.transpose(0, 3, 1, 2, 4, 5)\n\n        input_grad_padded = col2im(\n            input_grad_windows_transposed,\n            self.input_padded_shape,\n            self.output_shape,\n            self.kernel_size,\n            self.stride,\n        )\n\n        if self.padding &gt; 0:\n            input_grad = input_grad_padded[:, :, self.padding : -self.padding, self.padding : -self.padding]\n        else:\n            input_grad = input_grad_padded\n\n        return input_grad\n\n    @property\n    def params(self) -&gt; dict[str, tuple[ArrayType, ArrayType]]:\n        return {\n            \"kernels\": (self.kernels, self.kernels_gradient),\n            \"biases\": (self.biases, self.biases_gradient),\n        }\n\n    def load_params(self, params: dict[str, ArrayType]) -&gt; None:\n        self.kernels[:] = params[\"kernels\"]\n        self.biases[:] = params[\"biases\"]\n</code></pre>"},{"location":"reference/layers/#mpneuralnetwork.layers.Convolutional.__init__","title":"<code>__init__(output_depth, kernel_size, input_shape=None, initialization='auto', no_bias=False, padding='valid', stride=1)</code>","text":"<p>Initializes the Convolutional layer.</p> <p>Parameters:</p> Name Type Description Default <code>output_depth</code> <code>int</code> <p>Number of filters.</p> required <code>kernel_size</code> <code>int</code> <p>Height/Width of the filter (assumed square).</p> required <code>input_shape</code> <code>tuple | None</code> <p>Shape of input (depth, height, width).</p> <code>None</code> <code>initialization</code> <code>Lit_W</code> <p>Weight init method (\"auto\", \"he\", \"xavier\").</p> <code>'auto'</code> <code>no_bias</code> <code>bool</code> <p>Disable bias. Defaults to False.</p> <code>False</code> <code>padding</code> <code>int | str</code> <p>Padding strategy. Can be an integer (amount of padding), \"valid\" (no padding), or \"same\" (padding to preserve spatial dimensions with stride=1). Defaults to \"valid\".</p> <code>'valid'</code> <code>stride</code> <code>int</code> <p>Stride of the convolution. Defaults to 1.</p> <code>1</code> Source code in <code>src/mpneuralnetwork/layers/layer2d.py</code> <pre><code>def __init__(\n    self,\n    output_depth: int,\n    kernel_size: int,\n    input_shape: tuple | None = None,\n    initialization: Lit_W = \"auto\",\n    no_bias: bool = False,\n    padding: int | Literal[\"valid\", \"same\"] = \"valid\",\n    stride: int = 1,\n) -&gt; None:\n    \"\"\"Initializes the Convolutional layer.\n\n    Args:\n        output_depth (int): Number of filters.\n        kernel_size (int): Height/Width of the filter (assumed square).\n        input_shape (tuple | None, optional): Shape of input (depth, height, width).\n        initialization (Lit_W, optional): Weight init method (\"auto\", \"he\", \"xavier\").\n        no_bias (bool, optional): Disable bias. Defaults to False.\n        padding (int | str, optional): Padding strategy. Can be an integer (amount of padding),\n            \"valid\" (no padding), or \"same\" (padding to preserve spatial dimensions with stride=1).\n            Defaults to \"valid\".\n        stride (int, optional): Stride of the convolution. Defaults to 1.\n    \"\"\"\n    super().__init__()\n    self.output_depth: int = output_depth\n    self.kernel_size: int = kernel_size\n    self.initialization: Lit_W = initialization\n    self.no_bias: bool = no_bias\n    self.stride: int = stride\n    self.padding_arg: int | Literal[\"valid\", \"same\"] = padding\n\n    self.padding: int\n    if self.padding_arg == \"valid\":\n        self.padding = 0\n    elif self.padding_arg == \"same\":\n        self.padding = (self.kernel_size - 1) // 2\n    elif isinstance(self.padding_arg, int):\n        self.padding = self.padding_arg\n    else:\n        raise ValueError(\"Padding must be 'valid', 'same', or an integer.\")\n\n    self.kernels: ArrayType\n    self.kernels_gradient: ArrayType\n    self.biases: ArrayType\n    self.biases_gradient: ArrayType\n    self.input_padded_shape: tuple\n\n    if input_shape is not None:\n        self.build(input_shape)\n</code></pre>"},{"location":"reference/layers/#mpneuralnetwork.layers.Convolutional.backward","title":"<code>backward(output_gradient_batch)</code>","text":"<p>Backpropagates gradients through convolution.</p> <p>Uses <code>col2im</code> to reconstruct the gradient for the input image.</p> <p>Parameters:</p> Name Type Description Default <code>output_gradient_batch</code> <code>ArrayType</code> <p>Gradient w.r.t output.</p> required <p>Returns:</p> Name Type Description <code>ArrayType</code> <code>ArrayType</code> <p>Gradient w.r.t input.</p> Source code in <code>src/mpneuralnetwork/layers/layer2d.py</code> <pre><code>def backward(self, output_gradient_batch: ArrayType) -&gt; ArrayType:\n    \"\"\"Backpropagates gradients through convolution.\n\n    Uses `col2im` to reconstruct the gradient for the input image.\n\n    Args:\n        output_gradient_batch (ArrayType): Gradient w.r.t output.\n\n    Returns:\n        ArrayType: Gradient w.r.t input.\n    \"\"\"\n    grad_transposed = output_gradient_batch.transpose(0, 2, 3, 1)\n\n    if not self.no_bias:\n        self.biases_gradient = xp.sum(grad_transposed, axis=(0, 1, 2), dtype=DTYPE)\n\n    if self.padding &gt; 0:\n        input_batch_padded = xp.pad(\n            self.input,\n            (\n                (0, 0),\n                (0, 0),\n                (self.padding, self.padding),\n                (self.padding, self.padding),\n            ),\n        )\n    else:\n        input_batch_padded = self.input\n\n    input_windows = im2col(input_batch_padded, self.kernel_size, self.stride)\n\n    self.kernels_gradient = xp.tensordot(grad_transposed, input_windows, axes=((0, 1, 2), (0, 1, 2)))\n\n    input_grad_windows = xp.tensordot(grad_transposed, self.kernels, axes=((3), (0)))\n\n    input_grad_windows_transposed = input_grad_windows.transpose(0, 3, 1, 2, 4, 5)\n\n    input_grad_padded = col2im(\n        input_grad_windows_transposed,\n        self.input_padded_shape,\n        self.output_shape,\n        self.kernel_size,\n        self.stride,\n    )\n\n    if self.padding &gt; 0:\n        input_grad = input_grad_padded[:, :, self.padding : -self.padding, self.padding : -self.padding]\n    else:\n        input_grad = input_grad_padded\n\n    return input_grad\n</code></pre>"},{"location":"reference/layers/#mpneuralnetwork.layers.Convolutional.forward","title":"<code>forward(input_batch, training=True)</code>","text":"<p>Performs 2D Convolution.</p> <p>Parameters:</p> Name Type Description Default <code>input_batch</code> <code>ArrayType</code> <p>Input data (N, C_in, H, W).</p> required <code>training</code> <code>bool</code> <p>Unused. Defaults to True.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>ArrayType</code> <code>ArrayType</code> <p>Feature maps (N, C_out, H_out, W_out).</p> Source code in <code>src/mpneuralnetwork/layers/layer2d.py</code> <pre><code>def forward(self, input_batch: ArrayType, training: bool = True) -&gt; ArrayType:\n    \"\"\"Performs 2D Convolution.\n\n    Args:\n        input_batch (ArrayType): Input data (N, C_in, H, W).\n        training (bool, optional): Unused. Defaults to True.\n\n    Returns:\n        ArrayType: Feature maps (N, C_out, H_out, W_out).\n    \"\"\"\n    self.input = input_batch\n\n    if self.padding &gt; 0:\n        input_batch_padded = xp.pad(\n            input_batch,\n            (\n                (0, 0),\n                (0, 0),\n                (self.padding, self.padding),\n                (self.padding, self.padding),\n            ),\n        )\n    else:\n        input_batch_padded = input_batch\n\n    self.input_padded_shape = input_batch_padded.shape\n\n    input_windows = im2col(input_batch_padded, self.kernel_size, self.stride)\n\n    output = xp.tensordot(input_windows, self.kernels, axes=((3, 4, 5), (1, 2, 3)))\n\n    if not self.no_bias:\n        output += self.biases\n\n    return output.transpose(0, 3, 1, 2)  # type: ignore[no-any-return]\n</code></pre>"},{"location":"reference/layers/#mpneuralnetwork.layers.Convolutional.init_weights","title":"<code>init_weights(method, no_bias)</code>","text":"<p>Initializes kernels and biases.</p> Source code in <code>src/mpneuralnetwork/layers/layer2d.py</code> <pre><code>def init_weights(self, method: Lit_W, no_bias: bool) -&gt; None:\n    \"\"\"Initializes kernels and biases.\"\"\"\n    std_dev = 0.1\n\n    input_depth, _, _ = self.input_shape\n\n    if method == \"he\":\n        std_dev = xp.sqrt(2.0 / (input_depth * self.kernel_size * self.kernel_size), dtype=DTYPE)\n    elif method == \"xavier\":\n        std_dev = xp.sqrt(1.0 / (input_depth * self.kernel_size * self.kernel_size), dtype=DTYPE)\n\n    kernels_shape = (\n        self.output_depth,\n        input_depth,\n        self.kernel_size,\n        self.kernel_size,\n    )\n\n    self.kernels = xp.random.randn(*kernels_shape).astype(DTYPE) * std_dev\n    self.kernels_gradient = xp.zeros_like(self.kernels, dtype=DTYPE)\n\n    self.no_bias = no_bias\n\n    if not self.no_bias:\n        self.biases = xp.random.randn(self.output_depth).astype(DTYPE)\n        self.biases_gradient = xp.zeros_like(self.biases, dtype=DTYPE)\n</code></pre>"},{"location":"reference/layers/#mpneuralnetwork.layers.MaxPooling2D","title":"<code>mpneuralnetwork.layers.MaxPooling2D</code>","text":"<p>               Bases: <code>Layer</code></p> <p>Max Pooling 2D Layer.</p> <p>Downsamples the input by taking the maximum value over a window. Reduces spatial dimensions and computation, while providing translational invariance.</p> <p>Attributes:</p> Name Type Description <code>pool_size</code> <code>int</code> <p>Size of the pooling window.</p> <code>stride</code> <code>int</code> <p>Stride of the pooling operation.</p> Source code in <code>src/mpneuralnetwork/layers/layer2d.py</code> <pre><code>class MaxPooling2D(Layer):\n    \"\"\"Max Pooling 2D Layer.\n\n    Downsamples the input by taking the maximum value over a window.\n    Reduces spatial dimensions and computation, while providing translational invariance.\n\n    Attributes:\n        pool_size (int): Size of the pooling window.\n        stride (int): Stride of the pooling operation.\n    \"\"\"\n\n    def __init__(self, pool_size: int = 2, strides: int | None = None):\n        super().__init__()\n        self.pool_size: int = pool_size\n        self.stride: int = strides if strides is not None else pool_size\n\n        self.windows: ArrayType\n\n    def build(self, input_shape: int | tuple[int, ...]) -&gt; None:\n        super().build(input_shape)\n        C, H, W = self.input_shape\n\n        out_h = (H - self.pool_size) // self.stride + 1\n        out_w = (W - self.pool_size) // self.stride + 1\n\n        self.output_shape = (C, out_h, out_w)\n\n    def forward(self, input_batch: ArrayType, training: bool = True) -&gt; ArrayType:\n        self.input_shape = input_batch.shape\n        self.windows = im2col(input_batch, self.pool_size, self.stride)\n        max_val = xp.max(self.windows, axis=(4, 5))\n\n        return max_val.transpose(0, 3, 1, 2)  # type: ignore\n\n    def backward(self, output_gradient_batch: ArrayType) -&gt; ArrayType:\n        grad_transposed = output_gradient_batch.transpose(0, 2, 3, 1)\n\n        grad_expanded = grad_transposed[..., None, None]\n\n        max_vals = xp.max(self.windows, axis=(4, 5), keepdims=True)\n        mask = self.windows == max_vals\n\n        d_windows = grad_expanded * mask\n        d_windows = d_windows.transpose(0, 3, 1, 2, 4, 5)\n\n        output_shape_no_batch = output_gradient_batch.shape[1:]\n\n        return col2im(\n            d_windows,\n            self.input_shape,\n            output_shape_no_batch,\n            self.pool_size,\n            self.stride,\n        )\n</code></pre>"},{"location":"reference/layers/#mpneuralnetwork.layers.AveragePooling2D","title":"<code>mpneuralnetwork.layers.AveragePooling2D</code>","text":"<p>               Bases: <code>Layer</code></p> <p>Average Pooling 2D Layer.</p> <p>Downsamples the input by taking the average value over a window.</p> <p>Attributes:</p> Name Type Description <code>pool_size</code> <code>int</code> <p>Size of the pooling window.</p> <code>stride</code> <code>int</code> <p>Stride of the pooling operation.</p> Source code in <code>src/mpneuralnetwork/layers/layer2d.py</code> <pre><code>class AveragePooling2D(Layer):\n    \"\"\"Average Pooling 2D Layer.\n\n    Downsamples the input by taking the average value over a window.\n\n    Attributes:\n        pool_size (int): Size of the pooling window.\n        stride (int): Stride of the pooling operation.\n    \"\"\"\n\n    def __init__(self, pool_size: int = 2, strides: int | None = None):\n        super().__init__()\n        self.pool_size: int = pool_size\n        self.stride: int = strides if strides is not None else pool_size\n\n        self.windows: ArrayType\n\n    def build(self, input_shape: int | tuple[int, ...]) -&gt; None:\n        super().build(input_shape)\n        C, H, W = self.input_shape\n\n        out_h = (H - self.pool_size) // self.stride + 1\n        out_w = (W - self.pool_size) // self.stride + 1\n\n        self.output_shape = (C, out_h, out_w)\n\n    def forward(self, input_batch: ArrayType, training: bool = True) -&gt; ArrayType:\n        self.input_shape = input_batch.shape\n        self.windows = im2col(input_batch, self.pool_size, self.stride)\n        means = xp.mean(self.windows, axis=(4, 5), dtype=DTYPE)\n\n        return means.transpose(0, 3, 1, 2)  # type: ignore\n\n    def backward(self, output_gradient_batch: ArrayType) -&gt; ArrayType:\n        grad_transposed = output_gradient_batch.transpose(0, 2, 3, 1)\n\n        grad_expanded = grad_transposed[..., None, None]\n\n        d_windows = grad_expanded * xp.ones_like(self.windows, dtype=DTYPE) / (self.pool_size * self.pool_size)\n        d_windows = d_windows.transpose(0, 3, 1, 2, 4, 5)\n\n        output_shape_no_batch = output_gradient_batch.shape[1:]\n\n        return col2im(\n            d_windows,\n            self.input_shape,\n            output_shape_no_batch,\n            self.pool_size,\n            self.stride,\n        )\n</code></pre>"},{"location":"reference/layers/#mpneuralnetwork.layers.Flatten","title":"<code>mpneuralnetwork.layers.Flatten</code>","text":"<p>               Bases: <code>Layer</code></p> <p>Flatten Layer.</p> <p>Flattens the input tensor into a 1D tensor (vector) per sample. Crucial for connecting Convolutional/Pooling layers to Dense layers.</p> <p>Input: (Batch, Channel, Height, Width) Output: (Batch, Channel * Height * Width)</p> Source code in <code>src/mpneuralnetwork/layers/layer2d.py</code> <pre><code>class Flatten(Layer):\n    \"\"\"Flatten Layer.\n\n    Flattens the input tensor into a 1D tensor (vector) per sample.\n    Crucial for connecting Convolutional/Pooling layers to Dense layers.\n\n    Input: (Batch, Channel, Height, Width)\n    Output: (Batch, Channel * Height * Width)\n    \"\"\"\n\n    def forward(self, input_batch: ArrayType, training: bool = True) -&gt; ArrayType:\n        return input_batch.reshape(input_batch.shape[0], -1)\n\n    def backward(self, output_gradient_batch: ArrayType) -&gt; ArrayType:\n        return output_gradient_batch.reshape(output_gradient_batch.shape[0], *self.input_shape)\n</code></pre>"},{"location":"reference/layers/#mpneuralnetwork.layers.BatchNormalization2D","title":"<code>mpneuralnetwork.layers.BatchNormalization2D</code>","text":"<p>               Bases: <code>Layer</code></p> <p>Batch Normalization Layer (2D) for Convolutional Networks.</p> <p>Normalize the activations of the previous layer at each batch. Operates on the channel dimension (axis 1), so statistics are computed over (Batch, Height, Width).</p> <p>Attributes:</p> Name Type Description <code>momentum</code> <code>float</code> <p>Momentum for the moving average.</p> <code>epsilon</code> <code>float</code> <p>Small float added to variance to avoid dividing by zero.</p> Source code in <code>src/mpneuralnetwork/layers/layer2d.py</code> <pre><code>class BatchNormalization2D(Layer):\n    \"\"\"Batch Normalization Layer (2D) for Convolutional Networks.\n\n    Normalize the activations of the previous layer at each batch.\n    Operates on the channel dimension (axis 1), so statistics are computed\n    over (Batch, Height, Width).\n\n    Attributes:\n        momentum (float): Momentum for the moving average.\n        epsilon (float): Small float added to variance to avoid dividing by zero.\n    \"\"\"\n\n    def __init__(self, momentum: float = 0.9, epsilon: float = 1e-8) -&gt; None:\n        super().__init__()\n        self.momentum: float = momentum\n        self.epsilon: float = epsilon\n\n        self.gamma: ArrayType\n        self.beta: ArrayType\n\n        self.cache_m: ArrayType\n        self.cache_v: ArrayType\n\n    def build(self, input_shape: int | tuple[int, ...]) -&gt; None:\n        super().build(input_shape)\n        C, H, W = self.input_shape\n\n        self.gamma = xp.ones((1, C, 1, 1), dtype=DTYPE)\n        self.gamma_gradient = xp.zeros_like(self.gamma, dtype=DTYPE)\n\n        self.beta = xp.zeros((1, C, 1, 1), dtype=DTYPE)\n        self.beta_gradient = xp.zeros_like(self.beta, dtype=DTYPE)\n\n        self.cache_m = xp.zeros((1, C, 1, 1), dtype=DTYPE)\n        self.cache_v = xp.ones((1, C, 1, 1), dtype=DTYPE)\n\n        self.std_inv: ArrayType\n        self.x_centered: ArrayType\n        self.x_norm: ArrayType\n\n    def get_config(self) -&gt; dict:\n        config = super().get_config()\n        config.update({\"momentum\": self.momentum, \"epsilon\": self.epsilon})\n        return config\n\n    def forward(self, input_batch: ArrayType, training: bool = True) -&gt; ArrayType:\n        \"\"\"Performs spatial batch normalization.\n\n        Args:\n            input_batch (ArrayType): Input (N, C, H, W).\n            training (bool, optional): If True, updates running stats.\n\n        Returns:\n            ArrayType: Normalized input.\n        \"\"\"\n        self.input = input_batch\n\n        mean: ArrayType\n        var: ArrayType\n\n        if training:\n            mean = xp.mean(self.input, axis=(0, 2, 3), keepdims=True, dtype=DTYPE)  # type: ignore\n            var = xp.var(self.input, axis=(0, 2, 3), keepdims=True, dtype=DTYPE)\n\n            self.cache_m = self.momentum * self.cache_m + (1 - self.momentum) * mean\n            self.cache_v = self.momentum * self.cache_v + (1 - self.momentum) * var\n\n        else:\n            mean = self.cache_m\n            var = self.cache_v\n\n        self.std_inv = 1 / xp.sqrt(var + self.epsilon, dtype=DTYPE)\n        self.x_centered = self.input - mean\n        self.x_norm = self.x_centered * self.std_inv\n\n        res: ArrayType = self.x_norm * self.gamma\n        res += self.beta\n        return res\n\n    def backward(self, output_gradient_batch: ArrayType) -&gt; ArrayType:\n        self.gamma_gradient = xp.sum(  # type: ignore\n            self.x_norm * output_gradient_batch,\n            axis=(0, 2, 3),\n            keepdims=True,\n            dtype=DTYPE,\n        )\n        self.beta_gradient = xp.sum(output_gradient_batch, axis=(0, 2, 3), keepdims=True, dtype=DTYPE)  # type: ignore\n\n        N = output_gradient_batch.shape[0] * output_gradient_batch.shape[2] * output_gradient_batch.shape[3]\n\n        dx_norm = output_gradient_batch * self.gamma\n\n        grad: ArrayType = (\n            (1 / N)\n            * self.std_inv\n            * (\n                N * dx_norm\n                - xp.sum(dx_norm, axis=(0, 2, 3), keepdims=True, dtype=DTYPE)\n                - self.x_norm * xp.sum(dx_norm * self.x_norm, axis=(0, 2, 3), keepdims=True, dtype=DTYPE)\n            )\n        )\n        return grad\n\n    @property\n    def params(self) -&gt; dict[str, tuple[ArrayType, ArrayType]]:\n        return {  # type: ignore\n            \"gamma\": (self.gamma, self.gamma_gradient),\n            \"beta\": (self.beta, self.beta_gradient),\n        }\n\n    def load_params(self, params: dict[str, ArrayType]) -&gt; None:\n        self.gamma[:] = params[\"gamma\"]\n        self.beta[:] = params[\"beta\"]\n\n    @property\n    def state(self) -&gt; dict[str, ArrayType]:\n        return {\"cache_m\": self.cache_m, \"cache_v\": self.cache_v}\n\n    @state.setter\n    def state(self, state: dict[str, ArrayType]) -&gt; None:\n        self.cache_m = state[\"cache_m\"]\n        self.cache_v = state[\"cache_v\"]\n</code></pre>"},{"location":"reference/layers/#mpneuralnetwork.layers.BatchNormalization2D.forward","title":"<code>forward(input_batch, training=True)</code>","text":"<p>Performs spatial batch normalization.</p> <p>Parameters:</p> Name Type Description Default <code>input_batch</code> <code>ArrayType</code> <p>Input (N, C, H, W).</p> required <code>training</code> <code>bool</code> <p>If True, updates running stats.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>ArrayType</code> <code>ArrayType</code> <p>Normalized input.</p> Source code in <code>src/mpneuralnetwork/layers/layer2d.py</code> <pre><code>def forward(self, input_batch: ArrayType, training: bool = True) -&gt; ArrayType:\n    \"\"\"Performs spatial batch normalization.\n\n    Args:\n        input_batch (ArrayType): Input (N, C, H, W).\n        training (bool, optional): If True, updates running stats.\n\n    Returns:\n        ArrayType: Normalized input.\n    \"\"\"\n    self.input = input_batch\n\n    mean: ArrayType\n    var: ArrayType\n\n    if training:\n        mean = xp.mean(self.input, axis=(0, 2, 3), keepdims=True, dtype=DTYPE)  # type: ignore\n        var = xp.var(self.input, axis=(0, 2, 3), keepdims=True, dtype=DTYPE)\n\n        self.cache_m = self.momentum * self.cache_m + (1 - self.momentum) * mean\n        self.cache_v = self.momentum * self.cache_v + (1 - self.momentum) * var\n\n    else:\n        mean = self.cache_m\n        var = self.cache_v\n\n    self.std_inv = 1 / xp.sqrt(var + self.epsilon, dtype=DTYPE)\n    self.x_centered = self.input - mean\n    self.x_norm = self.x_centered * self.std_inv\n\n    res: ArrayType = self.x_norm * self.gamma\n    res += self.beta\n    return res\n</code></pre>"},{"location":"reference/layers/#utilities","title":"Utilities","text":"<p>Low-level utility functions used for implementing efficient convolutions.</p>"},{"location":"reference/layers/#mpneuralnetwork.layers.utils.im2col","title":"<code>mpneuralnetwork.layers.utils.im2col(input_batch, window_size, stride=None)</code>","text":"<p>Image to Column transformation.</p> <p>Rearranges image blocks into columns to perform convolution as a matrix multiplication. This is the core optimization that enables vectorized convolution.</p> Transformation <p>Input:  (N, C, H, W) Output: (N, H_out, W_out, C * K * K) (before flattening for matmul)</p> <p>Parameters:</p> Name Type Description Default <code>input_batch</code> <code>ArrayType</code> <p>Input images of shape (N, C, H, W).</p> required <code>window_size</code> <code>int</code> <p>Size of the kernel (K).</p> required <code>stride</code> <code>int | None</code> <p>Stride of the operation. Defaults to window_size if None (for pooling) or 1.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>ArrayType</code> <code>ArrayType</code> <p>The column matrix ready for GEMM (General Matrix Multiplication).</p> Source code in <code>src/mpneuralnetwork/layers/utils.py</code> <pre><code>def im2col(input_batch: ArrayType, window_size: int, stride: int | None = None) -&gt; ArrayType:\n    \"\"\"Image to Column transformation.\n\n    Rearranges image blocks into columns to perform convolution as a matrix multiplication.\n    This is the core optimization that enables vectorized convolution.\n\n    Transformation:\n        Input:  (N, C, H, W)\n        Output: (N, H_out, W_out, C * K * K) (before flattening for matmul)\n\n    Args:\n        input_batch (ArrayType): Input images of shape (N, C, H, W).\n        window_size (int): Size of the kernel (K).\n        stride (int | None, optional): Stride of the operation. Defaults to window_size if None (for pooling) or 1.\n\n    Returns:\n        ArrayType: The column matrix ready for GEMM (General Matrix Multiplication).\n    \"\"\"\n    windows = xp.lib.stride_tricks.sliding_window_view(input_batch, window_shape=(window_size, window_size), axis=(2, 3))  # type: ignore[call-overload]\n\n    if stride is not None:\n        windows = windows[:, :, ::stride, ::stride, :, :]\n\n    return windows.transpose(0, 2, 3, 1, 4, 5)  # type: ignore[no-any-return]\n</code></pre>"},{"location":"reference/layers/#mpneuralnetwork.layers.utils.col2im","title":"<code>mpneuralnetwork.layers.utils.col2im(cols, input_shape, output_shape, window_size, stride=1)</code>","text":"<p>Column to Image transformation (Reverse im2col).</p> <p>Used during backpropagation to reconstruct the gradient of the input image from the gradients of the columns. Accumulates gradients in overlapping regions.</p> <p>Parameters:</p> Name Type Description Default <code>cols</code> <code>ArrayType</code> <p>The column matrix from the gradient calculation.</p> required <code>input_shape</code> <code>tuple[int, ...]</code> <p>Original shape of the input image (N, C, H, W).</p> required <code>output_shape</code> <code>tuple[int, ...]</code> <p>Shape of the output (N, C_out, H_out, W_out).</p> required <code>window_size</code> <code>int</code> <p>Kernel size.</p> required <code>stride</code> <code>int</code> <p>Stride. Defaults to 1.</p> <code>1</code> <p>Returns:</p> Name Type Description <code>ArrayType</code> <code>ArrayType</code> <p>Reconstructed image gradient of shape (N, C, H, W).</p> Source code in <code>src/mpneuralnetwork/layers/utils.py</code> <pre><code>def col2im(\n    cols: ArrayType,\n    input_shape: tuple[int, ...],\n    output_shape: tuple[int, ...],\n    window_size: int,\n    stride: int = 1,\n) -&gt; ArrayType:\n    \"\"\"Column to Image transformation (Reverse im2col).\n\n    Used during backpropagation to reconstruct the gradient of the input image\n    from the gradients of the columns. Accumulates gradients in overlapping regions.\n\n    Args:\n        cols (ArrayType): The column matrix from the gradient calculation.\n        input_shape (tuple[int, ...]): Original shape of the input image (N, C, H, W).\n        output_shape (tuple[int, ...]): Shape of the output (N, C_out, H_out, W_out).\n        window_size (int): Kernel size.\n        stride (int, optional): Stride. Defaults to 1.\n\n    Returns:\n        ArrayType: Reconstructed image gradient of shape (N, C, H, W).\n    \"\"\"\n    _, H_out, W_out = output_shape\n    K = window_size\n\n    im = xp.zeros(input_shape, dtype=DTYPE)\n\n    for i in range(K):\n        for j in range(K):\n            im[:, :, i : i + H_out * stride : stride, j : j + W_out * stride : stride] += cols[:, :, :, :, i, j]\n\n    return im\n</code></pre>"},{"location":"reference/losses/","title":"Loss Functions API","text":"<p>Loss functions quantify how well the model's predictions match the ground truth. They provide the gradient signal used for optimization.</p>"},{"location":"reference/losses/#base-loss","title":"Base Loss","text":""},{"location":"reference/losses/#mpneuralnetwork.losses.Loss","title":"<code>mpneuralnetwork.losses.Loss</code>","text":"<p>Base class for loss functions.</p> <p>Computes the error between the model's predictions and the true values, as well as the gradient of this error for backpropagation.</p> Source code in <code>src/mpneuralnetwork/losses.py</code> <pre><code>class Loss:\n    \"\"\"Base class for loss functions.\n\n    Computes the error between the model's predictions and the true values,\n    as well as the gradient of this error for backpropagation.\n    \"\"\"\n\n    @abstractmethod\n    def direct(self, output: ArrayType, output_expected: ArrayType) -&gt; float:\n        \"\"\"Computes the scalar loss value.\n\n        Args:\n            output (ArrayType): Model predictions.\n            output_expected (ArrayType): Ground truth values.\n\n        Returns:\n            float: The loss value.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def prime(self, output: ArrayType, output_expected: ArrayType) -&gt; ArrayType:\n        \"\"\"Computes the gradient of the loss function w.r.t the input.\n\n        Args:\n            output (ArrayType): Model predictions.\n            output_expected (ArrayType): Ground truth values.\n\n        Returns:\n            ArrayType: Gradient of the loss.\n        \"\"\"\n        pass\n\n    def get_config(self) -&gt; dict:\n        return {\"type\": self.__class__.__name__}\n</code></pre>"},{"location":"reference/losses/#mpneuralnetwork.losses.Loss.direct","title":"<code>direct(output, output_expected)</code>  <code>abstractmethod</code>","text":"<p>Computes the scalar loss value.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>ArrayType</code> <p>Model predictions.</p> required <code>output_expected</code> <code>ArrayType</code> <p>Ground truth values.</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>The loss value.</p> Source code in <code>src/mpneuralnetwork/losses.py</code> <pre><code>@abstractmethod\ndef direct(self, output: ArrayType, output_expected: ArrayType) -&gt; float:\n    \"\"\"Computes the scalar loss value.\n\n    Args:\n        output (ArrayType): Model predictions.\n        output_expected (ArrayType): Ground truth values.\n\n    Returns:\n        float: The loss value.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/losses/#mpneuralnetwork.losses.Loss.prime","title":"<code>prime(output, output_expected)</code>  <code>abstractmethod</code>","text":"<p>Computes the gradient of the loss function w.r.t the input.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>ArrayType</code> <p>Model predictions.</p> required <code>output_expected</code> <code>ArrayType</code> <p>Ground truth values.</p> required <p>Returns:</p> Name Type Description <code>ArrayType</code> <code>ArrayType</code> <p>Gradient of the loss.</p> Source code in <code>src/mpneuralnetwork/losses.py</code> <pre><code>@abstractmethod\ndef prime(self, output: ArrayType, output_expected: ArrayType) -&gt; ArrayType:\n    \"\"\"Computes the gradient of the loss function w.r.t the input.\n\n    Args:\n        output (ArrayType): Model predictions.\n        output_expected (ArrayType): Ground truth values.\n\n    Returns:\n        ArrayType: Gradient of the loss.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/losses/#regression","title":"Regression","text":""},{"location":"reference/losses/#mpneuralnetwork.losses.MSE","title":"<code>mpneuralnetwork.losses.MSE</code>","text":"<p>               Bases: <code>Loss</code></p> <p>Mean Squared Error (MSE) Loss.</p> Formula <p><code>L = (1/N) * sum((y_pred - y_true)^2)</code></p> Derivative <p><code>dL/dy_pred = (2/N) * (y_pred - y_true)</code></p> <p>Used for regression problems.</p> Source code in <code>src/mpneuralnetwork/losses.py</code> <pre><code>class MSE(Loss):\n    \"\"\"Mean Squared Error (MSE) Loss.\n\n    Formula:\n        `L = (1/N) * sum((y_pred - y_true)^2)`\n\n    Derivative:\n        `dL/dy_pred = (2/N) * (y_pred - y_true)`\n\n    Used for regression problems.\n    \"\"\"\n\n    def direct(self, output: ArrayType, output_expected: ArrayType) -&gt; float:\n        res: float = xp.mean(\n            xp.sum(xp.square(output_expected - output), axis=1, dtype=DTYPE),\n            dtype=DTYPE,\n        )\n        return res\n\n    def prime(self, output: ArrayType, output_expected: ArrayType) -&gt; ArrayType:\n        grad: ArrayType = 2 * (output - output_expected) / output.shape[0]\n        return grad\n</code></pre>"},{"location":"reference/losses/#classification","title":"Classification","text":""},{"location":"reference/losses/#mpneuralnetwork.losses.BinaryCrossEntropy","title":"<code>mpneuralnetwork.losses.BinaryCrossEntropy</code>","text":"<p>               Bases: <code>Loss</code></p> <p>Binary Cross Entropy Loss.</p> <p>Formula (conceptually):     <code>L = - (y * log(p) + (1-y) * log(1-p))</code></p> Implementation details <p>Assumes the input <code>output</code> corresponds to LOGITS (raw scores), not probabilities. The Sigmoid activation is applied internally using the numerically stable log-sum-exp trick.</p> <p>Used for binary classification problems.</p> Source code in <code>src/mpneuralnetwork/losses.py</code> <pre><code>class BinaryCrossEntropy(Loss):\n    \"\"\"Binary Cross Entropy Loss.\n\n    Formula (conceptually):\n        `L = - (y * log(p) + (1-y) * log(1-p))`\n\n    Implementation details:\n        Assumes the input `output` corresponds to **LOGITS** (raw scores), not probabilities.\n        The Sigmoid activation is applied internally using the numerically stable log-sum-exp trick.\n\n    Used for binary classification problems.\n    \"\"\"\n\n    def __init__(self) -&gt; None:\n        self.sigmoid = Sigmoid()\n\n    def direct(self, output: ArrayType, output_expected: ArrayType) -&gt; float:\n        loss_per_element = xp.maximum(output, 0) - output * output_expected + xp.log(1 + xp.exp(-xp.abs(output), dtype=DTYPE), dtype=DTYPE)\n        res: float = xp.mean(xp.sum(loss_per_element, axis=1, dtype=DTYPE), dtype=DTYPE)\n        return res\n\n    def prime(self, output: ArrayType, output_expected: ArrayType) -&gt; ArrayType:\n        predictions = self.sigmoid.forward(output)\n        grad: ArrayType = (predictions - output_expected) / output.shape[0]\n        return grad\n</code></pre>"},{"location":"reference/losses/#mpneuralnetwork.losses.CategoricalCrossEntropy","title":"<code>mpneuralnetwork.losses.CategoricalCrossEntropy</code>","text":"<p>               Bases: <code>Loss</code></p> <p>Categorical Cross Entropy Loss.</p> <p>Formula (conceptually):     <code>L = - sum(y_i * log(p_i))</code></p> Implementation details <p>Assumes the input <code>output</code> corresponds to LOGITS. The Softmax activation is applied internally.</p> <p>Used for multi-class classification problems (one-hot encoded targets).</p> Source code in <code>src/mpneuralnetwork/losses.py</code> <pre><code>class CategoricalCrossEntropy(Loss):\n    \"\"\"Categorical Cross Entropy Loss.\n\n    Formula (conceptually):\n        `L = - sum(y_i * log(p_i))`\n\n    Implementation details:\n        Assumes the input `output` corresponds to **LOGITS**.\n        The Softmax activation is applied internally.\n\n    Used for multi-class classification problems (one-hot encoded targets).\n    \"\"\"\n\n    def __init__(self, temperature: float = 1.0, epsilon: float = 1e-8) -&gt; None:\n        \"\"\"Initializes the Categorical Cross Entropy loss.\n\n        Args:\n            temperature (float, optional): Temperature parameter for the softmax. Defaults to 1.0.\n            epsilon (float, optional): Small float added to denominator to avoid dividing by zero. Defaults to 1e-8.\n        \"\"\"\n        self.temperature = temperature\n        self.epsilon = epsilon\n        self.softmax = Softmax(temperature=temperature, epsilon=epsilon)\n\n    def direct(self, output: ArrayType, output_expected: ArrayType) -&gt; float:\n        predictions = self.softmax.forward(output)\n        res: float = xp.mean(\n            -xp.sum(\n                output_expected * xp.log(predictions + self.epsilon, dtype=DTYPE),\n                axis=1,\n                dtype=DTYPE,\n            ),\n            dtype=DTYPE,\n        )\n        return res\n\n    def prime(self, output: ArrayType, output_expected: ArrayType) -&gt; ArrayType:\n        predictions = self.softmax.forward(output)\n        grad: ArrayType = ((predictions - output_expected) / (self.temperature + self.epsilon)) / output.shape[0]\n        return grad\n</code></pre>"},{"location":"reference/losses/#mpneuralnetwork.losses.CategoricalCrossEntropy.__init__","title":"<code>__init__(temperature=1.0, epsilon=1e-08)</code>","text":"<p>Initializes the Categorical Cross Entropy loss.</p> <p>Parameters:</p> Name Type Description Default <code>temperature</code> <code>float</code> <p>Temperature parameter for the softmax. Defaults to 1.0.</p> <code>1.0</code> <code>epsilon</code> <code>float</code> <p>Small float added to denominator to avoid dividing by zero. Defaults to 1e-8.</p> <code>1e-08</code> Source code in <code>src/mpneuralnetwork/losses.py</code> <pre><code>def __init__(self, temperature: float = 1.0, epsilon: float = 1e-8) -&gt; None:\n    \"\"\"Initializes the Categorical Cross Entropy loss.\n\n    Args:\n        temperature (float, optional): Temperature parameter for the softmax. Defaults to 1.0.\n        epsilon (float, optional): Small float added to denominator to avoid dividing by zero. Defaults to 1e-8.\n    \"\"\"\n    self.temperature = temperature\n    self.epsilon = epsilon\n    self.softmax = Softmax(temperature=temperature, epsilon=epsilon)\n</code></pre>"},{"location":"reference/metrics/","title":"Metrics API","text":"<p>Metrics are used to evaluate the performance of the model. Unlike loss functions, they are not used for training (backpropagation).</p>"},{"location":"reference/metrics/#base-metric","title":"Base Metric","text":""},{"location":"reference/metrics/#mpneuralnetwork.metrics.Metric","title":"<code>mpneuralnetwork.metrics.Metric</code>","text":"<p>Base class for evaluation metrics.</p> <p>Metrics are used to judge the performance of the model. Unlike Loss functions, metrics are not used during backpropagation (optimization), only for reporting.</p> Source code in <code>src/mpneuralnetwork/metrics.py</code> <pre><code>class Metric:\n    \"\"\"Base class for evaluation metrics.\n\n    Metrics are used to judge the performance of the model. Unlike Loss functions,\n    metrics are not used during backpropagation (optimization), only for reporting.\n    \"\"\"\n\n    def get_config(self) -&gt; dict:\n        \"\"\"Returns the metric configuration.\"\"\"\n        return {\"type\": self.__class__.__name__}\n\n    @abstractmethod\n    def __call__(self, y_true: ArrayType, y_pred: ArrayType) -&gt; float:\n        \"\"\"Computes the metric value.\n\n        Args:\n            y_true (ArrayType): Ground truth values.\n            y_pred (ArrayType): Model predictions (probabilities or values).\n\n        Returns:\n            float: The metric score.\n        \"\"\"\n        pass\n</code></pre>"},{"location":"reference/metrics/#mpneuralnetwork.metrics.Metric.__call__","title":"<code>__call__(y_true, y_pred)</code>  <code>abstractmethod</code>","text":"<p>Computes the metric value.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ArrayType</code> <p>Ground truth values.</p> required <code>y_pred</code> <code>ArrayType</code> <p>Model predictions (probabilities or values).</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>The metric score.</p> Source code in <code>src/mpneuralnetwork/metrics.py</code> <pre><code>@abstractmethod\ndef __call__(self, y_true: ArrayType, y_pred: ArrayType) -&gt; float:\n    \"\"\"Computes the metric value.\n\n    Args:\n        y_true (ArrayType): Ground truth values.\n        y_pred (ArrayType): Model predictions (probabilities or values).\n\n    Returns:\n        float: The metric score.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/metrics/#mpneuralnetwork.metrics.Metric.get_config","title":"<code>get_config()</code>","text":"<p>Returns the metric configuration.</p> Source code in <code>src/mpneuralnetwork/metrics.py</code> <pre><code>def get_config(self) -&gt; dict:\n    \"\"\"Returns the metric configuration.\"\"\"\n    return {\"type\": self.__class__.__name__}\n</code></pre>"},{"location":"reference/metrics/#regression-metrics","title":"Regression Metrics","text":""},{"location":"reference/metrics/#mpneuralnetwork.metrics.RMSE","title":"<code>mpneuralnetwork.metrics.RMSE</code>","text":"<p>               Bases: <code>Metric</code></p> <p>Root Mean Squared Error.</p> Formula <p><code>RMSE = sqrt( (1/N) * sum((y_pred - y_true)^2) )</code></p> <p>Used primarily for regression tasks. Lower is better.</p> Source code in <code>src/mpneuralnetwork/metrics.py</code> <pre><code>class RMSE(Metric):\n    \"\"\"Root Mean Squared Error.\n\n    Formula:\n        `RMSE = sqrt( (1/N) * sum((y_pred - y_true)^2) )`\n\n    Used primarily for regression tasks. Lower is better.\n    \"\"\"\n\n    def __call__(self, y_true: ArrayType, y_pred: ArrayType) -&gt; float:\n        mse = xp.mean(xp.sum(xp.square(y_true - y_pred), axis=1, dtype=DTYPE), dtype=DTYPE)\n        return self.from_mse(float(mse))\n\n    def from_mse(self, mse: float) -&gt; float:\n        \"\"\"Helper to compute RMSE from an existing MSE value.\"\"\"\n        res: float = xp.sqrt(mse, dtype=DTYPE)\n        return res\n</code></pre>"},{"location":"reference/metrics/#mpneuralnetwork.metrics.RMSE.from_mse","title":"<code>from_mse(mse)</code>","text":"<p>Helper to compute RMSE from an existing MSE value.</p> Source code in <code>src/mpneuralnetwork/metrics.py</code> <pre><code>def from_mse(self, mse: float) -&gt; float:\n    \"\"\"Helper to compute RMSE from an existing MSE value.\"\"\"\n    res: float = xp.sqrt(mse, dtype=DTYPE)\n    return res\n</code></pre>"},{"location":"reference/metrics/#mpneuralnetwork.metrics.MAE","title":"<code>mpneuralnetwork.metrics.MAE</code>","text":"<p>               Bases: <code>Metric</code></p> <p>Mean Absolute Error.</p> Formula <p><code>MAE = (1/N) * sum( |y_pred - y_true| )</code></p> <p>Used for regression. Less sensitive to outliers than RMSE. Lower is better.</p> Source code in <code>src/mpneuralnetwork/metrics.py</code> <pre><code>class MAE(Metric):\n    \"\"\"Mean Absolute Error.\n\n    Formula:\n        `MAE = (1/N) * sum( |y_pred - y_true| )`\n\n    Used for regression. Less sensitive to outliers than RMSE. Lower is better.\n    \"\"\"\n\n    def __call__(self, y_true: ArrayType, y_pred: ArrayType) -&gt; float:\n        res: float = xp.mean(xp.sum(xp.abs(y_true - y_pred), axis=1, dtype=DTYPE), dtype=DTYPE)\n        return res\n</code></pre>"},{"location":"reference/metrics/#mpneuralnetwork.metrics.R2Score","title":"<code>mpneuralnetwork.metrics.R2Score</code>","text":"<p>               Bases: <code>Metric</code></p> <p>R^2 Score (Coefficient of Determination).</p> <p>Measures how well the regression predictions approximate the real data points.</p> Formula <p><code>R2 = 1 - (SS_res / SS_tot)</code> <code>SS_res = sum((y_true - y_pred)^2)</code> <code>SS_tot = sum((y_true - mean(y_true))^2)</code></p> <p>Range: (-inf, 1.0]. 1.0 is perfect prediction. 0.0 is equivalent to a constant model predicting the mean. Negative values indicate the model is worse than just predicting the mean.</p> Source code in <code>src/mpneuralnetwork/metrics.py</code> <pre><code>class R2Score(Metric):\n    \"\"\"R^2 Score (Coefficient of Determination).\n\n    Measures how well the regression predictions approximate the real data points.\n\n    Formula:\n        `R2 = 1 - (SS_res / SS_tot)`\n        `SS_res = sum((y_true - y_pred)^2)`\n        `SS_tot = sum((y_true - mean(y_true))^2)`\n\n    Range: (-inf, 1.0].\n    1.0 is perfect prediction. 0.0 is equivalent to a constant model predicting the mean.\n    Negative values indicate the model is worse than just predicting the mean.\n    \"\"\"\n\n    def __init__(self, epsilon: float = 1e-8) -&gt; None:\n        self.epsilon: float = epsilon\n\n    def get_config(self) -&gt; dict:\n        config = super().get_config()\n        config.update({\"epsilon\": self.epsilon})\n        return config\n\n    def __call__(self, y_true: ArrayType, y_pred: ArrayType) -&gt; float:\n        var_tp = xp.sum(xp.square(y_true - y_pred), dtype=DTYPE)\n        var_tm = xp.sum(xp.square(y_true - xp.mean(y_true, axis=0, dtype=DTYPE)), dtype=DTYPE)\n\n        res: float = 1 - var_tp / (var_tm + self.epsilon)\n        return res\n</code></pre>"},{"location":"reference/metrics/#classification-metrics","title":"Classification Metrics","text":""},{"location":"reference/metrics/#mpneuralnetwork.metrics.Accuracy","title":"<code>mpneuralnetwork.metrics.Accuracy</code>","text":"<p>               Bases: <code>Metric</code></p> <p>Classification Accuracy.</p> Formula <p><code>Accuracy = (TP + TN) / Total Samples</code></p> <p>Works for: - Binary classification (threshold at 0.5). - Multi-class classification (argmax).</p> Source code in <code>src/mpneuralnetwork/metrics.py</code> <pre><code>class Accuracy(Metric):\n    \"\"\"Classification Accuracy.\n\n    Formula:\n        `Accuracy = (TP + TN) / Total Samples`\n\n    Works for:\n    - Binary classification (threshold at 0.5).\n    - Multi-class classification (argmax).\n    \"\"\"\n\n    def __call__(self, y_true: ArrayType, y_pred: ArrayType) -&gt; float:\n        if y_true.ndim == 2 and y_true.shape[1] &gt; 1:\n            y_true = xp.argmax(y_true, axis=1)\n            y_pred = xp.argmax(y_pred, axis=1)\n        else:\n            y_pred = xp.round(y_pred)\n\n        res: float = xp.mean(y_true == y_pred, dtype=DTYPE)\n        return res\n</code></pre>"},{"location":"reference/metrics/#mpneuralnetwork.metrics.Precision","title":"<code>mpneuralnetwork.metrics.Precision</code>","text":"<p>               Bases: <code>Metric</code></p> <p>Precision Metric (Positive Predictive Value).</p> Formula <p><code>Precision = TP / (TP + FP)</code></p> <p>Measures the proportion of positive identifications that were actually correct.</p> Source code in <code>src/mpneuralnetwork/metrics.py</code> <pre><code>class Precision(Metric):\n    \"\"\"Precision Metric (Positive Predictive Value).\n\n    Formula:\n        `Precision = TP / (TP + FP)`\n\n    Measures the proportion of positive identifications that were actually correct.\n    \"\"\"\n\n    def __init__(self, epsilon: float = 1e-8) -&gt; None:\n        self.epsilon: float = epsilon\n\n    def get_config(self) -&gt; dict:\n        config = super().get_config()\n        config.update({\"epsilon\": self.epsilon})\n        return config\n\n    def __call__(\n        self,\n        y_true: ArrayType,\n        y_pred: ArrayType,\n        num_classes: int = 1,\n        no_check: bool = False,\n    ) -&gt; float:\n        if not no_check:\n            num_classes = y_true.shape[1]\n            if y_true.ndim == 2 and num_classes &gt; 1:\n                y_true = xp.argmax(y_true, axis=1)\n                y_pred = xp.argmax(y_pred, axis=1)\n            else:\n                y_pred = xp.round(y_pred)\n\n        sum_score: float = 0\n        for c in range(num_classes):\n            tp = xp.sum((y_pred == c) &amp; (y_true == c))\n            fp = xp.sum((y_pred == c) &amp; (y_true != c))\n\n            sum_score += tp / (tp + fp + self.epsilon)\n\n        return sum_score / num_classes\n</code></pre>"},{"location":"reference/metrics/#mpneuralnetwork.metrics.Recall","title":"<code>mpneuralnetwork.metrics.Recall</code>","text":"<p>               Bases: <code>Metric</code></p> <p>Recall Metric (Sensitivity / True Positive Rate).</p> Formula <p><code>Recall = TP / (TP + FN)</code></p> <p>Measures the proportion of actual positives that were identified correctly.</p> Source code in <code>src/mpneuralnetwork/metrics.py</code> <pre><code>class Recall(Metric):\n    \"\"\"Recall Metric (Sensitivity / True Positive Rate).\n\n    Formula:\n        `Recall = TP / (TP + FN)`\n\n    Measures the proportion of actual positives that were identified correctly.\n    \"\"\"\n\n    def __init__(self, epsilon: float = 1e-8) -&gt; None:\n        self.epsilon: float = epsilon\n\n    def get_config(self) -&gt; dict:\n        config = super().get_config()\n        config.update({\"epsilon\": self.epsilon})\n        return config\n\n    def __call__(\n        self,\n        y_true: ArrayType,\n        y_pred: ArrayType,\n        num_classes: int = 1,\n        no_check: bool = False,\n    ) -&gt; float:\n        if not no_check:\n            num_classes = y_true.shape[1]\n            if y_true.ndim == 2 and num_classes &gt; 1:\n                y_true = xp.argmax(y_true, axis=1)\n                y_pred = xp.argmax(y_pred, axis=1)\n            else:\n                y_pred = xp.round(y_pred)\n\n        sum_score: float = 0\n        for c in range(num_classes):\n            tp = xp.sum((y_pred == c) &amp; (y_true == c))\n            fn = xp.sum((y_pred != c) &amp; (y_true == c))\n            sum_score += tp / (tp + fn + self.epsilon)\n\n        return sum_score / num_classes\n</code></pre>"},{"location":"reference/metrics/#mpneuralnetwork.metrics.F1Score","title":"<code>mpneuralnetwork.metrics.F1Score</code>","text":"<p>               Bases: <code>Metric</code></p> <p>F1 Score.</p> Formula <p><code>F1 = 2 * (Precision * Recall) / (Precision + Recall)</code></p> <p>Harmonic mean of Precision and Recall. Useful for imbalanced datasets.</p> Source code in <code>src/mpneuralnetwork/metrics.py</code> <pre><code>class F1Score(Metric):\n    \"\"\"F1 Score.\n\n    Formula:\n        `F1 = 2 * (Precision * Recall) / (Precision + Recall)`\n\n    Harmonic mean of Precision and Recall. Useful for imbalanced datasets.\n    \"\"\"\n\n    def __init__(self, epsilon: float = 1e-8) -&gt; None:\n        self.epsilon: float = epsilon\n\n    def get_config(self) -&gt; dict:\n        config = super().get_config()\n        config.update({\"epsilon\": self.epsilon})\n        return config\n\n    def __call__(self, y_true: ArrayType, y_pred: ArrayType) -&gt; float:\n        num_classes = y_true.shape[1]\n        if y_true.ndim == 2 and num_classes &gt; 1:\n            y_true = xp.argmax(y_true, axis=1)\n            y_pred = xp.argmax(y_pred, axis=1)\n        else:\n            y_pred = xp.round(y_pred)\n\n        precision = Precision(self.epsilon)(y_true, y_pred, num_classes=num_classes, no_check=True)\n        recall = Recall(self.epsilon)(y_true, y_pred, num_classes=num_classes, no_check=True)\n\n        return 2 * precision * recall / (precision + recall + self.epsilon)\n</code></pre>"},{"location":"reference/metrics/#mpneuralnetwork.metrics.TopKAccuracy","title":"<code>mpneuralnetwork.metrics.TopKAccuracy</code>","text":"<p>               Bases: <code>Metric</code></p> <p>Top-K Accuracy.</p> <p>Consider the prediction correct if the true label is among the top K probabilities. Commonly used in ImageNet classification (Top-5).</p> <p>Parameters:</p> Name Type Description Default <code>k</code> <code>int</code> <p>Number of top predictions to consider.</p> required Source code in <code>src/mpneuralnetwork/metrics.py</code> <pre><code>class TopKAccuracy(Metric):\n    \"\"\"Top-K Accuracy.\n\n    Consider the prediction correct if the true label is among the top K probabilities.\n    Commonly used in ImageNet classification (Top-5).\n\n    Args:\n        k (int): Number of top predictions to consider.\n    \"\"\"\n\n    def __init__(self, k: int) -&gt; None:\n        self.k: int = k\n\n    def get_config(self) -&gt; dict:\n        config = super().get_config()\n        config.update({\"k\": self.k})\n        return config\n\n    def __call__(self, y_true: ArrayType, y_pred: ArrayType) -&gt; float:\n        # TODO: no_check ?\n        top_k_preds = xp.argsort(y_pred, axis=1)[:, -self.k :]\n\n        if y_true.ndim == 2 and y_true.shape[1] &gt; 1:\n            y_true = xp.argmax(y_true, axis=1)\n\n        y_true = y_true.reshape(-1, 1)\n\n        res: float = xp.mean(xp.any(top_k_preds == y_true, axis=1), dtype=DTYPE)\n        return res\n</code></pre>"},{"location":"reference/model/","title":"Model API","text":"<p>The <code>Model</code> class is the central component of the framework. It acts as a container for layers, manages the training loop, and handles the interaction between the optimizer, loss function, and metrics.</p>"},{"location":"reference/model/#mpneuralnetwork.model.Model","title":"<code>mpneuralnetwork.model.Model</code>","text":"<p>The main container for building and training neural networks.</p> <p>This class handles the assembly of layers, the training loop, validation, and prediction. It also implements \"smart features\" like automatic weight initialization and metric selection.</p> <p>Attributes:</p> Name Type Description <code>layers</code> <code>list[Layer]</code> <p>List of layers in the network.</p> <code>loss</code> <code>Loss</code> <p>The loss function to minimize.</p> <code>optimizer</code> <code>Optimizer</code> <p>The optimization algorithm.</p> <code>metrics</code> <code>list[Metric]</code> <p>List of metrics to monitor.</p> Source code in <code>src/mpneuralnetwork/model.py</code> <pre><code>class Model:\n    \"\"\"The main container for building and training neural networks.\n\n    This class handles the assembly of layers, the training loop, validation,\n    and prediction. It also implements \"smart features\" like automatic weight\n    initialization and metric selection.\n\n    Attributes:\n        layers (list[Layer]): List of layers in the network.\n        loss (Loss): The loss function to minimize.\n        optimizer (Optimizer): The optimization algorithm.\n        metrics (list[Metric]): List of metrics to monitor.\n    \"\"\"\n\n    def __init__(\n        self,\n        layers: list[Layer],\n        loss: Loss,\n        optimizer: Optimizer | None = None,\n        metrics: list[Metric] | None = None,\n    ) -&gt; None:\n        \"\"\"Initializes the Model.\n\n        Args:\n            layers (list[Layer]): The sequence of layers.\n            loss (Loss): The objective function.\n            optimizer (Optimizer | None, optional): Optimizer instance. Defaults to SGD().\n            metrics (list[Metric] | None, optional): Metrics to track. Defaults to [].\n        \"\"\"\n        self.layers: list[Layer] = layers\n        self.loss: Loss = loss\n        self.optimizer: Optimizer = SGD() if optimizer is None else optimizer\n        self.metrics: list[Metric] = metrics if metrics is not None else []\n        self.output_activation: Activation | Layer | None = None\n\n        self._build_graph()\n        self._init_smart_weights()\n        self._init_output_activation()\n        self._init_smart_metrics()\n\n    def _build_graph(self) -&gt; None:\n        \"\"\"Builds the computational graph by connecting layers.\n\n        Propagates shape information from the first layer through the rest of the network.\n        \"\"\"\n        first_layer = self.layers[0]\n\n        if not hasattr(first_layer, \"input_size\") or first_layer.input_size is None:\n            raise ValueError(\"Input layer does not define input size\")\n\n        current_output_size: tuple[int, ...] = first_layer.output_shape\n\n        for i in range(1, len(self.layers)):\n            layer = self.layers[i]\n\n            layer.build(current_output_size)\n\n            if hasattr(layer, \"output_shape\"):\n                current_output_size = layer.output_shape\n\n    def _init_smart_weights(self) -&gt; None:\n        \"\"\"Automatically initializes weights based on activation functions.\n\n        Uses He initialization for ReLU-like activations and Xavier for Sigmoid/Tanh.\n        Also handles bias disabling for BatchNormalization.\n        \"\"\"\n        for i in range(len(self.layers)):\n            layer = self.layers[i]\n\n            if isinstance(layer, (Dense, Convolutional)) and layer.initialization == \"auto\":\n                method: Lit_W = \"xavier\"\n                no_bias: bool = False\n\n                for j in range(i + 1, len(self.layers)):\n                    next_layer = self.layers[j]\n\n                    if isinstance(next_layer, BatchNormalization):\n                        no_bias = True\n                        continue\n\n                    if isinstance(next_layer, Dropout):\n                        continue\n\n                    if isinstance(next_layer, (ReLU, PReLU, Swish)):\n                        method = \"he\"\n                        break\n\n                    if isinstance(next_layer, (Activation, Dense, Convolutional)):\n                        break\n\n                layer.init_weights(method, no_bias)\n\n    def _init_output_activation(self) -&gt; None:\n        \"\"\"Configures the final activation for numerical stability.\n\n        The framework uses logits for Loss functions. This method ensures the\n        user hasn't redundantly added a Softmax/Sigmoid layer at the end if the\n        loss function expects logits, and sets up the implicit output activation\n        for predictions.\n        \"\"\"\n        if isinstance(self.loss, BinaryCrossEntropy):\n            self.output_activation = Sigmoid()\n\n        elif isinstance(self.loss, CategoricalCrossEntropy):\n            self.output_activation = Softmax()\n\n        if not self.output_activation:\n            return\n\n        if isinstance(self.layers[len(self.layers) - 1], type(self.output_activation)):\n            self.layers = self.layers[:-1]\n\n    def _init_smart_metrics(self) -&gt; None:\n        \"\"\"Automatically selects default metrics if none are provided.\n\n        - **Regression (MSE Loss):** Defaults to [RMSE, R2Score].\n        - **Classification (CrossEntropy):** Defaults to [Accuracy, F1Score].\n        \"\"\"\n        if len(self.metrics) != 0:\n            return\n\n        if isinstance(self.loss, MSE):\n            self.metrics = [RMSE(), R2Score()]\n        else:\n            self.metrics = [Accuracy(), F1Score()]\n\n    def train(\n        self,\n        X_train: ArrayType,\n        y_train: ArrayType,\n        epochs: int,\n        batch_size: int,\n        evaluation: tuple[ArrayType, ArrayType] | None = None,\n        auto_evaluation: float = 0.2,\n        early_stopping: int | None = None,\n        model_checkpoint: bool = True,\n        compute_train_metrics: bool = False,\n    ) -&gt; None:\n        \"\"\"Trains the model using the provided data.\n\n        Args:\n            X_train (ArrayType): Training features.\n            y_train (ArrayType): Training labels (one-hot encoded for classification).\n            epochs (int): Number of complete passes through the dataset.\n            batch_size (int): Number of samples per gradient update.\n            evaluation (tuple[ArrayType, ArrayType] | None, optional): Explicit validation set (X_val, y_val).\n            auto_evaluation (float, optional): Fraction of training data to use for validation if 'evaluation' is None.\n            early_stopping (int | None, optional): Number of epochs with no improvement to wait before stopping.\n            model_checkpoint (bool, optional): Whether to restore the best weights after training. Defaults to True.\n            compute_train_metrics (bool, optional): Whether to compute expensive metrics on training data every epoch.\n        \"\"\"\n        X_t = to_device(X_train.astype(DTYPE, copy=False))\n        y_t = to_device(y_train.astype(DTYPE, copy=False))\n\n        X_val: ArrayType | None = None\n        y_val: ArrayType | None = None\n\n        if evaluation is not None:\n            X_val = to_device(evaluation[0].astype(DTYPE, copy=False))\n            y_val = to_device(evaluation[1].astype(DTYPE, copy=False))\n\n        elif auto_evaluation &gt; 0.0:\n            split_i = int(len(X_t) * auto_evaluation)\n\n            all_indices = xp.random.permutation(X_t.shape[0])\n\n            train_indices = all_indices[:-split_i]\n            val_indices = all_indices[-split_i:]\n\n            X_val = X_t[val_indices]\n            y_val = y_t[val_indices]\n\n            X_t = X_t[train_indices]\n            y_t = y_t[train_indices]\n\n        num_samples = X_t.shape[0]\n        num_batches = int(np.floor(num_samples / batch_size))\n\n        early_stopping = early_stopping if early_stopping else epochs + 1\n        patience: int = early_stopping\n        best_error: float = float(\"inf\")\n        best_weights: dict | None = None\n        temp_t: int = 0  # TODO: Find a better solution\n\n        for epoch in range(epochs):\n            metric_dict: dict[str, float] = {}\n            metric_dict[\"loss\"] = 0\n\n            if compute_train_metrics:\n                for metric in self.metrics:\n                    metric_dict[metric.__class__.__name__.lower()] = 0\n\n            indices = xp.arange(num_samples)\n            xp.random.shuffle(indices)\n\n            for i in range(num_batches):\n                batch_idx = indices[i * batch_size : (i + 1) * batch_size]\n                X_batch: ArrayType = X_t[batch_idx]\n                y_batch: ArrayType = y_t[batch_idx]\n\n                predictions, new_metric_dict = self.evaluate(\n                    X_batch,\n                    y_batch,\n                    training=True,\n                    compute_metrics=compute_train_metrics,\n                )\n\n                for key, value in new_metric_dict.items():\n                    metric_dict[key] += value\n\n                grad: ArrayType = self.loss.prime(predictions, y_batch)\n\n                for layer in reversed(self.layers):\n                    grad = layer.backward(grad)\n\n                self.optimizer.step(self.layers)\n\n            spacing_str = \" \" * abs(len(str(epochs)) - len(str(epoch + 1)))\n            message = f\"epoch {spacing_str}{epoch + 1}/{epochs}   |   [training]\"\n\n            for key, _ in metric_dict.items():\n                metric_dict[key] /= num_batches\n                message += f\"   {key} = {metric_dict[key]:.4f}\"\n\n            if X_val is not None and y_val is not None:\n                _, val_metric_dict = self.evaluate(X_val, y_val, training=False)\n\n                if val_metric_dict[\"loss\"] &lt; best_error:\n                    best_error = val_metric_dict[\"loss\"]\n                    patience = early_stopping\n                    if model_checkpoint:\n                        best_weights = get_model_weights(self.layers)\n                        if isinstance(self.optimizer, Adam):\n                            temp_t = self.optimizer.t\n                else:\n                    patience -= 1\n\n                message += \"   |   [evaluation]\"\n                for key, value in val_metric_dict.items():\n                    message += f\"   {key} = {value:.4f}\"\n\n            elif metric_dict[\"loss\"] &lt; best_error:\n                best_error = metric_dict[\"loss\"]\n                patience = early_stopping\n                if model_checkpoint:\n                    best_weights = get_model_weights(self.layers)\n                    if isinstance(self.optimizer, Adam):\n                        temp_t = self.optimizer.t\n            else:\n                patience -= 1\n\n            print(message)\n\n            if patience == 0:\n                print(f\"EARLY STOPPING - Model did not learn since {early_stopping} epochs\")\n                break\n\n        if model_checkpoint and best_weights is not None:\n            restore_model_weights(self.layers, best_weights)\n            if isinstance(self.optimizer, Adam):\n                self.optimizer.t = temp_t\n            print(f\"MODEL CHECKPOINT: {best_error:.4f}\")\n            # TODO: Save also optimizer state, better user output\n\n    def evaluate(\n        self,\n        X: ArrayType,\n        y: ArrayType,\n        training: bool = False,\n        compute_metrics: bool = True,\n    ) -&gt; tuple[ArrayType, dict[str, float]]:\n        \"\"\"Evaluates the model on a given batch.\n\n        Args:\n            X (ArrayType): Input features.\n            y (ArrayType): True labels.\n            training (bool): Whether in training mode (affects Dropout/BatchNorm).\n            compute_metrics (bool): Whether to calculate extra metrics (Accuracy, etc.).\n\n        Returns:\n            tuple[ArrayType, dict[str, float]]: Predictions and dictionary of metric values.\n        \"\"\"\n        logits: ArrayType = X.astype(DTYPE, copy=False)\n        for layer in self.layers:\n            logits = layer.forward(logits, training=training)\n\n        loss: float = self.loss.direct(logits, y)\n\n        metric_dict: dict[str, float] = {}\n        metric_dict[\"loss\"] = loss\n\n        predictions_activated: ArrayType = logits\n        if self.output_activation is not None:\n            predictions_activated = self.output_activation.forward(logits)\n\n        if compute_metrics:\n            for metric in self.metrics:\n                key = metric.__class__.__name__.lower()\n\n                if isinstance(metric, RMSE) and isinstance(self.loss, MSE):\n                    metric_dict[key] = metric.from_mse(loss)\n                else:\n                    metric_dict[key] = metric(y, predictions_activated)\n\n        predictions: ArrayType = logits\n        if not training:\n            predictions = predictions_activated\n\n        return predictions, metric_dict\n\n    def test(self, X_test: ArrayType, y_test: ArrayType) -&gt; None:\n        \"\"\"Evaluates the model on the test set and prints results.\n\n        Args:\n            X_test (ArrayType): Test features.\n            y_test (ArrayType): Test labels.\n        \"\"\"\n        X_test = to_device(X_test)\n        y_test = to_device(y_test)\n\n        _, metric_dict = self.evaluate(X_test, y_test, training=False)\n\n        print(\"Test resuls:\")\n        for key, value in metric_dict.items():\n            print(f\"   {key} = {value:.4f}\")\n\n    def predict(self, X: ArrayType) -&gt; ArrayType:\n        \"\"\"Generates predictions for the input samples.\n\n        Automatically applies the final activation function (Softmax/Sigmoid)\n        to return probabilities/values instead of logits.\n\n        Args:\n            X (ArrayType): Input features.\n\n        Returns:\n            ArrayType: Model predictions (on CPU).\n        \"\"\"\n        y: ArrayType = to_device(X.astype(DTYPE, copy=False))\n        for layer in self.layers:\n            y = layer.forward(y, training=False)\n\n        if self.output_activation is not None:\n            y = self.output_activation.forward(y)\n\n        return to_host(y)\n\n    def get_weights(self, optimizer_params: dict | None = None) -&gt; dict:\n        \"\"\"DEPRECATED: Use `mpneuralnetwork.serialization.get_model_weights` instead.\"\"\"\n        import warnings\n\n        warnings.warn(\n            \"Model.get_weights() is deprecated and will be removed in a future version. \"\n            \"Please use mpneuralnetwork.serialization.get_model_weights(model.layers) instead.\",\n            DeprecationWarning,\n            stacklevel=2,\n        )\n        return get_model_weights(self.layers, optimizer_params)\n\n    def restore_weights(self, weights_dict: dict, optimizer: Optimizer | None = None) -&gt; None:\n        \"\"\"DEPRECATED: Use `mpneuralnetwork.serialization.restore_model_weights` instead.\"\"\"\n        import warnings\n\n        warnings.warn(\n            \"Model.restore_weights() is deprecated and will be removed in a future version. \"\n            \"Please use mpneuralnetwork.serialization.restore_model_weights(model.layers, ...) instead.\",\n            DeprecationWarning,\n            stacklevel=2,\n        )\n        restore_model_weights(self.layers, weights_dict, optimizer)\n</code></pre>"},{"location":"reference/model/#mpneuralnetwork.model.Model.__init__","title":"<code>__init__(layers, loss, optimizer=None, metrics=None)</code>","text":"<p>Initializes the Model.</p> <p>Parameters:</p> Name Type Description Default <code>layers</code> <code>list[Layer]</code> <p>The sequence of layers.</p> required <code>loss</code> <code>Loss</code> <p>The objective function.</p> required <code>optimizer</code> <code>Optimizer | None</code> <p>Optimizer instance. Defaults to SGD().</p> <code>None</code> <code>metrics</code> <code>list[Metric] | None</code> <p>Metrics to track. Defaults to [].</p> <code>None</code> Source code in <code>src/mpneuralnetwork/model.py</code> <pre><code>def __init__(\n    self,\n    layers: list[Layer],\n    loss: Loss,\n    optimizer: Optimizer | None = None,\n    metrics: list[Metric] | None = None,\n) -&gt; None:\n    \"\"\"Initializes the Model.\n\n    Args:\n        layers (list[Layer]): The sequence of layers.\n        loss (Loss): The objective function.\n        optimizer (Optimizer | None, optional): Optimizer instance. Defaults to SGD().\n        metrics (list[Metric] | None, optional): Metrics to track. Defaults to [].\n    \"\"\"\n    self.layers: list[Layer] = layers\n    self.loss: Loss = loss\n    self.optimizer: Optimizer = SGD() if optimizer is None else optimizer\n    self.metrics: list[Metric] = metrics if metrics is not None else []\n    self.output_activation: Activation | Layer | None = None\n\n    self._build_graph()\n    self._init_smart_weights()\n    self._init_output_activation()\n    self._init_smart_metrics()\n</code></pre>"},{"location":"reference/model/#mpneuralnetwork.model.Model.train","title":"<code>train(X_train, y_train, epochs, batch_size, evaluation=None, auto_evaluation=0.2, early_stopping=None, model_checkpoint=True, compute_train_metrics=False)</code>","text":"<p>Trains the model using the provided data.</p> <p>Parameters:</p> Name Type Description Default <code>X_train</code> <code>ArrayType</code> <p>Training features.</p> required <code>y_train</code> <code>ArrayType</code> <p>Training labels (one-hot encoded for classification).</p> required <code>epochs</code> <code>int</code> <p>Number of complete passes through the dataset.</p> required <code>batch_size</code> <code>int</code> <p>Number of samples per gradient update.</p> required <code>evaluation</code> <code>tuple[ArrayType, ArrayType] | None</code> <p>Explicit validation set (X_val, y_val).</p> <code>None</code> <code>auto_evaluation</code> <code>float</code> <p>Fraction of training data to use for validation if 'evaluation' is None.</p> <code>0.2</code> <code>early_stopping</code> <code>int | None</code> <p>Number of epochs with no improvement to wait before stopping.</p> <code>None</code> <code>model_checkpoint</code> <code>bool</code> <p>Whether to restore the best weights after training. Defaults to True.</p> <code>True</code> <code>compute_train_metrics</code> <code>bool</code> <p>Whether to compute expensive metrics on training data every epoch.</p> <code>False</code> Source code in <code>src/mpneuralnetwork/model.py</code> <pre><code>def train(\n    self,\n    X_train: ArrayType,\n    y_train: ArrayType,\n    epochs: int,\n    batch_size: int,\n    evaluation: tuple[ArrayType, ArrayType] | None = None,\n    auto_evaluation: float = 0.2,\n    early_stopping: int | None = None,\n    model_checkpoint: bool = True,\n    compute_train_metrics: bool = False,\n) -&gt; None:\n    \"\"\"Trains the model using the provided data.\n\n    Args:\n        X_train (ArrayType): Training features.\n        y_train (ArrayType): Training labels (one-hot encoded for classification).\n        epochs (int): Number of complete passes through the dataset.\n        batch_size (int): Number of samples per gradient update.\n        evaluation (tuple[ArrayType, ArrayType] | None, optional): Explicit validation set (X_val, y_val).\n        auto_evaluation (float, optional): Fraction of training data to use for validation if 'evaluation' is None.\n        early_stopping (int | None, optional): Number of epochs with no improvement to wait before stopping.\n        model_checkpoint (bool, optional): Whether to restore the best weights after training. Defaults to True.\n        compute_train_metrics (bool, optional): Whether to compute expensive metrics on training data every epoch.\n    \"\"\"\n    X_t = to_device(X_train.astype(DTYPE, copy=False))\n    y_t = to_device(y_train.astype(DTYPE, copy=False))\n\n    X_val: ArrayType | None = None\n    y_val: ArrayType | None = None\n\n    if evaluation is not None:\n        X_val = to_device(evaluation[0].astype(DTYPE, copy=False))\n        y_val = to_device(evaluation[1].astype(DTYPE, copy=False))\n\n    elif auto_evaluation &gt; 0.0:\n        split_i = int(len(X_t) * auto_evaluation)\n\n        all_indices = xp.random.permutation(X_t.shape[0])\n\n        train_indices = all_indices[:-split_i]\n        val_indices = all_indices[-split_i:]\n\n        X_val = X_t[val_indices]\n        y_val = y_t[val_indices]\n\n        X_t = X_t[train_indices]\n        y_t = y_t[train_indices]\n\n    num_samples = X_t.shape[0]\n    num_batches = int(np.floor(num_samples / batch_size))\n\n    early_stopping = early_stopping if early_stopping else epochs + 1\n    patience: int = early_stopping\n    best_error: float = float(\"inf\")\n    best_weights: dict | None = None\n    temp_t: int = 0  # TODO: Find a better solution\n\n    for epoch in range(epochs):\n        metric_dict: dict[str, float] = {}\n        metric_dict[\"loss\"] = 0\n\n        if compute_train_metrics:\n            for metric in self.metrics:\n                metric_dict[metric.__class__.__name__.lower()] = 0\n\n        indices = xp.arange(num_samples)\n        xp.random.shuffle(indices)\n\n        for i in range(num_batches):\n            batch_idx = indices[i * batch_size : (i + 1) * batch_size]\n            X_batch: ArrayType = X_t[batch_idx]\n            y_batch: ArrayType = y_t[batch_idx]\n\n            predictions, new_metric_dict = self.evaluate(\n                X_batch,\n                y_batch,\n                training=True,\n                compute_metrics=compute_train_metrics,\n            )\n\n            for key, value in new_metric_dict.items():\n                metric_dict[key] += value\n\n            grad: ArrayType = self.loss.prime(predictions, y_batch)\n\n            for layer in reversed(self.layers):\n                grad = layer.backward(grad)\n\n            self.optimizer.step(self.layers)\n\n        spacing_str = \" \" * abs(len(str(epochs)) - len(str(epoch + 1)))\n        message = f\"epoch {spacing_str}{epoch + 1}/{epochs}   |   [training]\"\n\n        for key, _ in metric_dict.items():\n            metric_dict[key] /= num_batches\n            message += f\"   {key} = {metric_dict[key]:.4f}\"\n\n        if X_val is not None and y_val is not None:\n            _, val_metric_dict = self.evaluate(X_val, y_val, training=False)\n\n            if val_metric_dict[\"loss\"] &lt; best_error:\n                best_error = val_metric_dict[\"loss\"]\n                patience = early_stopping\n                if model_checkpoint:\n                    best_weights = get_model_weights(self.layers)\n                    if isinstance(self.optimizer, Adam):\n                        temp_t = self.optimizer.t\n            else:\n                patience -= 1\n\n            message += \"   |   [evaluation]\"\n            for key, value in val_metric_dict.items():\n                message += f\"   {key} = {value:.4f}\"\n\n        elif metric_dict[\"loss\"] &lt; best_error:\n            best_error = metric_dict[\"loss\"]\n            patience = early_stopping\n            if model_checkpoint:\n                best_weights = get_model_weights(self.layers)\n                if isinstance(self.optimizer, Adam):\n                    temp_t = self.optimizer.t\n        else:\n            patience -= 1\n\n        print(message)\n\n        if patience == 0:\n            print(f\"EARLY STOPPING - Model did not learn since {early_stopping} epochs\")\n            break\n\n    if model_checkpoint and best_weights is not None:\n        restore_model_weights(self.layers, best_weights)\n        if isinstance(self.optimizer, Adam):\n            self.optimizer.t = temp_t\n        print(f\"MODEL CHECKPOINT: {best_error:.4f}\")\n</code></pre>"},{"location":"reference/model/#mpneuralnetwork.model.Model.evaluate","title":"<code>evaluate(X, y, training=False, compute_metrics=True)</code>","text":"<p>Evaluates the model on a given batch.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ArrayType</code> <p>Input features.</p> required <code>y</code> <code>ArrayType</code> <p>True labels.</p> required <code>training</code> <code>bool</code> <p>Whether in training mode (affects Dropout/BatchNorm).</p> <code>False</code> <code>compute_metrics</code> <code>bool</code> <p>Whether to calculate extra metrics (Accuracy, etc.).</p> <code>True</code> <p>Returns:</p> Type Description <code>tuple[ArrayType, dict[str, float]]</code> <p>tuple[ArrayType, dict[str, float]]: Predictions and dictionary of metric values.</p> Source code in <code>src/mpneuralnetwork/model.py</code> <pre><code>def evaluate(\n    self,\n    X: ArrayType,\n    y: ArrayType,\n    training: bool = False,\n    compute_metrics: bool = True,\n) -&gt; tuple[ArrayType, dict[str, float]]:\n    \"\"\"Evaluates the model on a given batch.\n\n    Args:\n        X (ArrayType): Input features.\n        y (ArrayType): True labels.\n        training (bool): Whether in training mode (affects Dropout/BatchNorm).\n        compute_metrics (bool): Whether to calculate extra metrics (Accuracy, etc.).\n\n    Returns:\n        tuple[ArrayType, dict[str, float]]: Predictions and dictionary of metric values.\n    \"\"\"\n    logits: ArrayType = X.astype(DTYPE, copy=False)\n    for layer in self.layers:\n        logits = layer.forward(logits, training=training)\n\n    loss: float = self.loss.direct(logits, y)\n\n    metric_dict: dict[str, float] = {}\n    metric_dict[\"loss\"] = loss\n\n    predictions_activated: ArrayType = logits\n    if self.output_activation is not None:\n        predictions_activated = self.output_activation.forward(logits)\n\n    if compute_metrics:\n        for metric in self.metrics:\n            key = metric.__class__.__name__.lower()\n\n            if isinstance(metric, RMSE) and isinstance(self.loss, MSE):\n                metric_dict[key] = metric.from_mse(loss)\n            else:\n                metric_dict[key] = metric(y, predictions_activated)\n\n    predictions: ArrayType = logits\n    if not training:\n        predictions = predictions_activated\n\n    return predictions, metric_dict\n</code></pre>"},{"location":"reference/model/#mpneuralnetwork.model.Model.predict","title":"<code>predict(X)</code>","text":"<p>Generates predictions for the input samples.</p> <p>Automatically applies the final activation function (Softmax/Sigmoid) to return probabilities/values instead of logits.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ArrayType</code> <p>Input features.</p> required <p>Returns:</p> Name Type Description <code>ArrayType</code> <code>ArrayType</code> <p>Model predictions (on CPU).</p> Source code in <code>src/mpneuralnetwork/model.py</code> <pre><code>def predict(self, X: ArrayType) -&gt; ArrayType:\n    \"\"\"Generates predictions for the input samples.\n\n    Automatically applies the final activation function (Softmax/Sigmoid)\n    to return probabilities/values instead of logits.\n\n    Args:\n        X (ArrayType): Input features.\n\n    Returns:\n        ArrayType: Model predictions (on CPU).\n    \"\"\"\n    y: ArrayType = to_device(X.astype(DTYPE, copy=False))\n    for layer in self.layers:\n        y = layer.forward(y, training=False)\n\n    if self.output_activation is not None:\n        y = self.output_activation.forward(y)\n\n    return to_host(y)\n</code></pre>"},{"location":"reference/model/#mpneuralnetwork.model.Model.test","title":"<code>test(X_test, y_test)</code>","text":"<p>Evaluates the model on the test set and prints results.</p> <p>Parameters:</p> Name Type Description Default <code>X_test</code> <code>ArrayType</code> <p>Test features.</p> required <code>y_test</code> <code>ArrayType</code> <p>Test labels.</p> required Source code in <code>src/mpneuralnetwork/model.py</code> <pre><code>def test(self, X_test: ArrayType, y_test: ArrayType) -&gt; None:\n    \"\"\"Evaluates the model on the test set and prints results.\n\n    Args:\n        X_test (ArrayType): Test features.\n        y_test (ArrayType): Test labels.\n    \"\"\"\n    X_test = to_device(X_test)\n    y_test = to_device(y_test)\n\n    _, metric_dict = self.evaluate(X_test, y_test, training=False)\n\n    print(\"Test resuls:\")\n    for key, value in metric_dict.items():\n        print(f\"   {key} = {value:.4f}\")\n</code></pre>"},{"location":"reference/model/#mpneuralnetwork.model.Model.get_weights","title":"<code>get_weights(optimizer_params=None)</code>","text":"<p>DEPRECATED: Use <code>mpneuralnetwork.serialization.get_model_weights</code> instead.</p> Source code in <code>src/mpneuralnetwork/model.py</code> <pre><code>def get_weights(self, optimizer_params: dict | None = None) -&gt; dict:\n    \"\"\"DEPRECATED: Use `mpneuralnetwork.serialization.get_model_weights` instead.\"\"\"\n    import warnings\n\n    warnings.warn(\n        \"Model.get_weights() is deprecated and will be removed in a future version. \"\n        \"Please use mpneuralnetwork.serialization.get_model_weights(model.layers) instead.\",\n        DeprecationWarning,\n        stacklevel=2,\n    )\n    return get_model_weights(self.layers, optimizer_params)\n</code></pre>"},{"location":"reference/model/#mpneuralnetwork.model.Model.restore_weights","title":"<code>restore_weights(weights_dict, optimizer=None)</code>","text":"<p>DEPRECATED: Use <code>mpneuralnetwork.serialization.restore_model_weights</code> instead.</p> Source code in <code>src/mpneuralnetwork/model.py</code> <pre><code>def restore_weights(self, weights_dict: dict, optimizer: Optimizer | None = None) -&gt; None:\n    \"\"\"DEPRECATED: Use `mpneuralnetwork.serialization.restore_model_weights` instead.\"\"\"\n    import warnings\n\n    warnings.warn(\n        \"Model.restore_weights() is deprecated and will be removed in a future version. \"\n        \"Please use mpneuralnetwork.serialization.restore_model_weights(model.layers, ...) instead.\",\n        DeprecationWarning,\n        stacklevel=2,\n    )\n    restore_model_weights(self.layers, weights_dict, optimizer)\n</code></pre>"},{"location":"reference/optimizers/","title":"Optimizers API","text":"<p>Optimizers update the model's parameters based on the computed gradients to minimize the loss function.</p>"},{"location":"reference/optimizers/#base-optimizer","title":"Base Optimizer","text":""},{"location":"reference/optimizers/#mpneuralnetwork.optimizers.Optimizer","title":"<code>mpneuralnetwork.optimizers.Optimizer</code>","text":"<p>Base class for all optimization algorithms.</p> <p>Optimizers update the weights of the network layers to minimize the loss function. They also handle regularization (L1/L2).</p> Source code in <code>src/mpneuralnetwork/optimizers.py</code> <pre><code>class Optimizer:\n    \"\"\"Base class for all optimization algorithms.\n\n    Optimizers update the weights of the network layers to minimize the loss function.\n    They also handle regularization (L1/L2).\n    \"\"\"\n\n    def __init__(self, learning_rate: float, regularization: Lit_R, weight_decay: float) -&gt; None:\n        \"\"\"Initializes the optimizer.\n\n        Args:\n            learning_rate (float): The step size for parameter updates.\n            regularization (Lit_R): Type of regularization ('L1' or 'L2').\n            weight_decay (float): The strength of the regularization (lambda).\n        \"\"\"\n        self.learning_rate: float = learning_rate\n        self.regularization: Lit_R = regularization\n        self.weight_decay: float = weight_decay\n\n    @abstractmethod\n    def step(self, layers: list[Layer]) -&gt; None:\n        \"\"\"Performs a single optimization step.\n\n        Iterates over all layers and updates their parameters based on stored gradients.\n\n        Args:\n            layers (list[Layer]): List of layers containing parameters to update.\n        \"\"\"\n        pass\n\n    def get_config(self) -&gt; dict:\n        return {\n            \"type\": self.__class__.__name__,\n            \"learning_rate\": self.learning_rate,\n            \"regularization\": self.regularization,\n            \"weight_decay\": self.weight_decay,\n        }\n\n    def apply_regularization(self, param_name: str, param: ArrayType) -&gt; ArrayType | int:\n        \"\"\"Computes the regularization gradient term.\n\n        Args:\n            param_name (str): Name of the parameter (e.g., 'weights', 'bias').\n            param (ArrayType): The parameter value.\n\n        Returns:\n            ArrayType | int: The gradient contribution from regularization.\n        \"\"\"\n        regularization: ArrayType\n        if \"bias\" in param_name or \"beta\" in param_name or \"gamma\" in param_name:\n            return 0\n\n        if self.regularization == \"L2\":\n            regularization = self.weight_decay * param\n        else:\n            regularization = self.weight_decay * xp.sign(param)\n\n        return regularization\n\n    @property\n    def params(self) -&gt; dict:\n        \"\"\"Returns the optimizer's internal state (velocities, moments).\"\"\"\n        return {}\n</code></pre>"},{"location":"reference/optimizers/#mpneuralnetwork.optimizers.Optimizer.params","title":"<code>params</code>  <code>property</code>","text":"<p>Returns the optimizer's internal state (velocities, moments).</p>"},{"location":"reference/optimizers/#mpneuralnetwork.optimizers.Optimizer.__init__","title":"<code>__init__(learning_rate, regularization, weight_decay)</code>","text":"<p>Initializes the optimizer.</p> <p>Parameters:</p> Name Type Description Default <code>learning_rate</code> <code>float</code> <p>The step size for parameter updates.</p> required <code>regularization</code> <code>Lit_R</code> <p>Type of regularization ('L1' or 'L2').</p> required <code>weight_decay</code> <code>float</code> <p>The strength of the regularization (lambda).</p> required Source code in <code>src/mpneuralnetwork/optimizers.py</code> <pre><code>def __init__(self, learning_rate: float, regularization: Lit_R, weight_decay: float) -&gt; None:\n    \"\"\"Initializes the optimizer.\n\n    Args:\n        learning_rate (float): The step size for parameter updates.\n        regularization (Lit_R): Type of regularization ('L1' or 'L2').\n        weight_decay (float): The strength of the regularization (lambda).\n    \"\"\"\n    self.learning_rate: float = learning_rate\n    self.regularization: Lit_R = regularization\n    self.weight_decay: float = weight_decay\n</code></pre>"},{"location":"reference/optimizers/#mpneuralnetwork.optimizers.Optimizer.apply_regularization","title":"<code>apply_regularization(param_name, param)</code>","text":"<p>Computes the regularization gradient term.</p> <p>Parameters:</p> Name Type Description Default <code>param_name</code> <code>str</code> <p>Name of the parameter (e.g., 'weights', 'bias').</p> required <code>param</code> <code>ArrayType</code> <p>The parameter value.</p> required <p>Returns:</p> Type Description <code>ArrayType | int</code> <p>ArrayType | int: The gradient contribution from regularization.</p> Source code in <code>src/mpneuralnetwork/optimizers.py</code> <pre><code>def apply_regularization(self, param_name: str, param: ArrayType) -&gt; ArrayType | int:\n    \"\"\"Computes the regularization gradient term.\n\n    Args:\n        param_name (str): Name of the parameter (e.g., 'weights', 'bias').\n        param (ArrayType): The parameter value.\n\n    Returns:\n        ArrayType | int: The gradient contribution from regularization.\n    \"\"\"\n    regularization: ArrayType\n    if \"bias\" in param_name or \"beta\" in param_name or \"gamma\" in param_name:\n        return 0\n\n    if self.regularization == \"L2\":\n        regularization = self.weight_decay * param\n    else:\n        regularization = self.weight_decay * xp.sign(param)\n\n    return regularization\n</code></pre>"},{"location":"reference/optimizers/#mpneuralnetwork.optimizers.Optimizer.step","title":"<code>step(layers)</code>  <code>abstractmethod</code>","text":"<p>Performs a single optimization step.</p> <p>Iterates over all layers and updates their parameters based on stored gradients.</p> <p>Parameters:</p> Name Type Description Default <code>layers</code> <code>list[Layer]</code> <p>List of layers containing parameters to update.</p> required Source code in <code>src/mpneuralnetwork/optimizers.py</code> <pre><code>@abstractmethod\ndef step(self, layers: list[Layer]) -&gt; None:\n    \"\"\"Performs a single optimization step.\n\n    Iterates over all layers and updates their parameters based on stored gradients.\n\n    Args:\n        layers (list[Layer]): List of layers containing parameters to update.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/optimizers/#algorithms","title":"Algorithms","text":""},{"location":"reference/optimizers/#mpneuralnetwork.optimizers.SGD","title":"<code>mpneuralnetwork.optimizers.SGD</code>","text":"<p>               Bases: <code>Optimizer</code></p> <p>Stochastic Gradient Descent (SGD) with Momentum.</p> Update rule <ol> <li><code>v = momentum * v - lr * gradient</code></li> <li><code>w = w + v</code></li> </ol> <p>Parameters:</p> Name Type Description Default <code>learning_rate</code> <code>float</code> <p>Step size. Defaults to 0.01.</p> <code>0.01</code> <code>regularization</code> <code>Lit_R</code> <p>'L1' or 'L2'. Defaults to 'L2'.</p> <code>'L2'</code> <code>weight_decay</code> <code>float</code> <p>Regularization strength. Defaults to 0.001.</p> <code>0.001</code> <code>momentum</code> <code>float</code> <p>Momentum factor (0 to 1). Defaults to 0.1.</p> <code>0.1</code> Source code in <code>src/mpneuralnetwork/optimizers.py</code> <pre><code>class SGD(Optimizer):\n    \"\"\"Stochastic Gradient Descent (SGD) with Momentum.\n\n    Update rule:\n        1. `v = momentum * v - lr * gradient`\n        2. `w = w + v`\n\n    Args:\n        learning_rate (float, optional): Step size. Defaults to 0.01.\n        regularization (Lit_R, optional): 'L1' or 'L2'. Defaults to 'L2'.\n        weight_decay (float, optional): Regularization strength. Defaults to 0.001.\n        momentum (float, optional): Momentum factor (0 to 1). Defaults to 0.1.\n    \"\"\"\n\n    def __init__(\n        self,\n        learning_rate: float = 0.01,\n        regularization: Lit_R = \"L2\",\n        weight_decay: float = 0.001,\n        momentum: float = 0.1,\n    ) -&gt; None:\n        super().__init__(learning_rate, regularization, weight_decay)\n        self.momentum: float = momentum\n\n        self.velocities: T = {}\n\n    def step(self, layers: list[Layer]) -&gt; None:\n        for layer in layers:\n            if not hasattr(layer, \"params\"):\n                continue\n\n            for param_name, (param, grad) in layer.params.items():\n                grad += self.apply_regularization(param_name, param)\n\n                p_id: int = id(param)\n\n                if p_id not in self.velocities:\n                    self.velocities[p_id] = xp.zeros_like(param, dtype=DTYPE)\n\n                # Velocity Update: v = momentum * v - lr * grad\n\n                # 1. v *= momentum (in-place)\n                xp.multiply(self.velocities[p_id], self.momentum, out=self.velocities[p_id])\n\n                # 2. v -= lr * grad\n                self.velocities[p_id] -= self.learning_rate * grad\n\n                # Parameter Update: w += v\n                param += self.velocities[p_id]\n\n    def get_config(self) -&gt; dict:\n        config = super().get_config()\n        config.update({\"momentum\": self.momentum})\n        return config\n\n    @property\n    def params(self) -&gt; dict:\n        return {\"velocities\": self.velocities}\n</code></pre>"},{"location":"reference/optimizers/#mpneuralnetwork.optimizers.RMSprop","title":"<code>mpneuralnetwork.optimizers.RMSprop</code>","text":"<p>               Bases: <code>Optimizer</code></p> <p>RMSprop optimizer.</p> <p>Adapts learning rates by dividing the gradient by a running average of its recent magnitude.</p> Update rule <ol> <li><code>cache = decay * cache + (1 - decay) * grad^2</code></li> <li><code>w = w - lr * grad / (sqrt(cache) + epsilon)</code></li> </ol> <p>Parameters:</p> Name Type Description Default <code>learning_rate</code> <code>float</code> <p>Defaults to 0.001.</p> <code>0.001</code> <code>regularization</code> <code>Lit_R</code> <p>'L1' or 'L2'.</p> <code>'L2'</code> <code>weight_decay</code> <code>float</code> <p>Defaults to 0.001.</p> <code>0.001</code> <code>decay_rate</code> <code>float</code> <p>Discounting factor. Defaults to 0.9.</p> <code>0.9</code> <code>epsilon</code> <code>float</code> <p>Small value for numerical stability. Defaults to 1e-8.</p> <code>1e-08</code> Source code in <code>src/mpneuralnetwork/optimizers.py</code> <pre><code>class RMSprop(Optimizer):\n    \"\"\"RMSprop optimizer.\n\n    Adapts learning rates by dividing the gradient by a running average of its recent magnitude.\n\n    Update rule:\n        1. `cache = decay * cache + (1 - decay) * grad^2`\n        2. `w = w - lr * grad / (sqrt(cache) + epsilon)`\n\n    Args:\n        learning_rate (float): Defaults to 0.001.\n        regularization (Lit_R): 'L1' or 'L2'.\n        weight_decay (float): Defaults to 0.001.\n        decay_rate (float, optional): Discounting factor. Defaults to 0.9.\n        epsilon (float, optional): Small value for numerical stability. Defaults to 1e-8.\n    \"\"\"\n\n    def __init__(\n        self,\n        learning_rate: float = 0.001,\n        regularization: Lit_R = \"L2\",\n        weight_decay: float = 0.001,\n        decay_rate: float = 0.9,\n        epsilon: float = 1e-8,\n    ) -&gt; None:\n        super().__init__(learning_rate, regularization, weight_decay)\n        self.decay_rate: float = decay_rate\n        self.epsilon: float = epsilon\n\n        self.cache: T = {}\n\n    def step(self, layers: list[Layer]) -&gt; None:\n        for layer in layers:\n            if not hasattr(layer, \"params\"):\n                continue\n\n            for param_name, (param, grad) in layer.params.items():\n                grad += self.apply_regularization(param_name, param)\n\n                p_id: int = id(param)\n\n                if p_id not in self.cache:\n                    self.cache[p_id] = xp.zeros_like(param, dtype=DTYPE)\n\n                # Cache Update: cache = decay * cache + (1 - decay) * grad^2\n\n                # 1. cache *= decay (in-place)\n                xp.multiply(self.cache[p_id], self.decay_rate, out=self.cache[p_id])\n\n                # 2. cache += (1 - decay) * grad^2\n                self.cache[p_id] += (1 - self.decay_rate) * xp.square(grad)\n\n                # Parameter Update: w -= lr * grad / (sqrt(cache) + epsilon)\n\n                # 1. Denominator = sqrt(cache) + epsilon\n                denom = xp.sqrt(self.cache[p_id])\n                xp.add(denom, self.epsilon, out=denom)\n\n                # 2. Update = lr * grad / denom\n                # w -= update\n                param -= self.learning_rate * grad / denom\n\n    def get_config(self) -&gt; dict:\n        config = super().get_config()\n        config.update({\"decay_rate\": self.decay_rate, \"epsilon\": self.epsilon})\n        return config\n\n    @property\n    def params(self) -&gt; dict:\n        return {\"cache\": self.cache}\n</code></pre>"},{"location":"reference/optimizers/#mpneuralnetwork.optimizers.Adam","title":"<code>mpneuralnetwork.optimizers.Adam</code>","text":"<p>               Bases: <code>Optimizer</code></p> <p>Adam Optimizer (Adaptive Moment Estimation).</p> <p>Combines Momentum and RMSprop. Implements Decoupled Weight Decay (AdamW) when <code>regularization='L2'</code>.</p> Update rule <ol> <li><code>m = beta1 * m + (1 - beta1) * g</code></li> <li><code>v = beta2 * v + (1 - beta2) * g^2</code></li> <li><code>m_hat = m / (1 - beta1^t)</code></li> <li><code>v_hat = v / (1 - beta2^t)</code></li> <li><code>w = w - lr * m_hat / (sqrt(v_hat) + eps)</code></li> <li>If L2: <code>w = w - lr * decay * w</code> (Decoupled)</li> </ol> <p>Parameters:</p> Name Type Description Default <code>beta1</code> <code>float</code> <p>Decay rate for first moment. Defaults to 0.9.</p> <code>0.9</code> <code>beta2</code> <code>float</code> <p>Decay rate for second moment. Defaults to 0.999.</p> <code>0.999</code> <code>epsilon</code> <code>float</code> <p>Stability term. Defaults to 1e-8.</p> <code>1e-08</code> Source code in <code>src/mpneuralnetwork/optimizers.py</code> <pre><code>class Adam(Optimizer):\n    \"\"\"Adam Optimizer (Adaptive Moment Estimation).\n\n    Combines Momentum and RMSprop.\n    Implements **Decoupled Weight Decay (AdamW)** when `regularization='L2'`.\n\n    Update rule:\n        1. `m = beta1 * m + (1 - beta1) * g`\n        2. `v = beta2 * v + (1 - beta2) * g^2`\n        3. `m_hat = m / (1 - beta1^t)`\n        4. `v_hat = v / (1 - beta2^t)`\n        5. `w = w - lr * m_hat / (sqrt(v_hat) + eps)`\n        6. If L2: `w = w - lr * decay * w` (Decoupled)\n\n    Args:\n        beta1 (float, optional): Decay rate for first moment. Defaults to 0.9.\n        beta2 (float, optional): Decay rate for second moment. Defaults to 0.999.\n        epsilon (float, optional): Stability term. Defaults to 1e-8.\n    \"\"\"\n\n    def __init__(\n        self,\n        learning_rate: float = 0.001,\n        regularization: Lit_R = \"L2\",\n        weight_decay: float = 0.001,\n        beta1: float = 0.9,\n        beta2: float = 0.999,\n        epsilon: float = 1e-8,\n    ) -&gt; None:\n        super().__init__(learning_rate, regularization, weight_decay)\n        self.beta1: float = beta1\n        self.beta2: float = beta2\n        self.epsilon: float = epsilon\n\n        self.t: int = 0\n        self.momentums: T = {}\n        self.velocities: T = {}\n\n    def step(self, layers: list[Layer]) -&gt; None:\n        self.t += 1\n\n        for layer in layers:\n            if not hasattr(layer, \"params\"):\n                continue\n\n            for param_name, (param, grad) in layer.params.items():\n                if self.regularization == \"L1\":\n                    grad += self.apply_regularization(param_name, param)\n\n                p_id: int = id(param)\n\n                if p_id not in self.momentums:\n                    self.momentums[p_id] = xp.zeros_like(param, dtype=DTYPE)\n                    self.velocities[p_id] = xp.zeros_like(param, dtype=DTYPE)\n\n                # --- 1. Update Momentum (First Moment) ---\n                # m = beta1 * m + (1 - beta1) * grad\n\n                # m *= beta1\n                xp.multiply(self.momentums[p_id], self.beta1, out=self.momentums[p_id])\n                # m += (1 - beta1) * grad\n                self.momentums[p_id] += (1 - self.beta1) * grad\n\n                # --- 2. Update Velocity (Second Moment) ---\n                # v = beta2 * v + (1 - beta2) * grad^2\n\n                # v *= beta2\n                xp.multiply(self.velocities[p_id], self.beta2, out=self.velocities[p_id])\n                # v += (1 - beta2) * grad^2\n                self.velocities[p_id] += (1 - self.beta2) * xp.square(grad)\n\n                # --- 3. Bias Correction ---\n                # m_hat = m / (1 - beta1^t)\n                # v_hat = v / (1 - beta2^t)\n\n                bias_correction1 = 1 - self.beta1**self.t\n                bias_correction2 = 1 - self.beta2**self.t\n\n                # Efficient Update Formula:\n                # w -= lr * m_hat / (sqrt(v_hat) + epsilon)\n                # w -= (lr / bias_correction1) * m / (sqrt(v / bias_correction2) + epsilon)\n\n                step_size = self.learning_rate / bias_correction1\n\n                # Denominator construction\n                denom = xp.sqrt(self.velocities[p_id])\n                # denom /= sqrt(bias_correction2)\n                xp.divide(denom, xp.sqrt(bias_correction2), out=denom)\n                # denom += epsilon\n                xp.add(denom, self.epsilon, out=denom)\n\n                # Final update: w -= step_size * m / denom\n                # param -= step_size * (self.momentums[p_id] / denom)\n                param -= step_size * self.momentums[p_id] / denom\n\n                if self.regularization == \"L2\":\n                    param -= self.learning_rate * self.apply_regularization(param_name, param)\n\n    def get_config(self) -&gt; dict:\n        config = super().get_config()\n        config.update(\n            {\n                \"beta1\": self.beta1,\n                \"beta2\": self.beta2,\n                \"epsilon\": self.epsilon,\n            }\n        )\n        return config\n\n    @property\n    def params(self) -&gt; dict:\n        return {\n            \"t\": self.t,\n            \"momentums\": self.momentums,\n            \"velocities\": self.velocities,\n        }\n</code></pre>"},{"location":"reference/serialization/","title":"Serialization","text":""},{"location":"reference/serialization/#mpneuralnetwork.serialization.save_model","title":"<code>mpneuralnetwork.serialization.save_model(model, filepath)</code>","text":"<p>Saves the full model state to a <code>.npz</code> archive.</p> <p>The archive contains: 1.  <code>architecture</code> (JSON): Configuration of layers (type, size, etc.). 2.  <code>model_config</code> (JSON): Loss, Optimizer config, and Optimizer globals (learning rate, etc.). 3.  <code>layer_{i}_{param}</code>: Raw numpy arrays for weights and biases. 4.  <code>layer_{i}_state_{name}</code>: Internal state (e.g., BatchNorm moving averages). 5.  <code>optimizer_{param}_{layer_param}</code>: Optimizer state (momentum, velocity) for each parameter.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Model</code> <p>The model instance to save.</p> required <code>filepath</code> <code>str</code> <p>Destination path. If extension is missing, <code>.npz</code> is appended.</p> required Source code in <code>src/mpneuralnetwork/serialization.py</code> <pre><code>def save_model(model: \"Model\", filepath: str) -&gt; None:\n    \"\"\"Saves the full model state to a `.npz` archive.\n\n    The archive contains:\n    1.  `architecture` (JSON): Configuration of layers (type, size, etc.).\n    2.  `model_config` (JSON): Loss, Optimizer config, and Optimizer globals (learning rate, etc.).\n    3.  `layer_{i}_{param}`: Raw numpy arrays for weights and biases.\n    4.  `layer_{i}_state_{name}`: Internal state (e.g., BatchNorm moving averages).\n    5.  `optimizer_{param}_{layer_param}`: Optimizer state (momentum, velocity) for each parameter.\n\n    Args:\n        model (Model): The model instance to save.\n        filepath (str): Destination path. If extension is missing, `.npz` is appended.\n    \"\"\"\n    layers_config: list = []\n    for layer in model.layers:\n        layers_config.append(layer.get_config())\n\n    optimizer_globals: dict = {}\n    optimizer_params: dict = {}\n\n    if model.optimizer.params:\n        for param_name, param in model.optimizer.params.items():\n            if isinstance(param, (int, float, str, bool)) or param is None:\n                optimizer_globals[param_name] = param\n            else:\n                optimizer_params[param_name] = param\n\n    model_config: dict = {\n        \"loss\": model.loss.get_config(),\n        \"optimizer\": model.optimizer.get_config(),\n        \"optimizer_globals\": optimizer_globals,\n    }\n\n    weights_dict: dict = get_model_weights(model.layers, optimizer_params)\n\n    if filepath[-4:] != \".npz\":\n        filepath = f\"{filepath}.npz\"\n\n    save_data: dict = weights_dict.copy()\n    save_data[\"architecture\"] = json.dumps(layers_config, cls=NumpyEncoder)\n    save_data[\"model_config\"] = json.dumps(model_config, cls=NumpyEncoder)\n\n    np.savez_compressed(filepath, **save_data)\n</code></pre>"},{"location":"reference/serialization/#mpneuralnetwork.serialization.load_model","title":"<code>mpneuralnetwork.serialization.load_model(path)</code>","text":"<p>Loads a full model from a <code>.npz</code> archive.</p> <p>This function instantiates a new <code>Model</code> object, rebuilds the layer graph, initializes the optimizer and loss function, and then loads all weights and states (including optimizer momentum) into memory.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str | Path</code> <p>Path to the <code>.npz</code> file.</p> required <p>Returns:</p> Name Type Description <code>Model</code> <code>Model</code> <p>The fully restored model, ready for training or inference.</p> Source code in <code>src/mpneuralnetwork/serialization.py</code> <pre><code>def load_model(path: str | Path) -&gt; \"Model\":\n    \"\"\"Loads a full model from a `.npz` archive.\n\n    This function instantiates a new `Model` object, rebuilds the layer graph,\n    initializes the optimizer and loss function, and then loads all weights\n    and states (including optimizer momentum) into memory.\n\n    Args:\n        path (str | Path): Path to the `.npz` file.\n\n    Returns:\n        Model: The fully restored model, ready for training or inference.\n    \"\"\"\n    from .model import Model\n\n    filepath: str = str(path) if isinstance(path, Path) else path\n\n    if filepath[-4:] != \".npz\":\n        filepath = f\"{filepath}.npz\"\n\n    data: dict = np.load(filepath, allow_pickle=True)\n\n    layers_config: dict = json.loads(str(data[\"architecture\"]))\n    model_config: dict = json.loads(str(data[\"model_config\"]))\n\n    model_layers: list = []\n    for conf in layers_config:\n        layer_type: str = str(conf.pop(\"type\"))\n        layer_class: Callable = _get_class(layer_type)\n        layer = layer_class(**conf)\n        model_layers.append(layer)\n\n    loss_conf: dict = model_config[\"loss\"]\n    loss_type: str = str(loss_conf.pop(\"type\"))\n    loss_class: Callable = _get_class(loss_type)\n    loss = loss_class(**loss_conf)\n\n    optim_conf: dict | str = model_config[\"optimizer\"]\n    optim_type: str\n\n    if isinstance(optim_conf, dict):\n        optim_type = str(optim_conf.pop(\"type\"))\n    else:\n        optim_type = str(optim_conf)\n        optim_conf = {}\n\n    optim_class: Callable = _get_class(optim_type)\n    optimizer = optim_class(**optim_conf)\n\n    if \"optimizer_globals\" in model_config:\n        for param_name, param in model_config[\"optimizer_globals\"].items():\n            setattr(optimizer, param_name, param)\n\n    model = Model(model_layers, loss, optimizer)\n    restore_model_weights(model.layers, data, optimizer)\n\n    return model\n</code></pre>"}]}